# Social Media Posts: Support Vector Machine Basics

================================================================================
## SHORT-FORM POSTS
================================================================================

### Twitter/X (< 280 chars)
--------------------------------------------------------------------------------
SVMs find the optimal line by maximizing the margin between classes. Only a few "support vectors" define the boundary - elegant & efficient!

Implemented from scratch in Python with dual optimization.

#MachineLearning #Python #SVM #DataScience

--------------------------------------------------------------------------------

### Bluesky (< 300 chars)
--------------------------------------------------------------------------------
Support Vector Machines explained from first principles.

Key insight: maximize margin = minimize ||w||²

The kernel trick lets SVMs learn non-linear boundaries without explicit high-dimensional mappings. Implemented dual optimization with RBF kernel in pure NumPy/SciPy.

#MachineLearning #Python

--------------------------------------------------------------------------------

### Threads (< 500 chars)
--------------------------------------------------------------------------------
Ever wondered how SVMs actually work under the hood?

Just built one from scratch! The math is beautiful:
- Find the hyperplane wᵀx + b = 0 that maximizes distance to nearest points
- Margin width = 2/||w||
- Only "support vectors" on the margin matter

The kernel trick is where it gets wild - RBF kernel K(xᵢ, xⱼ) = exp(-γ||xᵢ - xⱼ||²) lets you classify circles without computing infinite dimensions!

Low C = wider margin, more errors allowed
High C = tight margin, fewer errors

#MachineLearning #Python #DataScience

--------------------------------------------------------------------------------

### Mastodon (< 500 chars)
--------------------------------------------------------------------------------
Implemented SVM classifier from the dual formulation using SLSQP optimization.

Primal: min (1/2)||w||² + C∑ξᵢ
Dual: max ∑αᵢ - (1/2)∑∑αᵢαⱼyᵢyⱼxᵢᵀxⱼ

Key results:
- Linear kernel achieves 100% on separable data
- RBF kernel (γ=0.5) handles circular boundaries
- C controls margin-error tradeoff

Support vectors with αᵢ > 0 fully determine the decision boundary. Elegant Lagrangian duality at work!

#Python #MachineLearning #Optimization #SciPy

--------------------------------------------------------------------------------

================================================================================
## LONG-FORM POSTS
================================================================================

### Reddit (r/learnpython or r/MachineLearning)
--------------------------------------------------------------------------------
**Title:** I implemented Support Vector Machines from scratch - here's how the math actually works

**Body:**

I've been trying to really understand SVMs beyond just calling sklearn, so I implemented one from the ground up. Here's what I learned:

**The Core Idea (ELI5)**

Imagine you have red and blue dots on paper. You want to draw a line separating them. But not just any line - the BEST line that's as far as possible from both groups. That distance is called the "margin."

**The Math**

For a hyperplane wᵀx + b = 0:
- Distance from point xᵢ to hyperplane = |wᵀxᵢ + b| / ||w||
- We want: yᵢ(wᵀxᵢ + b) ≥ 1 for all points
- Margin width = 2/||w||

So maximizing margin = minimizing ||w||². That's the optimization problem!

**Soft Margins (Real Data is Messy)**

Real data isn't perfectly separable, so we add slack variables ξᵢ:

min (1/2)||w||² + C∑ξᵢ

The C parameter is crucial:
- Low C (like 0.1) = wide margin, allows misclassifications
- High C (like 100) = narrow margin, strict classification

**The Kernel Trick (Mind-Blowing Part)**

What if data isn't linearly separable? Like points arranged in circles?

Instead of finding a line in 2D, map to higher dimensions where it IS separable. But computing in infinite dimensions is impossible... unless you use the kernel trick!

K(xᵢ, xⱼ) = φ(xᵢ)ᵀφ(xⱼ)

The RBF kernel K(xᵢ, xⱼ) = exp(-γ||xᵢ - xⱼ||²) implicitly maps to infinite dimensions. You never actually compute φ(x)!

**Results**

- Linear SVM: 100% accuracy on separable data, only a handful of support vectors needed
- RBF SVM: Perfectly classifies circular data that's impossible for linear methods

**Code**

Used NumPy + SciPy's SLSQP optimizer to solve the dual Lagrangian. The elegance of Lagrange multipliers is that αᵢ > 0 only for support vectors - most training points don't matter!

Full notebook with visualizations: https://cocalc.com/github/Ok-landscape/computational-pipeline/blob/main/notebooks/published/support_vector_machine_basics.ipynb

--------------------------------------------------------------------------------

### Facebook (< 500 chars)
--------------------------------------------------------------------------------
Just implemented Support Vector Machines from scratch in Python!

The key insight: find the line that's furthest from BOTH groups of points. Only a handful of "support vectors" actually define this boundary.

What's really cool - the "kernel trick" lets you classify complex patterns (like circles within circles) without impossible math. The RBF kernel implicitly works in infinite dimensions!

See the full notebook with interactive visualizations:
https://cocalc.com/github/Ok-landscape/computational-pipeline/blob/main/notebooks/published/support_vector_machine_basics.ipynb

--------------------------------------------------------------------------------

### LinkedIn (< 1000 chars)
--------------------------------------------------------------------------------
Diving Deep into Support Vector Machines: Implementation from First Principles

In my latest computational notebook, I implemented SVM classifiers from the ground up to truly understand the mathematics behind this powerful algorithm.

Key Technical Highlights:

1. Dual Formulation - Solved the Lagrangian dual using SLSQP optimization, transforming a constrained problem into one where only support vectors matter

2. Kernel Methods - Implemented both linear and RBF kernels. The RBF kernel K(xᵢ, xⱼ) = exp(-γ||xᵢ - xⱼ||²) enables non-linear classification without explicit feature mapping

3. Regularization Analysis - Demonstrated how the C parameter controls the bias-variance tradeoff:
   - C=0.1: Wide margin, robust to outliers
   - C=100: Tight margin, sensitive to individual points

Skills Demonstrated:
- Convex optimization
- Lagrangian duality
- NumPy/SciPy numerical computing
- Mathematical visualization

The implementation achieves 100% training accuracy on both linearly separable and circular datasets, with clear visualizations of decision boundaries and margin structure.

Full notebook: https://cocalc.com/github/Ok-landscape/computational-pipeline/blob/main/notebooks/published/support_vector_machine_basics.ipynb

#MachineLearning #Python #DataScience #Optimization #SVM

--------------------------------------------------------------------------------

### Instagram (< 500 chars)
--------------------------------------------------------------------------------
Support Vector Machines - implemented from scratch!

The beautiful math of finding the optimal separator:

wᵀx + b = 0

Only a few "support vectors" on the margin actually matter. Everything else? Irrelevant.

The kernel trick lets you classify circles without computing infinite dimensions. Pure mathematical elegance.

Swipe to see:
→ Linear SVM on separable data
→ RBF kernel handling circular patterns
→ How regularization (C) changes the margin

Built with NumPy + SciPy
Full code in bio

#MachineLearning #Python #DataScience #SVM #CodingLife #Mathematics #AI #DeepLearning #Visualization

--------------------------------------------------------------------------------
