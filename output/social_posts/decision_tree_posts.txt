DECISION TREE SOCIAL MEDIA POSTS
=================================

## SHORT-FORM POSTS

### Twitter/X (280 chars)
Built a decision tree classifier from scratch in Python! Key insight: Gini impurity G = 1 - Σpₖ² guides optimal splits. Achieved 93%+ test accuracy on 3-class problem. The axis-aligned boundaries show why ensemble methods matter. #MachineLearning #Python #DataScience

### Bluesky (300 chars)
Implemented decision trees from scratch to understand the math behind the magic. The algorithm recursively partitions feature space using Gini impurity: G = 1 - Σpₖ². Visualized the bias-variance tradeoff: shallow trees underfit, deep trees overfit. Sweet spot matters.

### Threads (500 chars)
Ever wonder how decision trees actually work under the hood?

I implemented one from scratch and the core idea is beautifully simple: at each node, find the split that minimizes impurity.

Gini impurity: G = 1 - Σpₖ²

The visualization shows why trees create those characteristic "staircase" boundaries - they can only make axis-aligned cuts!

This also explains their main weakness: capturing diagonal patterns requires many splits. That's why Random Forests combine hundreds of trees.

### Mastodon (500 chars)
Deep dive into decision tree internals today. Implemented CART algorithm from scratch using Gini impurity as splitting criterion.

Key equations:
• Gini: G = 1 - Σpₖ²
• Split criterion: minimize weighted child impurity

Interesting findings:
- Optimal depth was 5 (bias-variance sweet spot)
- Shallow trees (d=2) clearly underfit
- Deep trees (d=15) overfit to noise

The rectangular decision boundaries clearly show the limitation that motivates ensemble methods.

Code uses only numpy/matplotlib.


## LONG-FORM POSTS

### Reddit (r/learnpython or r/MachineLearning)

**Title:** Built a Decision Tree Classifier from Scratch - Here's What I Learned About the Math

**Body:**

I implemented a decision tree classifier using only NumPy to really understand how they work. Here's the breakdown:

**The Core Idea**

Decision trees recursively partition your feature space into rectangular regions. At each node, they find the split that best separates the classes.

**Gini Impurity**

The splitting criterion I used: G = 1 - Σpₖ²

Where pₖ is the proportion of class k in the node. Gini = 0 means pure node (all one class), Gini = 0.5 for binary means maximum impurity.

**What I Learned**

1. **Axis-aligned boundaries**: Trees can only split perpendicular to feature axes. This creates those characteristic "staircase" patterns. Diagonal decision boundaries require many splits.

2. **Bias-variance tradeoff is real**:
   - Depth=2: Underfits badly, can't capture the pattern
   - Depth=5: Sweet spot, good generalization
   - Depth=15: Overfits, creates tiny regions around individual points

3. **Why ensembles work**: Single trees are unstable. Small data changes → completely different trees. Random Forests average out this variance.

**Results**

- 300 samples, 3 classes
- Test accuracy: 93%+ at optimal depth
- Training accuracy approaches 100% as depth increases (overfitting signal)

The implementation is ~100 lines of Python. The predict function just walks the tree following if-then rules - that's the interpretability advantage.

**Interactive Notebook:**
https://cocalc.com/github/Ok-landscape/computational-pipeline/blob/main/notebooks/published/decision_tree.ipynb


### Facebook (500 chars)

How do computers make decisions? Decision trees!

I built one from scratch and it's surprisingly intuitive. The algorithm asks yes/no questions about your data until it reaches an answer.

The cool part: you can trace exactly WHY it made each decision. Unlike neural networks, there's no black box.

Key finding: too simple = misses patterns, too complex = memorizes noise. Finding the sweet spot is the art of machine learning.

View the full interactive notebook: https://cocalc.com/github/Ok-landscape/computational-pipeline/blob/main/notebooks/published/decision_tree.ipynb


### LinkedIn (1000 chars)

**Understanding Machine Learning Fundamentals: Decision Trees from Scratch**

Implemented a decision tree classifier using only NumPy to deepen my understanding of fundamental ML algorithms.

**Technical Approach:**
- Built recursive CART algorithm using Gini impurity: G = 1 - Σpₖ²
- Implemented greedy splitting: at each node, exhaustively search for optimal (feature, threshold) pair
- Complexity: O(d·n²·log n) for training, O(log n) for prediction

**Key Insights:**

The bias-variance tradeoff was clearly visible:
• Shallow trees (depth=2): High bias, poor train and test accuracy
• Optimal depth (5): Balanced performance, 93%+ test accuracy
• Deep trees (depth=15): High variance, perfect training but degraded test performance

**Why This Matters:**

Decision trees are the foundation for powerful ensemble methods (Random Forest, XGBoost, LightGBM) that dominate structured data competitions. Understanding the base learner's limitations explains why ensembling works.

**Skills Demonstrated:** Algorithm implementation, mathematical foundations, data visualization, model evaluation

Full notebook with code and visualizations: https://cocalc.com/github/Ok-landscape/computational-pipeline/blob/main/notebooks/published/decision_tree.ipynb


### Instagram (500 chars)

The art of machine learning decisions

This visualization shows how a decision tree classifies data into three groups. Notice the rectangular boundaries? That's because trees can only make axis-aligned splits.

Left side: Different tree depths show the bias-variance tradeoff
• Too shallow → misses patterns
• Too deep → memorizes noise
• Just right → generalizes well

The Gini impurity formula (G = 1 - Σpₖ²) guides each split, seeking maximum class separation.

Built from scratch in Python.

#MachineLearning #DataScience #Python #DataVisualization #AI #Coding #Tech #Learning
