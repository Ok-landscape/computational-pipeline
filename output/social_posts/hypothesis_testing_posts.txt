# Social Media Posts: Hypothesis Testing
# Generated from: hypothesis_testing.ipynb

================================================================================
TWITTER/X (< 280 chars)
================================================================================

Is your sample mean real or just noise?

Hypothesis testing lets you decide with math:
t = (x̄ - μ₀) / (s/√n)

Simulated 10,000 tests to show Type I errors and statistical power in action.

#Statistics #Python #DataScience #Science

================================================================================
BLUESKY (< 300 chars)
================================================================================

Statistical hypothesis testing: the framework for separating signal from noise in experimental data.

Built a complete demo with t-tests, power analysis, and Monte Carlo simulations showing p-value distributions under H₀ and H₁.

Key insight: 80% power requires careful sample size planning.

#Statistics #Python

================================================================================
THREADS (< 500 chars)
================================================================================

Ever wonder how scientists decide if their results are "real" or just random chance?

It's called hypothesis testing, and it's surprisingly intuitive once you see it visually.

I built an interactive notebook that shows:
- How t-distributions create rejection regions
- Why p-values < 0.05 matter
- How to calculate statistical power
- The difference between Type I and Type II errors

The coolest part? Running 10,000 simulations to watch p-values behave differently under null vs alternative hypotheses.

#Statistics #DataScience #Python

================================================================================
MASTODON (< 500 chars)
================================================================================

New computational notebook: Hypothesis Testing with full mathematical derivations and Python implementation.

Covers:
- One-sample and two-sample t-tests
- Test statistic: t = (x̄ - μ₀)/(s/√n)
- Power function: P(reject H₀ | H₁ true)
- Cohen's d effect size calculation
- Monte Carlo p-value simulations (n=10,000)

Includes visualization of rejection regions, power curves vs sample size/effect size, and p-value distributions under H₀ and H₁.

All code self-contained with numpy/scipy/matplotlib.

#Statistics #Python #Science #Mathematics

================================================================================
REDDIT (r/learnpython or r/statistics)
================================================================================

Title: I built an interactive Python notebook explaining hypothesis testing with visualizations and Monte Carlo simulations

Body:

**What is this?**

A comprehensive Jupyter notebook that teaches statistical hypothesis testing from first principles, with full Python implementations and visualizations.

**ELI5: What is hypothesis testing?**

Imagine you're testing whether a factory produces widgets at the correct weight (100g). You grab 30 widgets and find the average is 102g. Is the factory broken, or did you just grab 30 slightly heavy ones by chance?

Hypothesis testing gives you a mathematical framework to answer this. You calculate a "t-statistic" that measures how far your sample is from expected, accounting for sample size and variability. Then you ask: "If the factory IS working correctly, what's the probability I'd see results this extreme?" That probability is your p-value.

**What the notebook covers:**

1. **One-sample t-test** - Testing if a sample mean differs from a hypothesized value
2. **Two-sample Welch's t-test** - Comparing means between two groups
3. **Confidence intervals** - The range where the true mean likely falls
4. **Power analysis** - How sample size affects your ability to detect real effects
5. **Effect sizes (Cohen's d)** - Measuring practical significance, not just statistical significance

**The cool visualizations:**

- t-distribution with rejection regions (the red zones where you reject H₀)
- Power curves showing how detecting an effect depends on sample size
- Monte Carlo simulation of 10,000 hypothesis tests showing p-value distributions under null (uniform!) vs alternative (skewed toward 0)

**Key formulas implemented:**

- t-statistic: t = (x̄ - μ₀) / (s/√n)
- Standard error: SE = s/√n
- Confidence interval: x̄ ± t(α/2) × SE
- Cohen's d: d = (x̄₁ - x̄₂) / s_pooled

**What I learned:**

The most eye-opening part was visualizing the p-value distributions. Under the null hypothesis, p-values are uniformly distributed (flat histogram). Under the alternative, they pile up near zero. This really drove home why we reject when p < α.

Also, power analysis is crucial. With n=30 and effect size=2, you need about 50 samples to achieve 80% power. Many studies are underpowered!

**View and run the notebook:**

https://cocalc.com/github/Ok-landscape/computational-pipeline/blob/main/notebooks/published/hypothesis_testing.ipynb

All code is self-contained using numpy, scipy, and matplotlib. Feel free to modify parameters and see how results change!

================================================================================
FACEBOOK (< 500 chars)
================================================================================

How do scientists know if their results are real or just coincidence?

It's called hypothesis testing - a mathematical framework for making decisions from data.

I created an interactive notebook that visualizes the whole process: calculating test statistics, understanding p-values, and running simulations to see how often we make the right (or wrong) calls.

The best part? You can run it yourself and experiment with different sample sizes and effect sizes.

Check it out: https://cocalc.com/github/Ok-landscape/computational-pipeline/blob/main/notebooks/published/hypothesis_testing.ipynb

================================================================================
LINKEDIN (< 1000 chars)
================================================================================

Statistical Rigor in Data-Driven Decision Making

Just published a computational notebook on hypothesis testing - the foundational framework that separates robust findings from noise in quantitative research.

The implementation covers:

• One-sample and two-sample t-tests with proper degrees of freedom
• Power analysis demonstrating the sample size requirements for detecting effects
• Monte Carlo simulations (n=10,000) validating theoretical Type I error rates
• Effect size calculation using Cohen's d for practical significance assessment

Key technical insight: The notebook demonstrates that under the null hypothesis, p-values follow a uniform distribution - a fundamental property that many practitioners overlook. This has direct implications for multiple testing corrections and false discovery rate control.

The power analysis visualization shows that detecting an effect size of 2 (with σ=5) requires approximately 50 samples to achieve 80% power at α=0.05. This kind of planning is essential for resource allocation in experimental design.

All implementations use scipy.stats and numpy, with publication-quality matplotlib visualizations.

Explore the interactive notebook: https://cocalc.com/github/Ok-landscape/computational-pipeline/blob/main/notebooks/published/hypothesis_testing.ipynb

#Statistics #DataScience #Python #QuantitativeAnalysis #Research

================================================================================
INSTAGRAM (< 500 chars)
================================================================================

The math behind "Is this result real?"

Hypothesis testing helps scientists separate signal from noise.

This visualization shows:
→ The t-distribution with rejection regions (red zones = reject H₀)
→ Power curves showing detection ability
→ 10,000 simulated tests

When p < 0.05, we reject the null hypothesis.
But there's so much more to understand - Type I/II errors, statistical power, effect sizes.

Swipe to see how sample size affects your ability to detect real effects.

#Statistics #DataVisualization #Python #Science #Math #DataScience #Coding
