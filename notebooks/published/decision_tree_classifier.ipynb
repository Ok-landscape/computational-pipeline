{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "Decision trees are supervised learning algorithms used for classification and regression tasks. They partition the feature space into regions by recursively splitting on feature values, creating a tree-like structure of decisions.\n",
    "\n",
    "### Information Theory Basis\n",
    "\n",
    "The core principle behind decision tree construction is **information gain**, derived from Shannon's entropy. For a dataset $S$ with $K$ classes, the **entropy** is defined as:\n",
    "\n",
    "$$H(S) = -\\sum_{k=1}^{K} p_k \\log_2(p_k)$$\n",
    "\n",
    "where $p_k$ is the proportion of samples belonging to class $k$.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "When we split dataset $S$ on attribute $A$ with possible values $\\{v_1, v_2, \\ldots, v_n\\}$, the **information gain** is:\n",
    "\n",
    "$$IG(S, A) = H(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
    "\n",
    "where $S_v$ is the subset of $S$ for which attribute $A$ has value $v$.\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "An alternative splitting criterion is the **Gini impurity**:\n",
    "\n",
    "$$G(S) = 1 - \\sum_{k=1}^{K} p_k^2$$\n",
    "\n",
    "Gini impurity measures the probability of incorrectly classifying a randomly chosen element if it were labeled according to the class distribution in $S$.\n",
    "\n",
    "### Splitting Criterion for Continuous Features\n",
    "\n",
    "For a continuous feature $x_j$, the tree finds the optimal threshold $\\theta$ that maximizes information gain:\n",
    "\n",
    "$$\\theta^* = \\arg\\max_{\\theta} IG(S, x_j \\leq \\theta)$$\n",
    "\n",
    "This creates a binary split: samples with $x_j \\leq \\theta$ go to the left child, others to the right.\n",
    "\n",
    "### Classification Rule\n",
    "\n",
    "For a leaf node $L$ containing samples from multiple classes, the predicted class is:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{k} |\\{(x_i, y_i) \\in L : y_i = k\\}|$$\n",
    "\n",
    "i.e., the majority class in that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement a decision tree classifier from scratch using numpy, then demonstrate its performance on a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    \"\"\"Node in a decision tree.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        # For internal nodes\n",
    "        self.feature_index = feature_index  # Index of feature to split on\n",
    "        self.threshold = threshold          # Threshold value for split\n",
    "        self.left = left                    # Left subtree (feature <= threshold)\n",
    "        self.right = right                  # Right subtree (feature > threshold)\n",
    "        \n",
    "        # For leaf nodes\n",
    "        self.value = value                  # Predicted class\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"Decision Tree Classifier using Gini impurity.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the decision tree.\"\"\"\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.root = self._grow_tree(X, y, depth=0)\n",
    "        return self\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        \"\"\"Calculate Gini impurity.\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "    \n",
    "    def _information_gain(self, y, y_left, y_right):\n",
    "        \"\"\"Calculate information gain from a split.\"\"\"\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        \n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0\n",
    "        \n",
    "        parent_gini = self._gini(y)\n",
    "        child_gini = (n_left / n) * self._gini(y_left) + (n_right / n) * self._gini(y_right)\n",
    "        \n",
    "        return parent_gini - child_gini\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find the best split for a node.\"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature_idx in range(self.n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        \"\"\"Recursively grow the decision tree.\"\"\"\n",
    "        n_samples = len(y)\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or \n",
    "            n_labels == 1 or \n",
    "            n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        \n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "        \n",
    "        # Create child splits\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        left_child = self._grow_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_child = self._grow_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return DecisionTreeNode(\n",
    "            feature_index=best_feature,\n",
    "            threshold=best_threshold,\n",
    "            left=left_child,\n",
    "            right=right_child\n",
    "        )\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        \"\"\"Return the most common class label.\"\"\"\n",
    "        counter = Counter(y)\n",
    "        return counter.most_common(1)[0][0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples.\"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        \"\"\"Traverse the tree to make a prediction.\"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset\n",
    "\n",
    "We create a two-dimensional dataset with three classes arranged in distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=300):\n",
    "    \"\"\"Generate synthetic classification data with 3 classes.\"\"\"\n",
    "    n_per_class = n_samples // 3\n",
    "    \n",
    "    # Class 0: Cluster centered at (1, 1)\n",
    "    X0 = np.random.randn(n_per_class, 2) * 0.5 + np.array([1, 1])\n",
    "    y0 = np.zeros(n_per_class, dtype=int)\n",
    "    \n",
    "    # Class 1: Cluster centered at (4, 1)\n",
    "    X1 = np.random.randn(n_per_class, 2) * 0.5 + np.array([4, 1])\n",
    "    y1 = np.ones(n_per_class, dtype=int)\n",
    "    \n",
    "    # Class 2: Cluster centered at (2.5, 4)\n",
    "    X2 = np.random.randn(n_per_class, 2) * 0.5 + np.array([2.5, 4])\n",
    "    y2 = np.full(n_per_class, 2, dtype=int)\n",
    "    \n",
    "    X = np.vstack([X0, X1, X2])\n",
    "    y = np.concatenate([y0, y1, y2])\n",
    "    \n",
    "    # Shuffle the data\n",
    "    shuffle_idx = np.random.permutation(len(y))\n",
    "    return X[shuffle_idx], y[shuffle_idx]\n",
    "\n",
    "# Generate dataset\n",
    "X, y = generate_data(300)\n",
    "\n",
    "# Split into train and test sets\n",
    "split_idx = int(0.8 * len(y))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "clf = DecisionTreeClassifier(max_depth=5, min_samples_split=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = np.mean(y_pred_train == y_train)\n",
    "test_accuracy = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Decision Boundaries\n",
    "\n",
    "We visualize the decision boundaries created by the tree along with the training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, title):\n",
    "    \"\"\"Plot decision boundaries and data points.\"\"\"\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Get predictions for mesh grid\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot decision regions\n",
    "    contour = ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    # Plot decision boundaries\n",
    "    ax.contour(xx, yy, Z, colors='k', linewidths=0.5)\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                         edgecolors='black', s=50)\n",
    "    \n",
    "    ax.set_xlabel('Feature $x_1$', fontsize=12)\n",
    "    ax.set_ylabel('Feature $x_2$', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Class', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create visualization\n",
    "fig = plot_decision_boundary(\n",
    "    clf, X_train, y_train,\n",
    "    f'Decision Tree Classifier\\nTrain Acc: {train_accuracy:.3f}, Test Acc: {test_accuracy:.3f}'\n",
    ")\n",
    "\n",
    "# Save plot\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "The decision tree classifier successfully partitions the feature space into regions corresponding to each class. The decision boundaries are axis-aligned (parallel to feature axes), which is characteristic of decision trees using single-feature splits.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Interpretability**: Each decision boundary corresponds to a threshold on a single feature, making the model highly interpretable.\n",
    "\n",
    "2. **Non-linear Boundaries**: Despite using only linear splits, the combination of multiple splits creates non-linear decision boundaries.\n",
    "\n",
    "3. **Gini Impurity**: The algorithm minimizes Gini impurity at each split:\n",
    "   $$G(S) = 1 - \\sum_{k=1}^{K} p_k^2$$\n",
    "\n",
    "4. **Overfitting Control**: The `max_depth` and `min_samples_split` hyperparameters prevent the tree from growing too deep and overfitting to noise.\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "- **Training**: $O(n \\cdot m \\cdot \\log(n))$ where $n$ is the number of samples and $m$ is the number of features\n",
    "- **Prediction**: $O(\\log(n))$ for balanced trees, $O(n)$ worst case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
