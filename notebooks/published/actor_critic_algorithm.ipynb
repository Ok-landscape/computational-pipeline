{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Algorithm in Reinforcement Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **Actor-Critic** algorithm is a foundational method in reinforcement learning that combines the strengths of both policy-based and value-based approaches. It maintains two separate models:\n",
    "\n",
    "1. **Actor**: A policy function $\\pi_\\theta(a|s)$ that selects actions\n",
    "2. **Critic**: A value function $V_w(s)$ that evaluates state quality\n",
    "\n",
    "This hybrid architecture addresses key limitations of pure policy gradient methods (high variance) and pure value-based methods (inability to handle continuous action spaces naturally).\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### Policy Gradient Theorem\n",
    "\n",
    "The objective in policy-based reinforcement learning is to maximize the expected cumulative reward:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t \\right]$$\n",
    "\n",
    "where $\\gamma \\in [0,1]$ is the discount factor and $\\tau$ represents a trajectory.\n",
    "\n",
    "The policy gradient theorem gives us:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi}(s, a) \\right]$$\n",
    "\n",
    "### Advantage Function\n",
    "\n",
    "The **advantage function** $A(s, a)$ measures how much better an action is compared to the average:\n",
    "\n",
    "$$A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)$$\n",
    "\n",
    "Using the advantage function reduces variance while maintaining an unbiased gradient estimate:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot A^{\\pi}(s, a) \\right]$$\n",
    "\n",
    "### Temporal Difference (TD) Error\n",
    "\n",
    "In practice, we estimate the advantage using the **TD error** $\\delta_t$:\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V_w(s_{t+1}) - V_w(s_t)$$\n",
    "\n",
    "This TD error serves as an unbiased estimate of the advantage function.\n",
    "\n",
    "## Actor-Critic Algorithm\n",
    "\n",
    "### Update Rules\n",
    "\n",
    "**Critic Update** (minimizing TD error):\n",
    "$$w \\leftarrow w + \\alpha_w \\delta_t \\nabla_w V_w(s_t)$$\n",
    "\n",
    "**Actor Update** (following policy gradient):\n",
    "$$\\theta \\leftarrow \\theta + \\alpha_\\theta \\delta_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$$\n",
    "\n",
    "where $\\alpha_w$ and $\\alpha_\\theta$ are learning rates for the critic and actor respectively.\n",
    "\n",
    "### Algorithm Pseudocode\n",
    "\n",
    "```\n",
    "Initialize actor parameters θ and critic parameters w\n",
    "For each episode:\n",
    "    Initialize state s\n",
    "    For each step:\n",
    "        Sample action a ~ π_θ(·|s)\n",
    "        Execute a, observe r, s'\n",
    "        Compute TD error: δ = r + γV_w(s') - V_w(s)\n",
    "        Update critic: w ← w + α_w δ ∇_w V_w(s)\n",
    "        Update actor: θ ← θ + α_θ δ ∇_θ log π_θ(a|s)\n",
    "        s ← s'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement an Actor-Critic agent to solve a simple continuous control task: balancing a pole on a cart (CartPole-like environment simulated from scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom CartPole Environment\n",
    "\n",
    "We implement a simplified CartPole environment based on the classic control problem. The state consists of:\n",
    "- Cart position $x$\n",
    "- Cart velocity $\\dot{x}$\n",
    "- Pole angle $\\theta$\n",
    "- Pole angular velocity $\\dot{\\theta}$\n",
    "\n",
    "The dynamics follow:\n",
    "\n",
    "$$\\ddot{\\theta} = \\frac{g \\sin\\theta + \\cos\\theta \\left( \\frac{-F - m_p l \\dot{\\theta}^2 \\sin\\theta}{m_c + m_p} \\right)}{l \\left( \\frac{4}{3} - \\frac{m_p \\cos^2\\theta}{m_c + m_p} \\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnv:\n",
    "    \"\"\"Simple CartPole environment implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Physical parameters\n",
    "        self.gravity = 9.8\n",
    "        self.cart_mass = 1.0\n",
    "        self.pole_mass = 0.1\n",
    "        self.total_mass = self.cart_mass + self.pole_mass\n",
    "        self.pole_length = 0.5  # half-length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # time step\n",
    "        \n",
    "        # Termination thresholds\n",
    "        self.x_threshold = 2.4\n",
    "        self.theta_threshold = 12 * np.pi / 180  # 12 degrees\n",
    "        \n",
    "        self.state = None\n",
    "        self.steps = 0\n",
    "        self.max_steps = 200\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        # Small random initial state\n",
    "        self.state = np.random.uniform(-0.05, 0.05, size=(4,))\n",
    "        self.steps = 0\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action and return next state, reward, done.\"\"\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        \n",
    "        # Apply force based on action (0 = left, 1 = right)\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        \n",
    "        # Physics calculations\n",
    "        cos_theta = np.cos(theta)\n",
    "        sin_theta = np.sin(theta)\n",
    "        \n",
    "        temp = (force + self.pole_mass * self.pole_length * theta_dot**2 * sin_theta) / self.total_mass\n",
    "        theta_acc = (self.gravity * sin_theta - cos_theta * temp) / \\\n",
    "                    (self.pole_length * (4.0/3.0 - self.pole_mass * cos_theta**2 / self.total_mass))\n",
    "        x_acc = temp - self.pole_mass * self.pole_length * theta_acc * cos_theta / self.total_mass\n",
    "        \n",
    "        # Euler integration\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * x_acc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * theta_acc\n",
    "        \n",
    "        self.state = np.array([x, x_dot, theta, theta_dot])\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Check termination\n",
    "        done = bool(\n",
    "            x < -self.x_threshold or x > self.x_threshold or\n",
    "            theta < -self.theta_threshold or theta > self.theta_threshold or\n",
    "            self.steps >= self.max_steps\n",
    "        )\n",
    "        \n",
    "        # Reward: +1 for each step the pole stays balanced\n",
    "        reward = 1.0 if not done else 0.0\n",
    "        \n",
    "        return self.state.copy(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Agent\n",
    "\n",
    "We implement a tabular-style Actor-Critic using linear function approximation with tile coding features for simplicity and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    \"\"\"Actor-Critic agent with linear function approximation.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=4, n_actions=2, n_features=256):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Feature extraction parameters (random Fourier features)\n",
    "        self.feature_weights = np.random.randn(state_dim, n_features) * 0.5\n",
    "        self.feature_bias = np.random.uniform(0, 2 * np.pi, n_features)\n",
    "        \n",
    "        # Actor parameters (policy weights)\n",
    "        self.theta = np.zeros((n_features, n_actions))\n",
    "        \n",
    "        # Critic parameters (value function weights)\n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        # Learning rates\n",
    "        self.alpha_actor = 0.001\n",
    "        self.alpha_critic = 0.01\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        \n",
    "    def get_features(self, state):\n",
    "        \"\"\"Extract features from state using random Fourier features.\"\"\"\n",
    "        return np.cos(state @ self.feature_weights + self.feature_bias)\n",
    "    \n",
    "    def get_policy(self, state):\n",
    "        \"\"\"Get action probabilities using softmax policy.\"\"\"\n",
    "        features = self.get_features(state)\n",
    "        logits = features @ self.theta\n",
    "        # Numerically stable softmax\n",
    "        logits = logits - np.max(logits)\n",
    "        exp_logits = np.exp(logits)\n",
    "        return exp_logits / np.sum(exp_logits)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Sample action from policy.\"\"\"\n",
    "        probs = self.get_policy(state)\n",
    "        return np.random.choice(self.n_actions, p=probs)\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        \"\"\"Get state value from critic.\"\"\"\n",
    "        features = self.get_features(state)\n",
    "        return features @ self.w\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update actor and critic parameters.\"\"\"\n",
    "        features = self.get_features(state)\n",
    "        \n",
    "        # Compute TD error (advantage estimate)\n",
    "        current_value = features @ self.w\n",
    "        next_value = 0 if done else self.get_value(next_state)\n",
    "        td_error = reward + self.gamma * next_value - current_value\n",
    "        \n",
    "        # Critic update: minimize TD error\n",
    "        self.w += self.alpha_critic * td_error * features\n",
    "        \n",
    "        # Actor update: policy gradient with advantage\n",
    "        probs = self.get_policy(state)\n",
    "        # Gradient of log policy: feature * (I[a] - π(a|s))\n",
    "        grad_log_policy = np.zeros((self.n_features, self.n_actions))\n",
    "        for a in range(self.n_actions):\n",
    "            if a == action:\n",
    "                grad_log_policy[:, a] = features * (1 - probs[a])\n",
    "            else:\n",
    "                grad_log_policy[:, a] = features * (-probs[a])\n",
    "        \n",
    "        self.theta += self.alpha_actor * td_error * grad_log_policy\n",
    "        \n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "We train the Actor-Critic agent on the CartPole environment and track learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(n_episodes=1000, print_every=100):\n",
    "    \"\"\"Train Actor-Critic agent and return training history.\"\"\"\n",
    "    env = CartPoleEnv()\n",
    "    agent = ActorCriticAgent()\n",
    "    \n",
    "    # Training history\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    td_errors = []\n",
    "    \n",
    "    # Moving average for tracking progress\n",
    "    reward_window = deque(maxlen=100)\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_td_errors = []\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            # Select and execute action\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Update agent\n",
    "            td_error = agent.update(state, action, reward, next_state, done)\n",
    "            episode_td_errors.append(abs(td_error))\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        # Record history\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(env.steps)\n",
    "        td_errors.append(np.mean(episode_td_errors))\n",
    "        reward_window.append(episode_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(reward_window)\n",
    "            print(f\"Episode {episode + 1:4d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:6.1f} | \"\n",
    "                  f\"Avg TD Error: {np.mean(td_errors[-100:]):6.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'td_errors': td_errors,\n",
    "        'agent': agent\n",
    "    }\n",
    "\n",
    "# Train the agent\n",
    "print(\"Training Actor-Critic Agent on CartPole...\\n\")\n",
    "history = train_actor_critic(n_episodes=1000, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Learning Progress\n",
    "\n",
    "We visualize the agent's learning curves including:\n",
    "1. Episode rewards over time\n",
    "2. TD error convergence\n",
    "3. Moving average of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(history, window=50):\n",
    "    \"\"\"Plot comprehensive training results.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Actor-Critic Algorithm: Training Progress on CartPole', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    rewards = history['rewards']\n",
    "    td_errors = history['td_errors']\n",
    "    episodes = range(1, len(rewards) + 1)\n",
    "    \n",
    "    # Plot 1: Episode Rewards\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(episodes, rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "    # Moving average\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window, len(rewards) + 1), moving_avg, \n",
    "             color='red', linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "    ax1.axhline(y=195, color='green', linestyle='--', label='Solved Threshold (195)')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.set_title('Learning Curve: Episode Rewards')\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: TD Error\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(episodes, td_errors, alpha=0.3, color='purple')\n",
    "    # Moving average of TD error\n",
    "    td_moving_avg = np.convolve(td_errors, np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(range(window, len(td_errors) + 1), td_moving_avg, \n",
    "             color='darkviolet', linewidth=2)\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Mean |TD Error|')\n",
    "    ax2.set_title('Critic Convergence: TD Error')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Reward Distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    # Split into early and late training\n",
    "    split = len(rewards) // 2\n",
    "    ax3.hist(rewards[:split], bins=30, alpha=0.5, label='First Half', color='coral')\n",
    "    ax3.hist(rewards[split:], bins=30, alpha=0.5, label='Second Half', color='steelblue')\n",
    "    ax3.axvline(x=np.mean(rewards[:split]), color='coral', linestyle='--', linewidth=2)\n",
    "    ax3.axvline(x=np.mean(rewards[split:]), color='steelblue', linestyle='--', linewidth=2)\n",
    "    ax3.set_xlabel('Episode Reward')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Reward Distribution: Early vs Late Training')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Performance Summary\n",
    "    ax4 = axes[1, 1]\n",
    "    # Calculate statistics for different training phases\n",
    "    n_phases = 5\n",
    "    phase_size = len(rewards) // n_phases\n",
    "    phases = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    for i in range(n_phases):\n",
    "        start = i * phase_size\n",
    "        end = (i + 1) * phase_size if i < n_phases - 1 else len(rewards)\n",
    "        phase_rewards = rewards[start:end]\n",
    "        phases.append(f'{start+1}-{end}')\n",
    "        means.append(np.mean(phase_rewards))\n",
    "        stds.append(np.std(phase_rewards))\n",
    "    \n",
    "    x_pos = np.arange(len(phases))\n",
    "    bars = ax4.bar(x_pos, means, yerr=stds, capsize=5, \n",
    "                   color=['#FF6B6B', '#FFA07A', '#98D8C8', '#7EC8E3', '#0077B6'],\n",
    "                   edgecolor='black', linewidth=1)\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(phases, rotation=45)\n",
    "    ax4.set_xlabel('Episode Range')\n",
    "    ax4.set_ylabel('Mean Reward ± Std')\n",
    "    ax4.set_title('Performance by Training Phase')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Episodes: {len(rewards)}\")\n",
    "    print(f\"Final 100 Episode Avg: {np.mean(rewards[-100:]):.1f}\")\n",
    "    print(f\"Best Episode Reward: {max(rewards):.0f}\")\n",
    "    print(f\"Final TD Error (avg last 100): {np.mean(td_errors[-100:]):.4f}\")\n",
    "    \n",
    "    # Check if solved\n",
    "    if np.mean(rewards[-100:]) >= 195:\n",
    "        print(\"\\n✓ Environment SOLVED! (Avg reward >= 195 over last 100 episodes)\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Not yet solved. Need {195 - np.mean(rewards[-100:]):.1f} more avg reward.\")\n",
    "\n",
    "# Generate visualization\n",
    "plot_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Discussion\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Variance Reduction**: The critic's baseline reduces variance in policy gradient estimates compared to REINFORCE\n",
    "\n",
    "2. **Bias-Variance Tradeoff**: TD learning introduces some bias through bootstrapping but significantly reduces variance\n",
    "\n",
    "3. **Sample Efficiency**: Actor-Critic methods can learn from incomplete episodes (online learning)\n",
    "\n",
    "### Advantages of Actor-Critic\n",
    "\n",
    "- **Lower variance** than pure policy gradient methods\n",
    "- **Online learning** capability (no need to wait for episode completion)\n",
    "- **Continuous action spaces** are naturally supported\n",
    "- **Function approximation** scales to high-dimensional problems\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Two sets of parameters** to tune (actor and critic learning rates)\n",
    "- **Potential instability** from bootstrapping errors\n",
    "- **Sensitive to hyperparameters** like feature representation\n",
    "\n",
    "### Extensions\n",
    "\n",
    "Modern variants include:\n",
    "- **A2C/A3C**: Asynchronous advantage actor-critic\n",
    "- **PPO**: Proximal Policy Optimization with clipped objectives\n",
    "- **SAC**: Soft Actor-Critic with entropy regularization\n",
    "- **TD3**: Twin Delayed DDPG for continuous control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.\n",
    "\n",
    "2. Konda, V. R., & Tsitsiklis, J. N. (2000). Actor-critic algorithms. *Advances in Neural Information Processing Systems*, 12.\n",
    "\n",
    "3. Mnih, V., et al. (2016). Asynchronous methods for deep reinforcement learning. *International Conference on Machine Learning*.\n",
    "\n",
    "4. Schulman, J., et al. (2017). Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
