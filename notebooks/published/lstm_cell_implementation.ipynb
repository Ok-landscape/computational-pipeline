{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Cell Implementation from Scratch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a specialized type of Recurrent Neural Network (RNN) designed to learn long-term dependencies in sequential data. Introduced by Hochreiter and Schmidhuber in 1997, LSTMs address the vanishing gradient problem that plagues standard RNNs.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### The LSTM Cell Architecture\n",
    "\n",
    "An LSTM cell maintains two state vectors:\n",
    "- **Cell state** $\\mathbf{c}_t$: The long-term memory\n",
    "- **Hidden state** $\\mathbf{h}_t$: The short-term memory (output)\n",
    "\n",
    "The cell uses three gates to control information flow:\n",
    "\n",
    "### 1. Forget Gate\n",
    "\n",
    "The forget gate decides what information to discard from the cell state:\n",
    "\n",
    "$$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_f)$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, $\\mathbf{W}_f$ is the weight matrix, $\\mathbf{h}_{t-1}$ is the previous hidden state, $\\mathbf{x}_t$ is the current input, and $\\mathbf{b}_f$ is the bias.\n",
    "\n",
    "### 2. Input Gate\n",
    "\n",
    "The input gate controls what new information to store:\n",
    "\n",
    "$$\\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_i)$$\n",
    "\n",
    "A candidate cell state is computed:\n",
    "\n",
    "$$\\tilde{\\mathbf{c}}_t = \\tanh(\\mathbf{W}_c \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_c)$$\n",
    "\n",
    "### 3. Cell State Update\n",
    "\n",
    "The cell state is updated by forgetting old information and adding new:\n",
    "\n",
    "$$\\mathbf{c}_t = \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication (Hadamard product).\n",
    "\n",
    "### 4. Output Gate\n",
    "\n",
    "The output gate determines the hidden state output:\n",
    "\n",
    "$$\\mathbf{o}_t = \\sigma(\\mathbf{W}_o \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_o)$$\n",
    "\n",
    "$$\\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)$$\n",
    "\n",
    "## Parameter Dimensions\n",
    "\n",
    "For input dimension $d$ and hidden dimension $n$:\n",
    "- Weight matrices: $\\mathbf{W}_f, \\mathbf{W}_i, \\mathbf{W}_c, \\mathbf{W}_o \\in \\mathbb{R}^{n \\times (n+d)}$\n",
    "- Bias vectors: $\\mathbf{b}_f, \\mathbf{b}_i, \\mathbf{b}_c, \\mathbf{b}_o \\in \\mathbb{R}^n$\n",
    "- Total parameters: $4n(n+d) + 4n = 4n(n+d+1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\n",
    "    \n",
    "    Ïƒ(x) = 1 / (1 + exp(-x))\n",
    "    \n",
    "    Clips input to prevent overflow.\n",
    "    \"\"\"\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Hyperbolic tangent activation function.\"\"\"\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell:\n",
    "    \"\"\"Long Short-Term Memory Cell.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        Dimension of input vector\n",
    "    hidden_dim : int\n",
    "        Dimension of hidden state\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Xavier/Glorot initialization for better gradient flow\n",
    "        scale = np.sqrt(2.0 / (input_dim + hidden_dim))\n",
    "        concat_dim = input_dim + hidden_dim\n",
    "        \n",
    "        # Forget gate parameters\n",
    "        self.W_f = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_f = np.ones(hidden_dim)  # Initialize to 1 for better gradient flow\n",
    "        \n",
    "        # Input gate parameters\n",
    "        self.W_i = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_i = np.zeros(hidden_dim)\n",
    "        \n",
    "        # Candidate cell state parameters\n",
    "        self.W_c = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_c = np.zeros(hidden_dim)\n",
    "        \n",
    "        # Output gate parameters\n",
    "        self.W_o = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_o = np.zeros(hidden_dim)\n",
    "        \n",
    "        # Cache for storing intermediate values (useful for backprop)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        \"\"\"Forward pass of LSTM cell.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_t : ndarray, shape (input_dim,)\n",
    "            Input at time t\n",
    "        h_prev : ndarray, shape (hidden_dim,)\n",
    "            Hidden state at time t-1\n",
    "        c_prev : ndarray, shape (hidden_dim,)\n",
    "            Cell state at time t-1\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        h_t : ndarray, shape (hidden_dim,)\n",
    "            Hidden state at time t\n",
    "        c_t : ndarray, shape (hidden_dim,)\n",
    "            Cell state at time t\n",
    "        \"\"\"\n",
    "        # Concatenate previous hidden state and current input\n",
    "        concat = np.concatenate([h_prev, x_t])\n",
    "        \n",
    "        # Forget gate\n",
    "        f_t = sigmoid(np.dot(self.W_f, concat) + self.b_f)\n",
    "        \n",
    "        # Input gate\n",
    "        i_t = sigmoid(np.dot(self.W_i, concat) + self.b_i)\n",
    "        \n",
    "        # Candidate cell state\n",
    "        c_tilde = tanh(np.dot(self.W_c, concat) + self.b_c)\n",
    "        \n",
    "        # Update cell state\n",
    "        c_t = f_t * c_prev + i_t * c_tilde\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = sigmoid(np.dot(self.W_o, concat) + self.b_o)\n",
    "        \n",
    "        # Hidden state\n",
    "        h_t = o_t * tanh(c_t)\n",
    "        \n",
    "        # Store intermediate values for visualization\n",
    "        self.cache = {\n",
    "            'f_t': f_t,\n",
    "            'i_t': i_t,\n",
    "            'o_t': o_t,\n",
    "            'c_tilde': c_tilde,\n",
    "            'c_t': c_t,\n",
    "            'h_t': h_t\n",
    "        }\n",
    "        \n",
    "        return h_t, c_t\n",
    "    \n",
    "    def forward_sequence(self, X):\n",
    "        \"\"\"Process a sequence of inputs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (seq_len, input_dim)\n",
    "            Input sequence\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        H : ndarray, shape (seq_len, hidden_dim)\n",
    "            Hidden states for all time steps\n",
    "        C : ndarray, shape (seq_len, hidden_dim)\n",
    "            Cell states for all time steps\n",
    "        gates : dict\n",
    "            Gate activations for all time steps\n",
    "        \"\"\"\n",
    "        seq_len = X.shape[0]\n",
    "        \n",
    "        # Initialize states\n",
    "        h_t = np.zeros(self.hidden_dim)\n",
    "        c_t = np.zeros(self.hidden_dim)\n",
    "        \n",
    "        # Storage for outputs\n",
    "        H = np.zeros((seq_len, self.hidden_dim))\n",
    "        C = np.zeros((seq_len, self.hidden_dim))\n",
    "        gates = {'f': [], 'i': [], 'o': [], 'c_tilde': []}\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.forward(X[t], h_t, c_t)\n",
    "            H[t] = h_t\n",
    "            C[t] = c_t\n",
    "            \n",
    "            # Store gate activations\n",
    "            gates['f'].append(self.cache['f_t'].copy())\n",
    "            gates['i'].append(self.cache['i_t'].copy())\n",
    "            gates['o'].append(self.cache['o_t'].copy())\n",
    "            gates['c_tilde'].append(self.cache['c_tilde'].copy())\n",
    "        \n",
    "        # Convert to arrays\n",
    "        for key in gates:\n",
    "            gates[key] = np.array(gates[key])\n",
    "        \n",
    "        return H, C, gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "### Generate Synthetic Sequence Data\n",
    "\n",
    "We'll create a simple sinusoidal signal to demonstrate the LSTM's ability to process sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sequence data\n",
    "seq_len = 50\n",
    "input_dim = 3\n",
    "hidden_dim = 8\n",
    "\n",
    "# Create time-varying input signals\n",
    "t = np.linspace(0, 4 * np.pi, seq_len)\n",
    "X = np.zeros((seq_len, input_dim))\n",
    "X[:, 0] = np.sin(t)                    # Sine wave\n",
    "X[:, 1] = np.cos(t)                    # Cosine wave\n",
    "X[:, 2] = np.sin(2 * t) * 0.5          # Higher frequency component\n",
    "\n",
    "print(f\"Input sequence shape: {X.shape}\")\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Hidden dimension: {hidden_dim}\")\n",
    "print(f\"Sequence length: {seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Run LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM cell\n",
    "lstm = LSTMCell(input_dim, hidden_dim)\n",
    "\n",
    "# Process the sequence\n",
    "H, C, gates = lstm.forward_sequence(X)\n",
    "\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"Hidden states H: {H.shape}\")\n",
    "print(f\"Cell states C: {C.shape}\")\n",
    "print(f\"\\nTotal parameters: {4 * hidden_dim * (hidden_dim + input_dim + 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "### Gate Activations and State Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Input signals\n",
    "ax = axes[0, 0]\n",
    "ax.plot(t, X[:, 0], label='sin(t)', linewidth=2)\n",
    "ax.plot(t, X[:, 1], label='cos(t)', linewidth=2)\n",
    "ax.plot(t, X[:, 2], label='sin(2t)/2', linewidth=2)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Input Signals')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Gate activations (averaged across hidden units)\n",
    "ax = axes[0, 1]\n",
    "ax.plot(t, gates['f'].mean(axis=1), label='Forget gate', linewidth=2)\n",
    "ax.plot(t, gates['i'].mean(axis=1), label='Input gate', linewidth=2)\n",
    "ax.plot(t, gates['o'].mean(axis=1), label='Output gate', linewidth=2)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Average Activation')\n",
    "ax.set_title('Gate Activations (Mean)')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Plot 3: Cell state evolution\n",
    "ax = axes[1, 0]\n",
    "for i in range(min(4, hidden_dim)):\n",
    "    ax.plot(t, C[:, i], label=f'Cell {i}', linewidth=1.5, alpha=0.8)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Cell State')\n",
    "ax.set_title('Cell State Evolution')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Hidden state evolution\n",
    "ax = axes[1, 1]\n",
    "for i in range(min(4, hidden_dim)):\n",
    "    ax.plot(t, H[:, i], label=f'Hidden {i}', linewidth=1.5, alpha=0.8)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Hidden State')\n",
    "ax.set_title('Hidden State Evolution')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Forget gate heatmap\n",
    "ax = axes[2, 0]\n",
    "im = ax.imshow(gates['f'].T, aspect='auto', cmap='RdYlBu_r', \n",
    "               extent=[0, seq_len, hidden_dim, 0])\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Hidden Unit')\n",
    "ax.set_title('Forget Gate Activations')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 6: Cell state heatmap\n",
    "ax = axes[2, 1]\n",
    "im = ax.imshow(C.T, aspect='auto', cmap='RdBu_r',\n",
    "               extent=[0, seq_len, hidden_dim, 0])\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Hidden Unit')\n",
    "ax.set_title('Cell State Values')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Examining Gate Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "print(\"Gate Activation Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Forget gate - Mean: {gates['f'].mean():.3f}, Std: {gates['f'].std():.3f}\")\n",
    "print(f\"Input gate  - Mean: {gates['i'].mean():.3f}, Std: {gates['i'].std():.3f}\")\n",
    "print(f\"Output gate - Mean: {gates['o'].mean():.3f}, Std: {gates['o'].std():.3f}\")\n",
    "\n",
    "print(\"\\nState Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Cell state   - Mean: {C.mean():.3f}, Std: {C.std():.3f}\")\n",
    "print(f\"Hidden state - Mean: {H.mean():.3f}, Std: {H.std():.3f}\")\n",
    "\n",
    "# Correlation between input and hidden state\n",
    "correlations = []\n",
    "for i in range(input_dim):\n",
    "    for j in range(hidden_dim):\n",
    "        corr = np.corrcoef(X[:, i], H[:, j])[0, 1]\n",
    "        correlations.append((i, j, corr))\n",
    "\n",
    "# Find strongest correlations\n",
    "correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "print(\"\\nStrongest Input-Hidden Correlations:\")\n",
    "print(\"=\" * 40)\n",
    "for i, j, corr in correlations[:5]:\n",
    "    print(f\"Input {i} <-> Hidden {j}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Gate Dynamics**: The gates learn to modulate information flow based on the input patterns. The forget gate typically maintains high values (~0.5-1.0) to preserve long-term information.\n",
    "\n",
    "2. **Cell State**: Acts as a memory highway, accumulating relevant information over time. Values can grow unbounded (unlike hidden states which are squashed by tanh).\n",
    "\n",
    "3. **Hidden State**: Represents the filtered output, bounded between -1 and 1 due to the tanh activation.\n",
    "\n",
    "4. **Parameter Efficiency**: With $d=3$ and $n=8$, we have $4 \\times 8 \\times (8+3+1) = 384$ parameters - relatively few for capturing temporal dynamics.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. *Neural Computation*, 9(8), 1735-1780.\n",
    "\n",
    "2. Gers, F. A., Schmidhuber, J., & Cummins, F. (2000). Learning to Forget: Continual Prediction with LSTM. *Neural Computation*, 12(10), 2451-2471.\n",
    "\n",
    "3. Greff, K., et al. (2017). LSTM: A Search Space Odyssey. *IEEE Transactions on Neural Networks and Learning Systems*, 28(10), 2222-2232."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
