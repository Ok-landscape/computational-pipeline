{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture: The Attention Mechanism\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The **Transformer** architecture, introduced by Vaswani et al. in the seminal paper *\"Attention Is All You Need\"* (2017), revolutionized sequence-to-sequence modeling by eliminating recurrence entirely in favor of **self-attention mechanisms**. This notebook provides a rigorous mathematical treatment of the attention mechanism and implements it from scratch.\n",
    "\n",
    "## 2. Theoretical Foundation\n",
    "\n",
    "### 2.1 The Attention Function\n",
    "\n",
    "The core idea of attention is to compute a weighted sum of **values** $V$, where the weights are determined by the compatibility between **queries** $Q$ and **keys** $K$.\n",
    "\n",
    "Given:\n",
    "- Query matrix $Q \\in \\mathbb{R}^{n \\times d_k}$\n",
    "- Key matrix $K \\in \\mathbb{R}^{m \\times d_k}$\n",
    "- Value matrix $V \\in \\mathbb{R}^{m \\times d_v}$\n",
    "\n",
    "The **Scaled Dot-Product Attention** is defined as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "where $d_k$ is the dimension of the keys (and queries).\n",
    "\n",
    "### 2.2 Why Scale by $\\sqrt{d_k}$?\n",
    "\n",
    "For large values of $d_k$, the dot products $q_i \\cdot k_j$ can grow large in magnitude. If we assume that the components of $q$ and $k$ are independent random variables with mean 0 and variance 1, then:\n",
    "\n",
    "$$\\mathbb{E}[q \\cdot k] = 0, \\quad \\text{Var}(q \\cdot k) = d_k$$\n",
    "\n",
    "Large variances push the softmax into regions where gradients are extremely small (saturation). Scaling by $\\sqrt{d_k}$ normalizes the variance to 1:\n",
    "\n",
    "$$\\text{Var}\\left(\\frac{q \\cdot k}{\\sqrt{d_k}}\\right) = 1$$\n",
    "\n",
    "### 2.3 Multi-Head Attention\n",
    "\n",
    "Rather than performing a single attention function, Multi-Head Attention allows the model to jointly attend to information from different representation subspaces:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "\n",
    "where each head is computed as:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "with learned projection matrices:\n",
    "- $W_i^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "- $W_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "- $W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$\n",
    "- $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$\n",
    "\n",
    "### 2.4 Self-Attention\n",
    "\n",
    "In **self-attention**, the queries, keys, and values all come from the same source sequence. For an input sequence $X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$:\n",
    "\n",
    "$$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$$\n",
    "\n",
    "This allows each position in the sequence to attend to all other positions, capturing long-range dependencies without the sequential bottleneck of RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "We now implement the attention mechanism from scratch using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Compute softmax values along the specified axis.\"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute Scaled Dot-Product Attention.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q : ndarray of shape (batch, seq_len_q, d_k)\n",
    "        Query matrix\n",
    "    K : ndarray of shape (batch, seq_len_k, d_k)\n",
    "        Key matrix\n",
    "    V : ndarray of shape (batch, seq_len_k, d_v)\n",
    "        Value matrix\n",
    "    mask : ndarray, optional\n",
    "        Mask to apply (e.g., for causal attention)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : ndarray of shape (batch, seq_len_q, d_v)\n",
    "        Attention output\n",
    "    attention_weights : ndarray of shape (batch, seq_len_q, seq_len_k)\n",
    "        Attention weights\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores: (batch, seq_len_q, seq_len_k)\n",
    "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Compute attention weights\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Compute output\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "print(\"Scaled Dot-Product Attention function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Example: Self-Attention on a Simple Sequence\n",
    "\n",
    "Let us create a synthetic sequence and visualize how self-attention works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "batch_size = 1\n",
    "seq_len = 6\n",
    "d_model = 8\n",
    "d_k = d_v = 4  # Dimension of keys/queries and values\n",
    "\n",
    "# Create a synthetic input sequence (could represent word embeddings)\n",
    "X = np.random.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Initialize projection matrices for self-attention\n",
    "W_Q = np.random.randn(d_model, d_k) * 0.1\n",
    "W_K = np.random.randn(d_model, d_k) * 0.1\n",
    "W_V = np.random.randn(d_model, d_v) * 0.1\n",
    "\n",
    "# Project input to Q, K, V\n",
    "Q = np.matmul(X, W_Q)\n",
    "K = np.matmul(X, W_K)\n",
    "V = np.matmul(X, W_V)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute self-attention\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights (sum to 1 along last axis):\")\n",
    "print(f\"Row sums: {attention_weights[0].sum(axis=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d_model : int\n",
    "        Model dimension\n",
    "    num_heads : int\n",
    "        Number of attention heads\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Initialize projection matrices\n",
    "        scale = np.sqrt(2.0 / (d_model + self.d_k))\n",
    "        self.W_Q = np.random.randn(d_model, d_model) * scale\n",
    "        self.W_K = np.random.randn(d_model, d_model) * scale\n",
    "        self.W_V = np.random.randn(d_model, d_model) * scale\n",
    "        self.W_O = np.random.randn(d_model, d_model) * scale\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k).\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(0, 2, 1, 3)  # (batch, heads, seq_len, d_k)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine heads back to original shape.\"\"\"\n",
    "        batch_size, _, seq_len, _ = x.shape\n",
    "        x = x.transpose(0, 2, 1, 3)  # (batch, seq_len, heads, d_k)\n",
    "        return x.reshape(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of Multi-Head Attention.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : ndarray\n",
    "            Multi-head attention output\n",
    "        attention_weights : ndarray\n",
    "            Attention weights for each head\n",
    "        \"\"\"\n",
    "        # Linear projections\n",
    "        Q = np.matmul(Q, self.W_Q)\n",
    "        K = np.matmul(K, self.W_K)\n",
    "        V = np.matmul(V, self.W_V)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Scaled dot-product attention for each head\n",
    "        d_k = Q.shape[-1]\n",
    "        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = np.where(mask == 0, -1e9, scores)\n",
    "        \n",
    "        attention_weights = softmax(scores, axis=-1)\n",
    "        attention_output = np.matmul(attention_weights, V)\n",
    "        \n",
    "        # Combine heads\n",
    "        output = self.combine_heads(attention_output)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = np.matmul(output, self.W_O)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"MultiHeadAttention class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multi-Head Attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Create input\n",
    "X = np.random.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Initialize and run multi-head attention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "output, attn_weights = mha.forward(X, X, X)  # Self-attention\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  (batch, heads, seq_len_q, seq_len_k)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Causal (Masked) Attention\n",
    "\n",
    "In autoregressive models (like GPT), we use **causal masking** to prevent positions from attending to subsequent positions. This ensures that the prediction for position $i$ can only depend on known outputs at positions less than $i$.\n",
    "\n",
    "The causal mask is a lower-triangular matrix:\n",
    "\n",
    "$$M_{ij} = \\begin{cases} 1 & \\text{if } j \\leq i \\\\ 0 & \\text{if } j > i \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create a causal (lower-triangular) mask.\"\"\"\n",
    "    mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "    return mask\n",
    "\n",
    "# Visualize causal mask\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(causal_mask, cmap='Blues')\n",
    "plt.colorbar(label='Mask Value')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Causal Attention Mask\\n(Lower Triangular)')\n",
    "plt.xticks(range(seq_len))\n",
    "plt.yticks(range(seq_len))\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        plt.text(j, i, int(causal_mask[i, j]), ha='center', va='center', \n",
    "                 color='white' if causal_mask[i, j] > 0.5 else 'black')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate causal attention\n",
    "batch_size = 1\n",
    "seq_len = 6\n",
    "d_k = d_v = 4\n",
    "\n",
    "Q = np.random.randn(batch_size, seq_len, d_k)\n",
    "K = np.random.randn(batch_size, seq_len, d_k)\n",
    "V = np.random.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "# Create causal mask\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "# Compute attention with and without mask\n",
    "output_unmasked, weights_unmasked = scaled_dot_product_attention(Q, K, V)\n",
    "output_masked, weights_masked = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "print(\"Unmasked attention weights (row 0 attends to all):\")\n",
    "print(weights_unmasked[0, 0, :].round(3))\n",
    "print(\"\\nMasked attention weights (row 0 only attends to position 0):\")\n",
    "print(weights_masked[0, 0, :].round(3))\n",
    "print(\"\\nMasked attention weights (last row attends to all previous):\")\n",
    "print(weights_masked[0, -1, :].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of Attention Patterns\n",
    "\n",
    "Let us create a comprehensive visualization comparing attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more structured example with interpretable patterns\n",
    "np.random.seed(123)\n",
    "\n",
    "# Simulate a sentence-like sequence with token labels\n",
    "tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat', '.', '<EOS>']\n",
    "seq_len = len(tokens)\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "\n",
    "# Create embeddings (in practice, these would be learned)\n",
    "# We add some structure: similar words have similar embeddings\n",
    "embeddings = np.random.randn(seq_len, d_model) * 0.5\n",
    "# Make 'The' and 'the' similar\n",
    "embeddings[4] = embeddings[0] + np.random.randn(d_model) * 0.1\n",
    "# Make punctuation distinct\n",
    "embeddings[6] = np.random.randn(d_model) * 2\n",
    "embeddings[7] = np.random.randn(d_model) * 2\n",
    "\n",
    "X = embeddings.reshape(1, seq_len, d_model)\n",
    "\n",
    "# Run multi-head attention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "output, attn_weights = mha.forward(X, X, X)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 10))\n",
    "\n",
    "# Plot attention weights for each head\n",
    "for h in range(num_heads):\n",
    "    row, col = h // 2, h % 2\n",
    "    ax = axes[row, col]\n",
    "    im = ax.imshow(attn_weights[0, h], cmap='viridis', vmin=0, vmax=0.5)\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.set_xlabel('Key (attending to)')\n",
    "    ax.set_ylabel('Query (from)')\n",
    "    ax.set_title(f'Head {h+1}')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "# Plot average attention across all heads\n",
    "ax = axes[1, 0]\n",
    "avg_attn = attn_weights[0].mean(axis=0)\n",
    "im = ax.imshow(avg_attn, cmap='viridis', vmin=0, vmax=0.5)\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "ax.set_yticklabels(tokens)\n",
    "ax.set_xlabel('Key (attending to)')\n",
    "ax.set_ylabel('Query (from)')\n",
    "ax.set_title('Average Across Heads')\n",
    "plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "# Plot causal attention pattern\n",
    "ax = axes[1, 1]\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "_, causal_attn = scaled_dot_product_attention(\n",
    "    X @ mha.W_Q.reshape(d_model, num_heads, -1)[:, 0, :],\n",
    "    X @ mha.W_K.reshape(d_model, num_heads, -1)[:, 0, :],\n",
    "    X @ mha.W_V.reshape(d_model, num_heads, -1)[:, 0, :],\n",
    "    mask=causal_mask\n",
    ")\n",
    "im = ax.imshow(causal_attn[0], cmap='viridis', vmin=0, vmax=0.5)\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "ax.set_yticklabels(tokens)\n",
    "ax.set_xlabel('Key (attending to)')\n",
    "ax.set_ylabel('Query (from)')\n",
    "ax.set_title('Causal (Masked) Attention')\n",
    "plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Transformer Self-Attention Visualization', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Positional Encoding\n",
    "\n",
    "Since attention has no inherent notion of position, Transformers use **positional encodings** to inject sequence order information. The original paper uses sinusoidal encodings:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional encodings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    max_len : int\n",
    "        Maximum sequence length\n",
    "    d_model : int\n",
    "        Model dimension\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    PE : ndarray of shape (max_len, d_model)\n",
    "        Positional encoding matrix\n",
    "    \"\"\"\n",
    "    PE = np.zeros((max_len, d_model))\n",
    "    position = np.arange(max_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "    \n",
    "    PE[:, 0::2] = np.sin(position * div_term)\n",
    "    PE[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return PE\n",
    "\n",
    "# Generate and visualize positional encodings\n",
    "max_len = 100\n",
    "d_model = 64\n",
    "PE = positional_encoding(max_len, d_model)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(PE.T, aspect='auto', cmap='RdBu')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Sinusoidal Positional Encodings')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Computational Complexity Analysis\n",
    "\n",
    "The attention mechanism has complexity:\n",
    "\n",
    "- **Time complexity**: $O(n^2 \\cdot d)$ where $n$ is sequence length and $d$ is dimension\n",
    "- **Space complexity**: $O(n^2)$ for storing attention weights\n",
    "\n",
    "This quadratic scaling limits Transformers to sequences of a few thousand tokens. Various efficient attention variants address this (Linear Attention, Sparse Attention, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate quadratic scaling\n",
    "import time\n",
    "\n",
    "seq_lengths = [64, 128, 256, 512, 1024]\n",
    "d_model = 64\n",
    "times = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    Q = np.random.randn(1, seq_len, d_model)\n",
    "    K = np.random.randn(1, seq_len, d_model)\n",
    "    V = np.random.randn(1, seq_len, d_model)\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _, _ = scaled_dot_product_attention(Q, K, V)\n",
    "    elapsed = (time.time() - start) / 10\n",
    "    times.append(elapsed * 1000)  # Convert to ms\n",
    "    print(f\"Seq length {seq_len:4d}: {elapsed*1000:.2f} ms\")\n",
    "\n",
    "# Plot scaling behavior\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.loglog(seq_lengths, times, 'bo-', markersize=8, linewidth=2, label='Measured')\n",
    "\n",
    "# Theoretical O(n^2) curve\n",
    "theoretical = [times[0] * (n/seq_lengths[0])**2 for n in seq_lengths]\n",
    "plt.loglog(seq_lengths, theoretical, 'r--', linewidth=2, label=r'$O(n^2)$ theoretical')\n",
    "\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.title('Attention Complexity Scaling')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "This notebook covered the essential components of the Transformer attention mechanism:\n",
    "\n",
    "1. **Scaled Dot-Product Attention**: The fundamental operation computing weighted sums of values based on query-key compatibility\n",
    "\n",
    "2. **Multi-Head Attention**: Parallel attention operations that allow the model to attend to different representation subspaces\n",
    "\n",
    "3. **Causal Masking**: Prevents future information leakage in autoregressive models\n",
    "\n",
    "4. **Positional Encodings**: Inject position information into the position-agnostic attention mechanism\n",
    "\n",
    "5. **Computational Complexity**: Quadratic $O(n^2)$ scaling with sequence length\n",
    "\n",
    "### Key Equations\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "\n",
    "The attention mechanism's ability to capture long-range dependencies in a single operation, combined with parallelizability, makes Transformers the dominant architecture for sequence modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Vaswani, A., et al. (2017). *Attention Is All You Need*. NeurIPS.\n",
    "2. Devlin, J., et al. (2019). *BERT: Pre-training of Deep Bidirectional Transformers*. NAACL.\n",
    "3. Brown, T., et al. (2020). *Language Models are Few-Shot Learners*. NeurIPS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
