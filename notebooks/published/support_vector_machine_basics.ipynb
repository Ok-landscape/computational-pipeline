{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Basics\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Support Vector Machines (SVMs) are powerful supervised learning algorithms used for classification and regression tasks. Introduced by Vapnik and colleagues in the 1990s, SVMs are particularly effective in high-dimensional spaces and remain one of the most robust classification methods in machine learning.\n",
    "\n",
    "## 2. Mathematical Foundation\n",
    "\n",
    "### 2.1 Linear Classification\n",
    "\n",
    "Given a training dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n}$ where $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, +1\\}$, a linear classifier seeks a hyperplane:\n",
    "\n",
    "$$\\mathbf{w}^T \\mathbf{x} + b = 0$$\n",
    "\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^d$ is the weight vector (normal to the hyperplane) and $b \\in \\mathbb{R}$ is the bias term.\n",
    "\n",
    "### 2.2 Maximum Margin Principle\n",
    "\n",
    "The key insight of SVM is to find the hyperplane that maximizes the **margin** - the distance between the hyperplane and the nearest training points (support vectors).\n",
    "\n",
    "The distance from a point $\\mathbf{x}_i$ to the hyperplane is:\n",
    "\n",
    "$$\\text{distance} = \\frac{|\\mathbf{w}^T \\mathbf{x}_i + b|}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "For correctly classified points, we require:\n",
    "\n",
    "$$y_i(\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1$$\n",
    "\n",
    "The margin width is $\\frac{2}{\\|\\mathbf{w}\\|}$, so maximizing the margin is equivalent to minimizing $\\|\\mathbf{w}\\|^2$.\n",
    "\n",
    "### 2.3 Optimization Problem\n",
    "\n",
    "The **primal formulation** for hard-margin SVM is:\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "subject to: $y_i(\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i = 1, \\ldots, n$\n",
    "\n",
    "### 2.4 Soft-Margin SVM\n",
    "\n",
    "For non-separable data, we introduce **slack variables** $\\xi_i \\geq 0$:\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_{i=1}^{n}\\xi_i$$\n",
    "\n",
    "subject to: $y_i(\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i$ and $\\xi_i \\geq 0$\n",
    "\n",
    "The parameter $C > 0$ controls the trade-off between maximizing the margin and minimizing classification errors.\n",
    "\n",
    "### 2.5 Dual Formulation and Lagrange Multipliers\n",
    "\n",
    "Using Lagrange multipliers $\\alpha_i \\geq 0$, the dual problem becomes:\n",
    "\n",
    "$$\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j$$\n",
    "\n",
    "subject to: $\\sum_{i=1}^{n} \\alpha_i y_i = 0$ and $0 \\leq \\alpha_i \\leq C$\n",
    "\n",
    "The solution yields:\n",
    "\n",
    "$$\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i$$\n",
    "\n",
    "Points with $\\alpha_i > 0$ are the **support vectors**.\n",
    "\n",
    "### 2.6 Kernel Trick\n",
    "\n",
    "For non-linear classification, we map data to a higher-dimensional feature space via $\\phi(\\mathbf{x})$. The kernel function computes inner products in this space:\n",
    "\n",
    "$$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j)$$\n",
    "\n",
    "Common kernels include:\n",
    "\n",
    "- **Linear:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j$\n",
    "- **Polynomial:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)^d$\n",
    "- **RBF (Gaussian):** $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "We will implement SVM classification on synthetic datasets to demonstrate both linear and non-linear classification scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Generate Synthetic Data\n",
    "\n",
    "We create a linearly separable dataset with two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_data(n_samples=100, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate linearly separable 2D data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples per class\n",
    "    noise : float\n",
    "        Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray of shape (2*n_samples, 2)\n",
    "        Feature matrix\n",
    "    y : ndarray of shape (2*n_samples,)\n",
    "        Labels (+1 or -1)\n",
    "    \"\"\"\n",
    "    # Class +1: centered at (2, 2)\n",
    "    X_pos = np.random.randn(n_samples, 2) * noise + np.array([2, 2])\n",
    "    \n",
    "    # Class -1: centered at (0, 0)\n",
    "    X_neg = np.random.randn(n_samples, 2) * noise + np.array([0, 0])\n",
    "    \n",
    "    X = np.vstack([X_pos, X_neg])\n",
    "    y = np.hstack([np.ones(n_samples), -np.ones(n_samples)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate training data\n",
    "X_train, y_train = generate_linear_data(n_samples=50, noise=0.5)\n",
    "\n",
    "print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Class distribution: +1: {np.sum(y_train == 1)}, -1: {np.sum(y_train == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 SVM Implementation Using Dual Formulation\n",
    "\n",
    "We solve the dual optimization problem using sequential minimal optimization (SMO) principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \"\"\"\n",
    "    Support Vector Machine classifier using the dual formulation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, kernel='linear', gamma=1.0):\n",
    "        \"\"\"\n",
    "        Initialize SVM classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        C : float\n",
    "            Regularization parameter\n",
    "        kernel : str\n",
    "            Kernel type: 'linear' or 'rbf'\n",
    "        gamma : float\n",
    "            Kernel coefficient for RBF\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.b = 0\n",
    "        \n",
    "    def _kernel_function(self, x1, x2):\n",
    "        \"\"\"Compute kernel between two vectors.\"\"\"\n",
    "        if self.kernel == 'linear':\n",
    "            return np.dot(x1, x2)\n",
    "        elif self.kernel == 'rbf':\n",
    "            return np.exp(-self.gamma * np.sum((x1 - x2) ** 2))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel: {self.kernel}\")\n",
    "    \n",
    "    def _compute_kernel_matrix(self, X):\n",
    "        \"\"\"Compute the kernel (Gram) matrix.\"\"\"\n",
    "        n = X.shape[0]\n",
    "        K = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                K[i, j] = self._kernel_function(X[i], X[j])\n",
    "        return K\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the SVM model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target labels (+1 or -1)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        K = self._compute_kernel_matrix(X)\n",
    "        \n",
    "        # Define the dual objective function (to minimize, so negate)\n",
    "        def objective(alpha):\n",
    "            return 0.5 * np.sum((alpha * y)[:, None] * (alpha * y)[None, :] * K) - np.sum(alpha)\n",
    "        \n",
    "        # Gradient of the objective\n",
    "        def gradient(alpha):\n",
    "            return (alpha * y)[:, None] * y[None, :] * K @ np.ones(n_samples) - np.ones(n_samples)\n",
    "        \n",
    "        # Constraints: sum(alpha_i * y_i) = 0\n",
    "        constraints = {'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)}\n",
    "        \n",
    "        # Bounds: 0 <= alpha_i <= C\n",
    "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
    "        \n",
    "        # Initial guess\n",
    "        alpha0 = np.zeros(n_samples)\n",
    "        \n",
    "        # Solve optimization problem\n",
    "        result = minimize(objective, alpha0, method='SLSQP', \n",
    "                         bounds=bounds, constraints=constraints,\n",
    "                         options={'maxiter': 1000, 'ftol': 1e-8})\n",
    "        \n",
    "        self.alpha = result.x\n",
    "        \n",
    "        # Identify support vectors (alpha > threshold)\n",
    "        sv_threshold = 1e-5\n",
    "        sv_indices = self.alpha > sv_threshold\n",
    "        \n",
    "        self.support_vectors = X[sv_indices]\n",
    "        self.support_vector_labels = y[sv_indices]\n",
    "        self.support_vector_alphas = self.alpha[sv_indices]\n",
    "        \n",
    "        # Compute bias term using support vectors on the margin\n",
    "        # These are points where 0 < alpha < C\n",
    "        margin_sv = (self.alpha > sv_threshold) & (self.alpha < self.C - sv_threshold)\n",
    "        \n",
    "        if np.sum(margin_sv) > 0:\n",
    "            self.b = np.mean(\n",
    "                y[margin_sv] - np.sum(\n",
    "                    (self.alpha * y)[:, None] * K[:, margin_sv], axis=0\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: use all support vectors\n",
    "            self.b = np.mean(\n",
    "                self.support_vector_labels - np.array([\n",
    "                    np.sum(self.support_vector_alphas * self.support_vector_labels * \n",
    "                           np.array([self._kernel_function(sv, x) \n",
    "                                    for sv in self.support_vectors]))\n",
    "                    for x in self.support_vectors\n",
    "                ])\n",
    "            )\n",
    "        \n",
    "        # Store training data for predictions\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute the decision function for samples.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        decision : ndarray of shape (n_samples,)\n",
    "            Decision function values\n",
    "        \"\"\"\n",
    "        decision = np.zeros(X.shape[0])\n",
    "        for i, x in enumerate(X):\n",
    "            s = 0\n",
    "            for alpha, y_sv, sv in zip(self.support_vector_alphas, \n",
    "                                       self.support_vector_labels,\n",
    "                                       self.support_vectors):\n",
    "                s += alpha * y_sv * self._kernel_function(sv, x)\n",
    "            decision[i] = s + self.b\n",
    "        return decision\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : ndarray of shape (n_samples,)\n",
    "            Predicted labels\n",
    "        \"\"\"\n",
    "        return np.sign(self.decision_function(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train linear SVM\n",
    "svm_linear = SVM(C=10.0, kernel='linear')\n",
    "svm_linear.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of support vectors: {len(svm_linear.support_vectors)}\")\n",
    "print(f\"Bias term (b): {svm_linear.b:.4f}\")\n",
    "\n",
    "# Compute training accuracy\n",
    "y_pred = svm_linear.predict(X_train)\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "print(f\"Training accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(svm, X, y, ax, title):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary and margins of an SVM.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    svm : SVM\n",
    "        Trained SVM classifier\n",
    "    X : ndarray\n",
    "        Feature matrix\n",
    "    y : ndarray\n",
    "        Labels\n",
    "    ax : matplotlib axes\n",
    "        Axes to plot on\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Compute decision function on grid\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    ax.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.3)\n",
    "    ax.contour(xx, yy, Z, levels=[-1, 0, 1], colors=['blue', 'black', 'red'],\n",
    "               linestyles=['--', '-', '--'], linewidths=[1.5, 2, 1.5])\n",
    "    \n",
    "    # Plot data points\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='o', \n",
    "               edgecolors='k', s=50, label='Class +1')\n",
    "    ax.scatter(X[y == -1, 0], X[y == -1, 1], c='blue', marker='s',\n",
    "               edgecolors='k', s=50, label='Class -1')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    ax.scatter(svm.support_vectors[:, 0], svm.support_vectors[:, 1],\n",
    "               s=200, facecolors='none', edgecolors='green', linewidths=2,\n",
    "               label='Support Vectors')\n",
    "    \n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(loc='upper left', fontsize=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Non-Linear Classification with RBF Kernel\n",
    "\n",
    "We now create a non-linearly separable dataset and apply the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circular_data(n_samples=100, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate circular (non-linearly separable) 2D data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples per class\n",
    "    noise : float\n",
    "        Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray\n",
    "        Feature matrix\n",
    "    y : ndarray\n",
    "        Labels\n",
    "    \"\"\"\n",
    "    # Inner circle (class -1)\n",
    "    r_inner = 1\n",
    "    theta_inner = np.random.uniform(0, 2 * np.pi, n_samples)\n",
    "    X_inner = np.column_stack([\n",
    "        r_inner * np.cos(theta_inner) + np.random.randn(n_samples) * noise,\n",
    "        r_inner * np.sin(theta_inner) + np.random.randn(n_samples) * noise\n",
    "    ])\n",
    "    \n",
    "    # Outer circle (class +1)\n",
    "    r_outer = 3\n",
    "    theta_outer = np.random.uniform(0, 2 * np.pi, n_samples)\n",
    "    X_outer = np.column_stack([\n",
    "        r_outer * np.cos(theta_outer) + np.random.randn(n_samples) * noise,\n",
    "        r_outer * np.sin(theta_outer) + np.random.randn(n_samples) * noise\n",
    "    ])\n",
    "    \n",
    "    X = np.vstack([X_inner, X_outer])\n",
    "    y = np.hstack([-np.ones(n_samples), np.ones(n_samples)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate circular data\n",
    "X_circular, y_circular = generate_circular_data(n_samples=50, noise=0.2)\n",
    "\n",
    "print(f\"Circular data shape: X={X_circular.shape}, y={y_circular.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RBF SVM\n",
    "svm_rbf = SVM(C=10.0, kernel='rbf', gamma=0.5)\n",
    "svm_rbf.fit(X_circular, y_circular)\n",
    "\n",
    "print(f\"Number of support vectors: {len(svm_rbf.support_vectors)}\")\n",
    "\n",
    "# Compute training accuracy\n",
    "y_pred_rbf = svm_rbf.predict(X_circular)\n",
    "accuracy_rbf = np.mean(y_pred_rbf == y_circular)\n",
    "print(f\"Training accuracy: {accuracy_rbf * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Effect of Regularization Parameter C\n",
    "\n",
    "We examine how different values of $C$ affect the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVMs with different C values\n",
    "C_values = [0.1, 1.0, 100.0]\n",
    "svm_models = []\n",
    "\n",
    "for C in C_values:\n",
    "    svm = SVM(C=C, kernel='linear')\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_models.append(svm)\n",
    "    print(f\"C={C}: {len(svm.support_vectors)} support vectors, \"\n",
    "          f\"accuracy={np.mean(svm.predict(X_train) == y_train)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Visualization\n",
    "\n",
    "We create a comprehensive figure showing:\n",
    "1. Linear SVM on separable data\n",
    "2. RBF SVM on circular data\n",
    "3. Effect of regularization parameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Linear SVM\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "plot_decision_boundary(svm_linear, X_train, y_train, ax1, \n",
    "                       'Linear SVM on Separable Data')\n",
    "\n",
    "# Plot 2: RBF SVM\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "plot_decision_boundary(svm_rbf, X_circular, y_circular, ax2,\n",
    "                       'RBF Kernel SVM on Circular Data')\n",
    "\n",
    "# Plot 3-4: Effect of C\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "plot_decision_boundary(svm_models[0], X_train, y_train, ax3,\n",
    "                       f'Linear SVM (C={C_values[0]}, soft margin)')\n",
    "\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "plot_decision_boundary(svm_models[2], X_train, y_train, ax4,\n",
    "                       f'Linear SVM (C={C_values[2]}, hard margin)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "This notebook demonstrated the fundamental concepts of Support Vector Machines:\n",
    "\n",
    "1. **Maximum Margin Principle**: SVMs find the optimal hyperplane by maximizing the margin between classes.\n",
    "\n",
    "2. **Soft-Margin Classification**: The parameter $C$ controls the trade-off between margin width and classification errors, enabling SVMs to handle non-separable data.\n",
    "\n",
    "3. **Kernel Trick**: By using kernel functions like RBF, SVMs can learn non-linear decision boundaries without explicitly computing high-dimensional feature mappings.\n",
    "\n",
    "4. **Support Vectors**: Only a subset of training points (those on or within the margin) determine the decision boundary, making SVMs memory-efficient.\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "- Low $C$ values produce wider margins with more margin violations (soft margin)\n",
    "- High $C$ values produce narrower margins with fewer violations (approaching hard margin)\n",
    "- The RBF kernel can capture complex, non-linear decision boundaries\n",
    "- The number of support vectors indicates model complexity\n",
    "\n",
    "### References\n",
    "\n",
    "1. Cortes, C., & Vapnik, V. (1995). Support-vector networks. *Machine Learning*, 20(3), 273-297.\n",
    "2. Sch√∂lkopf, B., & Smola, A. J. (2002). *Learning with Kernels*. MIT Press.\n",
    "3. Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
