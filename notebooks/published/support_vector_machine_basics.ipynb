{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Basics\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Support Vector Machines (SVMs) are powerful supervised learning models used for classification and regression tasks. Originally developed by Vladimir Vapnik and colleagues in the 1990s, SVMs remain one of the most robust and theoretically grounded machine learning algorithms.\n",
    "\n",
    "The fundamental idea behind SVMs is to find the optimal hyperplane that separates data points of different classes with the maximum margin.\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "### Linear Classification\n",
    "\n",
    "Given a training dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n}$ where $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, +1\\}$, we seek a hyperplane defined by:\n",
    "\n",
    "$$\\mathbf{w}^T \\mathbf{x} + b = 0$$\n",
    "\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^d$ is the normal vector to the hyperplane and $b$ is the bias term.\n",
    "\n",
    "### Decision Function\n",
    "\n",
    "The classification decision for a new point $\\mathbf{x}$ is:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\text{sign}(\\mathbf{w}^T \\mathbf{x} + b)$$\n",
    "\n",
    "### Maximum Margin Classifier\n",
    "\n",
    "The distance from a point $\\mathbf{x}_i$ to the hyperplane is:\n",
    "\n",
    "$$\\text{distance} = \\frac{|\\mathbf{w}^T \\mathbf{x}_i + b|}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "For correctly classified points, we require:\n",
    "\n",
    "$$y_i(\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1$$\n",
    "\n",
    "The margin $\\gamma$ is defined as:\n",
    "\n",
    "$$\\gamma = \\frac{2}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "### Optimization Problem\n",
    "\n",
    "Maximizing the margin is equivalent to minimizing $\\|\\mathbf{w}\\|^2$. The primal optimization problem becomes:\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$y_i(\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1, \\quad i = 1, \\ldots, n$$\n",
    "\n",
    "### Lagrangian Dual Formulation\n",
    "\n",
    "Introducing Lagrange multipliers $\\alpha_i \\geq 0$, the Lagrangian is:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 - \\sum_{i=1}^{n} \\alpha_i \\left[ y_i(\\mathbf{w}^T \\mathbf{x}_i + b) - 1 \\right]$$\n",
    "\n",
    "The dual problem is:\n",
    "\n",
    "$$\\max_{\\boldsymbol{\\alpha}} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\\alpha_i \\geq 0, \\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0$$\n",
    "\n",
    "### Support Vectors\n",
    "\n",
    "Points with $\\alpha_i > 0$ are called **support vectors**. These are the critical points that lie on the margin boundaries and fully determine the decision boundary.\n",
    "\n",
    "The optimal weight vector is:\n",
    "\n",
    "$$\\mathbf{w}^* = \\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's implement a basic SVM from scratch using gradient descent on the primal problem with hinge loss, then compare with scikit-learn's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data\n",
    "\n",
    "We create a linearly separable dataset with two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=100, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate linearly separable 2D data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples per class\n",
    "    noise : float\n",
    "        Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray of shape (2*n_samples, 2)\n",
    "        Feature matrix\n",
    "    y : ndarray of shape (2*n_samples,)\n",
    "        Labels (-1 or +1)\n",
    "    \"\"\"\n",
    "    # Class +1: centered around (2, 2)\n",
    "    X_pos = np.random.randn(n_samples, 2) * noise + np.array([2, 2])\n",
    "    \n",
    "    # Class -1: centered around (0, 0)\n",
    "    X_neg = np.random.randn(n_samples, 2) * noise + np.array([0, 0])\n",
    "    \n",
    "    X = np.vstack([X_pos, X_neg])\n",
    "    y = np.hstack([np.ones(n_samples), -np.ones(n_samples)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate training data\n",
    "X_train, y_train = generate_data(n_samples=50, noise=0.5)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Feature dimension: {X_train.shape[1]}\")\n",
    "print(f\"Class distribution: {np.sum(y_train == 1)} positive, {np.sum(y_train == -1)} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Implementation from Scratch\n",
    "\n",
    "We implement the soft-margin SVM using sub-gradient descent on the primal objective with hinge loss:\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b} \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 + \\frac{1}{n} \\sum_{i=1}^{n} \\max(0, 1 - y_i(\\mathbf{w}^T \\mathbf{x}_i + b))$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \"\"\"\n",
    "    Support Vector Machine classifier using sub-gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float\n",
    "        Step size for gradient descent\n",
    "    lambda_param : float\n",
    "        Regularization parameter\n",
    "    n_iters : int\n",
    "        Number of iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.losses = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the SVM model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target labels (-1 or +1)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        self.losses = []\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            # Compute hinge loss and gradients\n",
    "            margins = y * (np.dot(X, self.w) + self.b)\n",
    "            \n",
    "            # Hinge loss: max(0, 1 - margin)\n",
    "            hinge_loss = np.maximum(0, 1 - margins)\n",
    "            loss = self.lambda_param / 2 * np.dot(self.w, self.w) + np.mean(hinge_loss)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Sub-gradients\n",
    "            # For points violating margin (margin < 1)\n",
    "            mask = margins < 1\n",
    "            \n",
    "            # Gradient of regularization term\n",
    "            dw = self.lambda_param * self.w\n",
    "            db = 0\n",
    "            \n",
    "            # Gradient of hinge loss\n",
    "            if np.any(mask):\n",
    "                dw -= np.mean(X[mask] * y[mask].reshape(-1, 1), axis=0)\n",
    "                db -= np.mean(y[mask])\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Samples to classify\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples,)\n",
    "            Predicted labels (-1 or +1)\n",
    "        \"\"\"\n",
    "        linear_output = np.dot(X, self.w) + self.b\n",
    "        return np.sign(linear_output)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute the signed distance to the hyperplane.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Samples\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        distances : ndarray of shape (n_samples,)\n",
    "            Signed distances\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train SVM\n",
    "svm = SVM(learning_rate=0.01, lambda_param=0.01, n_iters=1000)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred = svm.predict(X_train)\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "\n",
    "print(f\"Training accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Weight vector w: [{svm.w[0]:.4f}, {svm.w[1]:.4f}]\")\n",
    "print(f\"Bias b: {svm.b:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_decision_boundary(X, y, svm, title=\"SVM Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot data points, decision boundary, and margin.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (n_samples, 2)\n",
    "        Feature matrix\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Labels\n",
    "    svm : SVM object\n",
    "        Trained SVM model\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Decision boundary and margins\n",
    "    ax = axes[0]\n",
    "    \n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Compute decision function on mesh\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(xx, yy, Z, levels=[-1, 0, 1], \n",
    "               colors=['blue', 'black', 'red'],\n",
    "               linestyles=['--', '-', '--'],\n",
    "               linewidths=[1.5, 2, 1.5])\n",
    "    \n",
    "    # Fill regions\n",
    "    ax.contourf(xx, yy, Z, levels=[-np.inf, 0, np.inf],\n",
    "                colors=['#FFAAAA', '#AAAAFF'], alpha=0.3)\n",
    "    \n",
    "    # Plot data points\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='o', \n",
    "               s=50, edgecolors='k', label='Class +1')\n",
    "    ax.scatter(X[y == -1, 0], X[y == -1, 1], c='blue', marker='s', \n",
    "               s=50, edgecolors='k', label='Class -1')\n",
    "    \n",
    "    # Highlight support vectors (points near margin)\n",
    "    margins = np.abs(svm.decision_function(X))\n",
    "    sv_mask = margins < 1.1  # Points on or within margin\n",
    "    ax.scatter(X[sv_mask, 0], X[sv_mask, 1], s=150, \n",
    "               facecolors='none', edgecolors='green', linewidths=2,\n",
    "               label='Support Vectors')\n",
    "    \n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training loss\n",
    "    ax = axes[1]\n",
    "    ax.plot(svm.losses, 'b-', linewidth=1.5)\n",
    "    ax.set_xlabel('Iteration', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title('Training Loss Over Iterations', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create visualization\n",
    "fig = plot_svm_decision_boundary(X_train, y_train, svm, \n",
    "                                  title=\"SVM Decision Boundary (Custom Implementation)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin Analysis\n",
    "\n",
    "The margin width is $\\gamma = \\frac{2}{\\|\\mathbf{w}\\|}$. Let's compute this and identify the support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute margin\n",
    "w_norm = np.linalg.norm(svm.w)\n",
    "margin = 2 / w_norm\n",
    "\n",
    "print(f\"||w|| = {w_norm:.4f}\")\n",
    "print(f\"Margin width γ = 2/||w|| = {margin:.4f}\")\n",
    "\n",
    "# Find support vectors (points with margin < 1 + tolerance)\n",
    "distances = svm.decision_function(X_train)\n",
    "functional_margins = y_train * distances\n",
    "\n",
    "# Support vectors are those with functional margin close to 1\n",
    "sv_indices = np.where(functional_margins < 1.1)[0]\n",
    "print(f\"\\nNumber of support vectors: {len(sv_indices)}\")\n",
    "print(f\"Support vector indices: {sv_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-Margin SVM and the Kernel Trick\n",
    "\n",
    "### Soft-Margin SVM\n",
    "\n",
    "For non-linearly separable data, we introduce slack variables $\\xi_i \\geq 0$:\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$y_i(\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0$$\n",
    "\n",
    "The parameter $C > 0$ controls the trade-off between maximizing the margin and minimizing classification errors.\n",
    "\n",
    "### The Kernel Trick\n",
    "\n",
    "For non-linear decision boundaries, we map data to a higher-dimensional space using a feature map $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^D$.\n",
    "\n",
    "The kernel function computes inner products in this space:\n",
    "\n",
    "$$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j)$$\n",
    "\n",
    "Common kernels:\n",
    "\n",
    "- **Linear**: $K(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^T \\mathbf{z}$\n",
    "- **Polynomial**: $K(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^T \\mathbf{z} + c)^d$\n",
    "- **RBF (Gaussian)**: $K(\\mathbf{x}, \\mathbf{z}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{z}\\|^2}{2\\sigma^2}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration with Non-Linear Data\n",
    "\n",
    "Let's generate data that requires a non-linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circular_data(n_samples=100, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate circular data (non-linearly separable).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples per class\n",
    "    noise : float\n",
    "        Standard deviation of noise\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray of shape (2*n_samples, 2)\n",
    "        Feature matrix\n",
    "    y : ndarray of shape (2*n_samples,)\n",
    "        Labels\n",
    "    \"\"\"\n",
    "    # Inner circle (class -1)\n",
    "    r_inner = 1\n",
    "    theta_inner = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "    X_inner = np.column_stack([\n",
    "        r_inner * np.cos(theta_inner) + np.random.randn(n_samples) * noise,\n",
    "        r_inner * np.sin(theta_inner) + np.random.randn(n_samples) * noise\n",
    "    ])\n",
    "    \n",
    "    # Outer circle (class +1)\n",
    "    r_outer = 3\n",
    "    theta_outer = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "    X_outer = np.column_stack([\n",
    "        r_outer * np.cos(theta_outer) + np.random.randn(n_samples) * noise,\n",
    "        r_outer * np.sin(theta_outer) + np.random.randn(n_samples) * noise\n",
    "    ])\n",
    "    \n",
    "    X = np.vstack([X_inner, X_outer])\n",
    "    y = np.hstack([-np.ones(n_samples), np.ones(n_samples)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate circular data\n",
    "X_circular, y_circular = generate_circular_data(n_samples=100, noise=0.3)\n",
    "\n",
    "print(f\"Circular dataset size: {X_circular.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement RBF Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute RBF (Gaussian) kernel matrix.\n",
    "    \n",
    "    K(x, z) = exp(-gamma * ||x - z||^2)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X1 : ndarray of shape (n1, d)\n",
    "    X2 : ndarray of shape (n2, d)\n",
    "    gamma : float\n",
    "        Kernel parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    K : ndarray of shape (n1, n2)\n",
    "        Kernel matrix\n",
    "    \"\"\"\n",
    "    # Compute squared Euclidean distances\n",
    "    sq_dists = np.sum(X1**2, axis=1).reshape(-1, 1) + \\\n",
    "               np.sum(X2**2, axis=1).reshape(1, -1) - \\\n",
    "               2 * np.dot(X1, X2.T)\n",
    "    return np.exp(-gamma * sq_dists)\n",
    "\n",
    "\n",
    "class KernelSVM:\n",
    "    \"\"\"\n",
    "    Kernel SVM using simplified SMO-like optimization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    C : float\n",
    "        Regularization parameter\n",
    "    gamma : float\n",
    "        RBF kernel parameter\n",
    "    n_iters : int\n",
    "        Number of iterations\n",
    "    tol : float\n",
    "        Tolerance for convergence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, gamma=1.0, n_iters=100, tol=1e-3):\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.n_iters = n_iters\n",
    "        self.tol = tol\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the kernel SVM model.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        K = rbf_kernel(X, X, self.gamma)\n",
    "        \n",
    "        # Initialize alphas\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "        self.b = 0\n",
    "        \n",
    "        # Simplified SMO-like optimization\n",
    "        for _ in range(self.n_iters):\n",
    "            alpha_prev = self.alpha.copy()\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                # Compute error for sample i\n",
    "                Ei = self._decision_function_train(K, i) - y[i]\n",
    "                \n",
    "                # Check KKT conditions\n",
    "                if ((y[i] * Ei < -self.tol and self.alpha[i] < self.C) or\n",
    "                    (y[i] * Ei > self.tol and self.alpha[i] > 0)):\n",
    "                    \n",
    "                    # Select random j != i\n",
    "                    j = i\n",
    "                    while j == i:\n",
    "                        j = np.random.randint(0, n_samples)\n",
    "                    \n",
    "                    # Compute error for sample j\n",
    "                    Ej = self._decision_function_train(K, j) - y[j]\n",
    "                    \n",
    "                    # Save old alphas\n",
    "                    alpha_i_old = self.alpha[i]\n",
    "                    alpha_j_old = self.alpha[j]\n",
    "                    \n",
    "                    # Compute bounds\n",
    "                    if y[i] != y[j]:\n",
    "                        L = max(0, self.alpha[j] - self.alpha[i])\n",
    "                        H = min(self.C, self.C + self.alpha[j] - self.alpha[i])\n",
    "                    else:\n",
    "                        L = max(0, self.alpha[i] + self.alpha[j] - self.C)\n",
    "                        H = min(self.C, self.alpha[i] + self.alpha[j])\n",
    "                    \n",
    "                    if L == H:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute eta\n",
    "                    eta = 2 * K[i, j] - K[i, i] - K[j, j]\n",
    "                    if eta >= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Update alpha_j\n",
    "                    self.alpha[j] = alpha_j_old - y[j] * (Ei - Ej) / eta\n",
    "                    self.alpha[j] = np.clip(self.alpha[j], L, H)\n",
    "                    \n",
    "                    # Update alpha_i\n",
    "                    self.alpha[i] = alpha_i_old + y[i] * y[j] * (alpha_j_old - self.alpha[j])\n",
    "                    \n",
    "                    # Update bias\n",
    "                    b1 = self.b - Ei - y[i] * (self.alpha[i] - alpha_i_old) * K[i, i] - \\\n",
    "                         y[j] * (self.alpha[j] - alpha_j_old) * K[i, j]\n",
    "                    b2 = self.b - Ej - y[i] * (self.alpha[i] - alpha_i_old) * K[i, j] - \\\n",
    "                         y[j] * (self.alpha[j] - alpha_j_old) * K[j, j]\n",
    "                    \n",
    "                    if 0 < self.alpha[i] < self.C:\n",
    "                        self.b = b1\n",
    "                    elif 0 < self.alpha[j] < self.C:\n",
    "                        self.b = b2\n",
    "                    else:\n",
    "                        self.b = (b1 + b2) / 2\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(self.alpha - alpha_prev) < self.tol:\n",
    "                break\n",
    "        \n",
    "        # Store support vectors\n",
    "        self.sv_mask = self.alpha > 1e-5\n",
    "        \n",
    "    def _decision_function_train(self, K, i):\n",
    "        \"\"\"Compute decision function for training sample i.\"\"\"\n",
    "        return np.sum(self.alpha * self.y_train * K[:, i]) + self.b\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute decision function for new samples.\"\"\"\n",
    "        K = rbf_kernel(X, self.X_train, self.gamma)\n",
    "        return np.dot(K, self.alpha * self.y_train) + self.b\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        return np.sign(self.decision_function(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train kernel SVM on circular data\n",
    "kernel_svm = KernelSVM(C=10.0, gamma=0.5, n_iters=200)\n",
    "kernel_svm.fit(X_circular, y_circular)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_circular = kernel_svm.predict(X_circular)\n",
    "accuracy_circular = np.mean(y_pred_circular == y_circular)\n",
    "\n",
    "print(f\"Training accuracy (RBF kernel): {accuracy_circular * 100:.2f}%\")\n",
    "print(f\"Number of support vectors: {np.sum(kernel_svm.sv_mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Kernel SVM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernel_svm(X, y, svm, title=\"Kernel SVM\"):\n",
    "    \"\"\"\n",
    "    Plot kernel SVM decision boundary.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Compute decision function\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.6)\n",
    "    ax.contour(xx, yy, Z, levels=[0], colors='black', linewidths=2)\n",
    "    \n",
    "    # Plot data points\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='o',\n",
    "               s=50, edgecolors='k', label='Class +1')\n",
    "    ax.scatter(X[y == -1, 0], X[y == -1, 1], c='blue', marker='s',\n",
    "               s=50, edgecolors='k', label='Class -1')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    ax.scatter(X[svm.sv_mask, 0], X[svm.sv_mask, 1], s=150,\n",
    "               facecolors='none', edgecolors='green', linewidths=2,\n",
    "               label='Support Vectors')\n",
    "    \n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig_kernel = plot_kernel_svm(X_circular, y_circular, kernel_svm,\n",
    "                              title=\"RBF Kernel SVM on Circular Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Summary Visualization\n",
    "\n",
    "Let's create a final comprehensive figure showing both linear and non-linear SVM results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Linear SVM - Decision Boundary\n",
    "ax = axes[0, 0]\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "ax.contour(xx, yy, Z, levels=[-1, 0, 1], colors=['blue', 'black', 'red'],\n",
    "           linestyles=['--', '-', '--'], linewidths=[1.5, 2, 1.5])\n",
    "ax.contourf(xx, yy, Z, levels=[-np.inf, 0, np.inf],\n",
    "            colors=['#FFAAAA', '#AAAAFF'], alpha=0.3)\n",
    "ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n",
    "           c='red', marker='o', s=50, edgecolors='k', label='Class +1')\n",
    "ax.scatter(X_train[y_train == -1, 0], X_train[y_train == -1, 1], \n",
    "           c='blue', marker='s', s=50, edgecolors='k', label='Class -1')\n",
    "margins = np.abs(svm.decision_function(X_train))\n",
    "sv_mask = margins < 1.1\n",
    "ax.scatter(X_train[sv_mask, 0], X_train[sv_mask, 1], s=150,\n",
    "           facecolors='none', edgecolors='green', linewidths=2, label='Support Vectors')\n",
    "ax.set_xlabel('$x_1$', fontsize=11)\n",
    "ax.set_ylabel('$x_2$', fontsize=11)\n",
    "ax.set_title('Linear SVM: Decision Boundary and Margins', fontsize=12)\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Linear SVM - Training Loss\n",
    "ax = axes[0, 1]\n",
    "ax.plot(svm.losses, 'b-', linewidth=1.5)\n",
    "ax.set_xlabel('Iteration', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Linear SVM: Training Loss Convergence', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 3: Kernel SVM - Decision Boundary\n",
    "ax = axes[1, 0]\n",
    "x_min, x_max = X_circular[:, 0].min() - 1, X_circular[:, 0].max() + 1\n",
    "y_min, y_max = X_circular[:, 1].min() - 1, X_circular[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "Z = kernel_svm.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "ax.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.6)\n",
    "ax.contour(xx, yy, Z, levels=[0], colors='black', linewidths=2)\n",
    "ax.scatter(X_circular[y_circular == 1, 0], X_circular[y_circular == 1, 1],\n",
    "           c='red', marker='o', s=50, edgecolors='k', label='Class +1')\n",
    "ax.scatter(X_circular[y_circular == -1, 0], X_circular[y_circular == -1, 1],\n",
    "           c='blue', marker='s', s=50, edgecolors='k', label='Class -1')\n",
    "ax.scatter(X_circular[kernel_svm.sv_mask, 0], X_circular[kernel_svm.sv_mask, 1],\n",
    "           s=150, facecolors='none', edgecolors='green', linewidths=2, label='Support Vectors')\n",
    "ax.set_xlabel('$x_1$', fontsize=11)\n",
    "ax.set_ylabel('$x_2$', fontsize=11)\n",
    "ax.set_title('RBF Kernel SVM: Non-Linear Decision Boundary', fontsize=12)\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Gamma Effect Comparison\n",
    "ax = axes[1, 1]\n",
    "gammas = [0.1, 0.5, 2.0]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for gamma, color in zip(gammas, colors):\n",
    "    temp_svm = KernelSVM(C=10.0, gamma=gamma, n_iters=200)\n",
    "    temp_svm.fit(X_circular, y_circular)\n",
    "    \n",
    "    Z = temp_svm.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    ax.contour(xx, yy, Z, levels=[0], colors=color, linewidths=2,\n",
    "               linestyles='-', label=f'γ = {gamma}')\n",
    "\n",
    "ax.scatter(X_circular[y_circular == 1, 0], X_circular[y_circular == 1, 1],\n",
    "           c='red', marker='o', s=30, alpha=0.5)\n",
    "ax.scatter(X_circular[y_circular == -1, 0], X_circular[y_circular == -1, 1],\n",
    "           c='blue', marker='s', s=30, alpha=0.5)\n",
    "ax.set_xlabel('$x_1$', fontsize=11)\n",
    "ax.set_ylabel('$x_2$', fontsize=11)\n",
    "ax.set_title('Effect of RBF Kernel Parameter γ', fontsize=12)\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Support Vector Machine: Theory and Implementation', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Plot saved to plot.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook demonstrated the fundamental concepts of Support Vector Machines:\n",
    "\n",
    "1. **Maximum Margin Principle**: SVMs find the optimal hyperplane that maximizes the margin between classes, providing robust generalization.\n",
    "\n",
    "2. **Support Vectors**: Only the data points lying on the margin boundaries (support vectors) determine the decision boundary, making SVMs memory-efficient.\n",
    "\n",
    "3. **Dual Formulation**: The Lagrangian dual form enables the kernel trick, allowing efficient computation in high-dimensional feature spaces.\n",
    "\n",
    "4. **Kernel Methods**: The kernel trick enables non-linear decision boundaries by implicitly mapping data to higher-dimensional spaces, with the RBF kernel being particularly versatile.\n",
    "\n",
    "5. **Hyperparameter Sensitivity**: The regularization parameter $C$ and kernel parameters (like $\\gamma$ in RBF) significantly affect model performance and must be tuned carefully.\n",
    "\n",
    "### Key Equations Summary\n",
    "\n",
    "- **Decision function**: $f(\\mathbf{x}) = \\text{sign}(\\mathbf{w}^T \\mathbf{x} + b)$\n",
    "- **Margin width**: $\\gamma = \\frac{2}{\\|\\mathbf{w}\\|}$\n",
    "- **Primal objective**: $\\min \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_i \\xi_i$\n",
    "- **RBF kernel**: $K(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma\\|\\mathbf{x} - \\mathbf{z}\\|^2)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
