{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer Implementation from Scratch\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have revolutionized computer vision and signal processing. At their core lies the **convolutional layer**, which applies learned filters to extract hierarchical features from input data. This notebook provides a rigorous mathematical foundation and a complete implementation of a 2D convolutional layer.\n",
    "\n",
    "## 2. Mathematical Foundation\n",
    "\n",
    "### 2.1 Discrete Convolution\n",
    "\n",
    "The discrete 2D convolution operation between an input signal $\\mathbf{X}$ and a kernel $\\mathbf{K}$ is defined as:\n",
    "\n",
    "$$\n",
    "(\\mathbf{X} * \\mathbf{K})[i, j] = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} \\mathbf{X}[i+m, j+n] \\cdot \\mathbf{K}[m, n]\n",
    "$$\n",
    "\n",
    "where $M \\times N$ is the kernel size.\n",
    "\n",
    "### 2.2 Cross-Correlation in Deep Learning\n",
    "\n",
    "In practice, deep learning frameworks implement **cross-correlation** rather than true convolution (which would flip the kernel). The operation is:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y}[i, j] = \\sum_{m=0}^{K_h-1} \\sum_{n=0}^{K_w-1} \\mathbf{X}[i \\cdot s + m, j \\cdot s + n] \\cdot \\mathbf{W}[m, n] + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{H \\times W}$ is the input feature map\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{K_h \\times K_w}$ is the learnable kernel (weights)\n",
    "- $b \\in \\mathbb{R}$ is the bias term\n",
    "- $s$ is the stride\n",
    "- $\\mathbf{Y} \\in \\mathbb{R}^{H' \\times W'}$ is the output feature map\n",
    "\n",
    "### 2.3 Output Dimensions\n",
    "\n",
    "For an input of size $H \\times W$, kernel size $K_h \\times K_w$, stride $s$, and padding $p$, the output dimensions are:\n",
    "\n",
    "$$\n",
    "H' = \\left\\lfloor \\frac{H + 2p - K_h}{s} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "W' = \\left\\lfloor \\frac{W + 2p - K_w}{s} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "### 2.4 Multi-Channel Convolution\n",
    "\n",
    "For inputs with $C_{in}$ channels and $C_{out}$ output filters:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y}[c_{out}, i, j] = \\sum_{c_{in}=0}^{C_{in}-1} \\sum_{m=0}^{K_h-1} \\sum_{n=0}^{K_w-1} \\mathbf{X}[c_{in}, i \\cdot s + m, j \\cdot s + n] \\cdot \\mathbf{W}[c_{out}, c_{in}, m, n] + b[c_{out}]\n",
    "$$\n",
    "\n",
    "The weight tensor has shape $\\mathbf{W} \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K_h \\times K_w}$.\n",
    "\n",
    "### 2.5 Backpropagation Through Convolution\n",
    "\n",
    "Given the loss gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}}$, the gradients are:\n",
    "\n",
    "**Gradient w.r.t. weights:**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}[m, n]} = \\sum_{i} \\sum_{j} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}[i, j]} \\cdot \\mathbf{X}[i \\cdot s + m, j \\cdot s + n]\n",
    "$$\n",
    "\n",
    "**Gradient w.r.t. input:**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}[i', j']} = \\sum_{i} \\sum_{j} \\sum_{m} \\sum_{n} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}[i, j]} \\cdot \\mathbf{W}[m, n] \\cdot \\delta_{i'+m, i \\cdot s} \\cdot \\delta_{j'+n, j \\cdot s}\n",
    "$$\n",
    "\n",
    "This is equivalent to a **full convolution** of the output gradient with the flipped kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Convolutional Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \"\"\"\n",
    "    2D Convolutional Layer implementation from scratch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    in_channels : int\n",
    "        Number of input channels\n",
    "    out_channels : int\n",
    "        Number of output channels (filters)\n",
    "    kernel_size : int or tuple\n",
    "        Size of the convolutional kernel\n",
    "    stride : int\n",
    "        Stride of the convolution\n",
    "    padding : int\n",
    "        Zero-padding added to both sides of the input\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Xavier/Glorot initialization\n",
    "        fan_in = in_channels * self.kernel_size[0] * self.kernel_size[1]\n",
    "        fan_out = out_channels * self.kernel_size[0] * self.kernel_size[1]\n",
    "        std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "        \n",
    "        # Weights: (out_channels, in_channels, kernel_h, kernel_w)\n",
    "        self.weights = np.random.randn(\n",
    "            out_channels, in_channels, self.kernel_size[0], self.kernel_size[1]\n",
    "        ) * std\n",
    "        \n",
    "        # Bias: (out_channels,)\n",
    "        self.bias = np.zeros(out_channels)\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        self.cache = None\n",
    "        \n",
    "        # Gradients\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "    \n",
    "    def _pad_input(self, x):\n",
    "        \"\"\"Apply zero-padding to input.\"\"\"\n",
    "        if self.padding == 0:\n",
    "            return x\n",
    "        return np.pad(\n",
    "            x,\n",
    "            ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of convolution.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : ndarray of shape (batch_size, in_channels, height, width)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        out : ndarray of shape (batch_size, out_channels, out_height, out_width)\n",
    "        \"\"\"\n",
    "        batch_size, _, h, w = x.shape\n",
    "        kh, kw = self.kernel_size\n",
    "        \n",
    "        # Apply padding\n",
    "        x_padded = self._pad_input(x)\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        h_padded, w_padded = x_padded.shape[2], x_padded.shape[3]\n",
    "        out_h = (h_padded - kh) // self.stride + 1\n",
    "        out_w = (w_padded - kw) // self.stride + 1\n",
    "        \n",
    "        # Initialize output\n",
    "        out = np.zeros((batch_size, self.out_channels, out_h, out_w))\n",
    "        \n",
    "        # Perform convolution\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                h_start = i * self.stride\n",
    "                h_end = h_start + kh\n",
    "                w_start = j * self.stride\n",
    "                w_end = w_start + kw\n",
    "                \n",
    "                # Extract receptive field: (batch_size, in_channels, kh, kw)\n",
    "                receptive_field = x_padded[:, :, h_start:h_end, w_start:w_end]\n",
    "                \n",
    "                # Compute output for all filters\n",
    "                # weights: (out_channels, in_channels, kh, kw)\n",
    "                # result: (batch_size, out_channels)\n",
    "                for k in range(self.out_channels):\n",
    "                    out[:, k, i, j] = np.sum(\n",
    "                        receptive_field * self.weights[k], axis=(1, 2, 3)\n",
    "                    ) + self.bias[k]\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = (x, x_padded)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass of convolution.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        grad_output : ndarray of shape (batch_size, out_channels, out_h, out_w)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        grad_input : ndarray of shape (batch_size, in_channels, height, width)\n",
    "        \"\"\"\n",
    "        x, x_padded = self.cache\n",
    "        batch_size, _, h, w = x.shape\n",
    "        _, _, out_h, out_w = grad_output.shape\n",
    "        kh, kw = self.kernel_size\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.grad_weights = np.zeros_like(self.weights)\n",
    "        self.grad_bias = np.zeros_like(self.bias)\n",
    "        grad_x_padded = np.zeros_like(x_padded)\n",
    "        \n",
    "        # Compute gradients\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                h_start = i * self.stride\n",
    "                h_end = h_start + kh\n",
    "                w_start = j * self.stride\n",
    "                w_end = w_start + kw\n",
    "                \n",
    "                receptive_field = x_padded[:, :, h_start:h_end, w_start:w_end]\n",
    "                \n",
    "                for k in range(self.out_channels):\n",
    "                    # Gradient w.r.t. weights\n",
    "                    self.grad_weights[k] += np.sum(\n",
    "                        receptive_field * grad_output[:, k, i, j].reshape(-1, 1, 1, 1),\n",
    "                        axis=0\n",
    "                    )\n",
    "                    \n",
    "                    # Gradient w.r.t. bias\n",
    "                    self.grad_bias[k] += np.sum(grad_output[:, k, i, j])\n",
    "                    \n",
    "                    # Gradient w.r.t. input\n",
    "                    grad_x_padded[:, :, h_start:h_end, w_start:w_end] += (\n",
    "                        self.weights[k] * grad_output[:, k, i, j].reshape(-1, 1, 1, 1)\n",
    "                    )\n",
    "        \n",
    "        # Remove padding from gradient\n",
    "        if self.padding > 0:\n",
    "            grad_input = grad_x_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "        else:\n",
    "            grad_input = grad_x_padded\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_backward(grad_output, x):\n",
    "    \"\"\"Backward pass for ReLU.\"\"\"\n",
    "    return grad_output * (x > 0)\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Mean Squared Error loss.\"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def mse_loss_backward(y_pred, y_true):\n",
    "    \"\"\"Gradient of MSE loss.\"\"\"\n",
    "    return 2 * (y_pred - y_true) / y_pred.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demonstration\n",
    "\n",
    "### 4.1 Edge Detection Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic image with edges\n",
    "def create_test_image(size=32):\n",
    "    \"\"\"Create a test image with various edge patterns.\"\"\"\n",
    "    img = np.zeros((size, size))\n",
    "    \n",
    "    # Vertical edge\n",
    "    img[:, size//4:size//2] = 1.0\n",
    "    \n",
    "    # Horizontal edge\n",
    "    img[size//4:size//2, :] = np.maximum(img[size//4:size//2, :], 0.5)\n",
    "    \n",
    "    # Diagonal pattern\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if (i + j) > size and (i + j) < int(1.5 * size):\n",
    "                img[i, j] = np.maximum(img[i, j], 0.7)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create test image\n",
    "test_img = create_test_image(32)\n",
    "\n",
    "# Reshape to (batch_size, channels, height, width)\n",
    "x = test_img.reshape(1, 1, 32, 32)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Apply Convolution with Different Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standard edge detection kernels\n",
    "sobel_x = np.array([[-1, 0, 1],\n",
    "                    [-2, 0, 2],\n",
    "                    [-1, 0, 1]], dtype=np.float64)\n",
    "\n",
    "sobel_y = np.array([[-1, -2, -1],\n",
    "                    [ 0,  0,  0],\n",
    "                    [ 1,  2,  1]], dtype=np.float64)\n",
    "\n",
    "laplacian = np.array([[0,  1, 0],\n",
    "                      [1, -4, 1],\n",
    "                      [0,  1, 0]], dtype=np.float64)\n",
    "\n",
    "sharpen = np.array([[ 0, -1,  0],\n",
    "                    [-1,  5, -1],\n",
    "                    [ 0, -1,  0]], dtype=np.float64)\n",
    "\n",
    "# Create convolutional layer\n",
    "conv_layer = Conv2D(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Set predefined kernels\n",
    "conv_layer.weights[0, 0] = sobel_x\n",
    "conv_layer.weights[1, 0] = sobel_y\n",
    "conv_layer.weights[2, 0] = laplacian\n",
    "conv_layer.weights[3, 0] = sharpen\n",
    "\n",
    "# Forward pass\n",
    "output = conv_layer.forward(x)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(test_img, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Sobel X\n",
    "axes[0, 1].imshow(output[0, 0], cmap='RdBu', vmin=-2, vmax=2)\n",
    "axes[0, 1].set_title('Sobel X (Vertical Edges)', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Sobel Y\n",
    "axes[0, 2].imshow(output[0, 1], cmap='RdBu', vmin=-2, vmax=2)\n",
    "axes[0, 2].set_title('Sobel Y (Horizontal Edges)', fontsize=12)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Laplacian\n",
    "axes[1, 0].imshow(output[0, 2], cmap='RdBu', vmin=-2, vmax=2)\n",
    "axes[1, 0].set_title('Laplacian (All Edges)', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Sharpen\n",
    "axes[1, 1].imshow(output[0, 3], cmap='gray')\n",
    "axes[1, 1].set_title('Sharpened', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Edge magnitude (Sobel)\n",
    "edge_magnitude = np.sqrt(output[0, 0]**2 + output[0, 1]**2)\n",
    "axes[1, 2].imshow(edge_magnitude, cmap='hot')\n",
    "axes[1, 2].set_title('Edge Magnitude', fontsize=12)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Convolutional Layer: Edge Detection Filters', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Check\n",
    "\n",
    "Verify the backward pass implementation using numerical gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"Compute numerical gradient using central differences.\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_val = x[idx]\n",
    "        \n",
    "        x[idx] = old_val + eps\n",
    "        fx_plus = f(x)\n",
    "        \n",
    "        x[idx] = old_val - eps\n",
    "        fx_minus = f(x)\n",
    "        \n",
    "        grad[idx] = (fx_plus - fx_minus) / (2 * eps)\n",
    "        x[idx] = old_val\n",
    "        \n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# Create small test case\n",
    "np.random.seed(123)\n",
    "x_test = np.random.randn(2, 3, 8, 8)\n",
    "y_target = np.random.randn(2, 4, 6, 6)\n",
    "\n",
    "# Create layer\n",
    "conv_test = Conv2D(in_channels=3, out_channels=4, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "# Forward and backward\n",
    "output_test = conv_test.forward(x_test)\n",
    "loss = mse_loss(output_test, y_target)\n",
    "grad_output = mse_loss_backward(output_test, y_target)\n",
    "grad_input = conv_test.backward(grad_output)\n",
    "\n",
    "# Numerical gradient check for weights\n",
    "def loss_fn_weights(w):\n",
    "    conv_test.weights = w.reshape(conv_test.weights.shape)\n",
    "    out = conv_test.forward(x_test)\n",
    "    return mse_loss(out, y_target)\n",
    "\n",
    "num_grad_weights = numerical_gradient(loss_fn_weights, conv_test.weights.flatten())\n",
    "num_grad_weights = num_grad_weights.reshape(conv_test.weights.shape)\n",
    "\n",
    "# Recompute analytical gradient\n",
    "output_test = conv_test.forward(x_test)\n",
    "grad_output = mse_loss_backward(output_test, y_target)\n",
    "conv_test.backward(grad_output)\n",
    "\n",
    "# Compare gradients\n",
    "diff = np.abs(num_grad_weights - conv_test.grad_weights)\n",
    "rel_error = np.max(diff / (np.maximum(np.abs(num_grad_weights) + np.abs(conv_test.grad_weights), 1e-8)))\n",
    "\n",
    "print(f\"Gradient Check Results:\")\n",
    "print(f\"  Max absolute difference: {np.max(diff):.2e}\")\n",
    "print(f\"  Relative error: {rel_error:.2e}\")\n",
    "print(f\"  Status: {'PASSED' if rel_error < 1e-5 else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Example: Simple Pattern Detection\n",
    "\n",
    "Train a convolutional layer to detect a specific pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def generate_training_data(n_samples=100, size=16):\n",
    "    \"\"\"Generate images with vertical or horizontal lines.\"\"\"\n",
    "    X = []\n",
    "    Y = []  # 1 for vertical, 0 for horizontal\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        img = np.random.randn(size, size) * 0.1  # Noise\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            # Vertical line\n",
    "            col = np.random.randint(2, size - 2)\n",
    "            img[:, col-1:col+2] += 1.0\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            # Horizontal line\n",
    "            row = np.random.randint(2, size - 2)\n",
    "            img[row-1:row+2, :] += 1.0\n",
    "            Y.append(0)\n",
    "        \n",
    "        X.append(img)\n",
    "    \n",
    "    X = np.array(X).reshape(n_samples, 1, size, size)\n",
    "    Y = np.array(Y).reshape(n_samples, 1)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# Generate data\n",
    "X_train, Y_train = generate_training_data(200, 16)\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Labels shape: {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training loop\n",
    "conv_learn = Conv2D(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=0)\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        x_batch = X_train[batch_idx]\n",
    "        y_batch = Y_train[batch_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        out = conv_learn.forward(x_batch)\n",
    "        \n",
    "        # Global average pooling\n",
    "        pooled = np.mean(out, axis=(2, 3))  # (batch, 2)\n",
    "        \n",
    "        # Simple prediction: difference between two channel means\n",
    "        pred = (pooled[:, 0:1] - pooled[:, 1:2] + 1) / 2  # Normalize to [0, 1]\n",
    "        \n",
    "        # MSE loss\n",
    "        loss = mse_loss(pred, y_batch)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Backward pass\n",
    "        grad_pred = mse_loss_backward(pred, y_batch)\n",
    "        \n",
    "        # Gradient through pooling\n",
    "        grad_pooled = np.zeros_like(pooled)\n",
    "        grad_pooled[:, 0:1] = grad_pred / 2\n",
    "        grad_pooled[:, 1:2] = -grad_pred / 2\n",
    "        \n",
    "        # Gradient through average pooling\n",
    "        grad_out = grad_pooled[:, :, np.newaxis, np.newaxis] / (out.shape[2] * out.shape[3])\n",
    "        grad_out = np.broadcast_to(grad_out, out.shape).copy()\n",
    "        \n",
    "        # Backward through conv\n",
    "        conv_learn.backward(grad_out)\n",
    "        \n",
    "        # Update weights\n",
    "        conv_learn.weights -= learning_rate * conv_learn.grad_weights\n",
    "        conv_learn.bias -= learning_rate * conv_learn.grad_bias\n",
    "    \n",
    "    losses.append(epoch_loss / (len(X_train) // batch_size))\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned filters and training progress\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Training Loss', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learned filter 1\n",
    "im1 = axes[1].imshow(conv_learn.weights[0, 0], cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[1].set_title('Learned Filter 1', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "\n",
    "# Learned filter 2\n",
    "im2 = axes[2].imshow(conv_learn.weights[1, 0], cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[2].set_title('Learned Filter 2', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "\n",
    "plt.suptitle('Convolutional Layer Training Results', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Final plot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Mathematical foundations** of 2D convolution operations in deep learning\n",
    "2. **Complete implementation** of a convolutional layer with forward and backward passes\n",
    "3. **Edge detection** using classical kernels (Sobel, Laplacian)\n",
    "4. **Gradient verification** using numerical differentiation\n",
    "5. **Learning from data** - training the layer to detect patterns\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Convolutional layers learn spatially-invariant features through shared weights\n",
    "- The backward pass involves computing gradients w.r.t. both weights and inputs\n",
    "- Padding preserves spatial dimensions; stride controls downsampling\n",
    "- Xavier initialization helps with gradient flow in deep networks\n",
    "\n",
    "### References\n",
    "\n",
    "1. LeCun, Y., et al. (1998). Gradient-based learning applied to document recognition. *Proceedings of the IEEE*.\n",
    "2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.\n",
    "3. He, K., et al. (2015). Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. *ICCV*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
