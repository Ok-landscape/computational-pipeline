{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Iteration Method\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **Power Iteration** (also known as the power method) is one of the simplest and oldest algorithms for computing the dominant eigenvalue and its corresponding eigenvector of a matrix. Despite its simplicity, it forms the foundation for more sophisticated eigenvalue algorithms and remains relevant in modern applications such as Google's PageRank algorithm.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Eigenvalue Problem\n",
    "\n",
    "Given a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, the eigenvalue problem seeks scalars $\\lambda$ (eigenvalues) and non-zero vectors $\\mathbf{v}$ (eigenvectors) satisfying:\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "### Dominant Eigenvalue\n",
    "\n",
    "If $\\mathbf{A}$ has eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$ ordered such that:\n",
    "\n",
    "$$|\\lambda_1| > |\\lambda_2| \\geq |\\lambda_3| \\geq \\cdots \\geq |\\lambda_n|$$\n",
    "\n",
    "then $\\lambda_1$ is called the **dominant eigenvalue**.\n",
    "\n",
    "### Algorithm Derivation\n",
    "\n",
    "Let $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}$ be a basis of eigenvectors for $\\mathbb{R}^n$. Any initial vector $\\mathbf{x}_0$ can be expressed as:\n",
    "\n",
    "$$\\mathbf{x}_0 = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n$$\n",
    "\n",
    "Applying $\\mathbf{A}$ repeatedly:\n",
    "\n",
    "$$\\mathbf{A}^k\\mathbf{x}_0 = c_1\\lambda_1^k\\mathbf{v}_1 + c_2\\lambda_2^k\\mathbf{v}_2 + \\cdots + c_n\\lambda_n^k\\mathbf{v}_n$$\n",
    "\n",
    "Factoring out $\\lambda_1^k$:\n",
    "\n",
    "$$\\mathbf{A}^k\\mathbf{x}_0 = \\lambda_1^k \\left[ c_1\\mathbf{v}_1 + c_2\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k\\mathbf{v}_2 + \\cdots + c_n\\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k\\mathbf{v}_n \\right]$$\n",
    "\n",
    "Since $|\\lambda_i/\\lambda_1| < 1$ for $i > 1$, as $k \\to \\infty$:\n",
    "\n",
    "$$\\mathbf{A}^k\\mathbf{x}_0 \\approx \\lambda_1^k c_1\\mathbf{v}_1$$\n",
    "\n",
    "The sequence converges to a scalar multiple of the dominant eigenvector $\\mathbf{v}_1$.\n",
    "\n",
    "### Convergence Rate\n",
    "\n",
    "The rate of convergence depends on the ratio $|\\lambda_2/\\lambda_1|$. The error decreases as:\n",
    "\n",
    "$$\\text{Error} \\sim \\mathcal{O}\\left(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|^k\\right)$$\n",
    "\n",
    "### Rayleigh Quotient\n",
    "\n",
    "The eigenvalue estimate is obtained via the **Rayleigh quotient**:\n",
    "\n",
    "$$\\lambda \\approx \\frac{\\mathbf{x}_k^T \\mathbf{A} \\mathbf{x}_k}{\\mathbf{x}_k^T \\mathbf{x}_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "**Power Iteration Algorithm:**\n",
    "\n",
    "1. Choose an initial vector $\\mathbf{x}_0$ (typically random)\n",
    "2. For $k = 0, 1, 2, \\ldots$ until convergence:\n",
    "   - Compute $\\mathbf{y}_{k+1} = \\mathbf{A}\\mathbf{x}_k$\n",
    "   - Normalize: $\\mathbf{x}_{k+1} = \\frac{\\mathbf{y}_{k+1}}{\\|\\mathbf{y}_{k+1}\\|}$\n",
    "   - Estimate eigenvalue: $\\lambda_{k+1} = \\mathbf{x}_{k+1}^T \\mathbf{A} \\mathbf{x}_{k+1}$\n",
    "3. Return $\\lambda$ and $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm, eig\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration(A, x0=None, max_iter=100, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Power Iteration method for finding the dominant eigenvalue and eigenvector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : ndarray\n",
    "        Square matrix (n x n)\n",
    "    x0 : ndarray, optional\n",
    "        Initial guess vector. If None, uses random vector.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations\n",
    "    tol : float\n",
    "        Convergence tolerance for eigenvalue\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    eigenvalue : float\n",
    "        Estimated dominant eigenvalue\n",
    "    eigenvector : ndarray\n",
    "        Estimated dominant eigenvector (normalized)\n",
    "    history : dict\n",
    "        Dictionary containing convergence history\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # Initialize with random vector if not provided\n",
    "    if x0 is None:\n",
    "        x = np.random.randn(n)\n",
    "    else:\n",
    "        x = x0.copy()\n",
    "    \n",
    "    # Normalize initial vector\n",
    "    x = x / norm(x)\n",
    "    \n",
    "    # Storage for convergence history\n",
    "    eigenvalue_history = []\n",
    "    error_history = []\n",
    "    \n",
    "    eigenvalue_old = 0\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        # Power iteration step\n",
    "        y = A @ x\n",
    "        \n",
    "        # Normalize\n",
    "        x_new = y / norm(y)\n",
    "        \n",
    "        # Compute Rayleigh quotient for eigenvalue estimate\n",
    "        eigenvalue = x_new @ A @ x_new\n",
    "        \n",
    "        # Store history\n",
    "        eigenvalue_history.append(eigenvalue)\n",
    "        error = abs(eigenvalue - eigenvalue_old)\n",
    "        error_history.append(error)\n",
    "        \n",
    "        # Check convergence\n",
    "        if error < tol:\n",
    "            break\n",
    "            \n",
    "        x = x_new\n",
    "        eigenvalue_old = eigenvalue\n",
    "    \n",
    "    history = {\n",
    "        'eigenvalues': eigenvalue_history,\n",
    "        'errors': error_history,\n",
    "        'iterations': k + 1\n",
    "    }\n",
    "    \n",
    "    return eigenvalue, x_new, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Symmetric Positive Definite Matrix\n",
    "\n",
    "We first test on a well-conditioned symmetric positive definite matrix with distinct eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a symmetric positive definite matrix with known eigenvalues\n",
    "# A = Q * D * Q^T where D is diagonal with eigenvalues\n",
    "n = 5\n",
    "\n",
    "# Define eigenvalues (dominant eigenvalue is 10)\n",
    "eigenvalues_true = np.array([10.0, 5.0, 2.0, 1.0, 0.5])\n",
    "\n",
    "# Create random orthogonal matrix Q\n",
    "Q, _ = np.linalg.qr(np.random.randn(n, n))\n",
    "\n",
    "# Construct matrix A\n",
    "A = Q @ np.diag(eigenvalues_true) @ Q.T\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"\\nTrue dominant eigenvalue: {eigenvalues_true[0]}\")\n",
    "print(f\"Eigenvalue ratio |λ₂/λ₁|: {eigenvalues_true[1]/eigenvalues_true[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run power iteration\n",
    "eigenvalue_est, eigenvector_est, history = power_iteration(A, max_iter=50)\n",
    "\n",
    "print(f\"Estimated dominant eigenvalue: {eigenvalue_est:.10f}\")\n",
    "print(f\"True dominant eigenvalue: {eigenvalues_true[0]:.10f}\")\n",
    "print(f\"Absolute error: {abs(eigenvalue_est - eigenvalues_true[0]):.2e}\")\n",
    "print(f\"Iterations to converge: {history['iterations']}\")\n",
    "\n",
    "# Verify eigenvector\n",
    "residual = norm(A @ eigenvector_est - eigenvalue_est * eigenvector_est)\n",
    "print(f\"\\nEigenvector residual ||Av - λv||: {residual:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Convergence Analysis\n",
    "\n",
    "We analyze how the convergence rate depends on the eigenvalue ratio $|\\lambda_2/\\lambda_1|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different eigenvalue ratios\n",
    "ratios = [0.9, 0.7, 0.5, 0.3, 0.1]\n",
    "results = {}\n",
    "\n",
    "for ratio in ratios:\n",
    "    # Create matrix with specified ratio\n",
    "    eigenvalues_test = np.array([10.0, 10.0*ratio, 1.0, 0.5, 0.1])\n",
    "    A_test = Q @ np.diag(eigenvalues_test) @ Q.T\n",
    "    \n",
    "    # Run power iteration with more iterations to see convergence\n",
    "    _, _, hist = power_iteration(A_test, max_iter=100, tol=1e-12)\n",
    "    results[ratio] = hist\n",
    "\n",
    "print(\"Iterations to converge for different |λ₂/λ₁| ratios:\")\n",
    "for ratio, hist in results.items():\n",
    "    print(f\"  Ratio {ratio}: {hist['iterations']} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Application to Covariance Matrix (PCA)\n",
    "\n",
    "Power iteration is used in Principal Component Analysis to find the direction of maximum variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D data with clear principal direction\n",
    "n_samples = 200\n",
    "\n",
    "# Create correlated data\n",
    "mean = [0, 0]\n",
    "cov = [[3, 2], [2, 2]]  # Covariance matrix\n",
    "data = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "\n",
    "# Compute sample covariance matrix\n",
    "data_centered = data - data.mean(axis=0)\n",
    "cov_matrix = (data_centered.T @ data_centered) / (n_samples - 1)\n",
    "\n",
    "print(\"Sample covariance matrix:\")\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find first principal component using power iteration\n",
    "pc1_eigenvalue, pc1_direction, _ = power_iteration(cov_matrix)\n",
    "\n",
    "# Compare with numpy's eig\n",
    "eigenvalues_np, eigenvectors_np = eig(cov_matrix)\n",
    "idx = np.argmax(eigenvalues_np)\n",
    "\n",
    "print(f\"Power iteration - eigenvalue: {pc1_eigenvalue:.6f}\")\n",
    "print(f\"NumPy eig - eigenvalue: {eigenvalues_np[idx]:.6f}\")\n",
    "print(f\"\\nPower iteration - direction: {pc1_direction}\")\n",
    "print(f\"NumPy eig - direction: {eigenvectors_np[:, idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We create a comprehensive visualization showing:\n",
    "1. Convergence of eigenvalue estimates\n",
    "2. Error decay for different eigenvalue ratios\n",
    "3. PCA application with principal direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Eigenvalue convergence\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['eigenvalues'], 'b-o', markersize=4, label='Estimated')\n",
    "ax1.axhline(y=eigenvalues_true[0], color='r', linestyle='--', label='True value')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Eigenvalue estimate')\n",
    "ax1.set_title('Convergence of Dominant Eigenvalue')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error decay (log scale)\n",
    "ax2 = axes[0, 1]\n",
    "for ratio in ratios:\n",
    "    errors = results[ratio]['errors']\n",
    "    ax2.semilogy(errors, label=f'|λ₂/λ₁| = {ratio}')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('|λₖ - λₖ₋₁|')\n",
    "ax2.set_title('Convergence Rate vs Eigenvalue Ratio')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: PCA visualization\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(data[:, 0], data[:, 1], alpha=0.5, s=20, label='Data')\n",
    "\n",
    "# Plot principal direction\n",
    "origin = data.mean(axis=0)\n",
    "scale = 3 * np.sqrt(pc1_eigenvalue)\n",
    "ax3.arrow(origin[0], origin[1], \n",
    "          scale*pc1_direction[0], scale*pc1_direction[1],\n",
    "          head_width=0.2, head_length=0.1, fc='red', ec='red',\n",
    "          label='1st Principal Component')\n",
    "ax3.set_xlabel('x₁')\n",
    "ax3.set_ylabel('x₂')\n",
    "ax3.set_title('PCA: First Principal Component via Power Iteration')\n",
    "ax3.legend()\n",
    "ax3.axis('equal')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Theoretical vs observed convergence rate\n",
    "ax4 = axes[1, 1]\n",
    "theoretical_rate = eigenvalues_true[1] / eigenvalues_true[0]\n",
    "iterations = np.arange(1, len(history['errors']) + 1)\n",
    "\n",
    "# Normalize errors for comparison\n",
    "normalized_errors = np.array(history['errors']) / history['errors'][0]\n",
    "theoretical_decay = theoretical_rate ** iterations\n",
    "\n",
    "ax4.semilogy(iterations, normalized_errors, 'b-o', markersize=4, label='Observed')\n",
    "ax4.semilogy(iterations, theoretical_decay, 'r--', label=f'Theoretical O((λ₂/λ₁)ᵏ)')\n",
    "ax4.set_xlabel('Iteration k')\n",
    "ax4.set_ylabel('Normalized error')\n",
    "ax4.set_title('Convergence: Theory vs Practice')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and Extensions\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Requires dominant eigenvalue**: The method fails or converges slowly if $|\\lambda_1| = |\\lambda_2|$\n",
    "2. **Only finds one eigenvalue**: Cannot directly find other eigenvalues\n",
    "3. **Sensitive to initial vector**: If $c_1 = 0$, the method converges to the wrong eigenvector\n",
    "\n",
    "### Extensions\n",
    "\n",
    "1. **Inverse Power Iteration**: Apply power iteration to $\\mathbf{A}^{-1}$ to find the smallest eigenvalue\n",
    "2. **Shifted Inverse Iteration**: Use $(\\mathbf{A} - \\sigma\\mathbf{I})^{-1}$ to find eigenvalue closest to $\\sigma$\n",
    "3. **Deflation**: Find subsequent eigenvalues by deflating the matrix\n",
    "4. **QR Algorithm**: Modern method based on repeated power iteration concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Power Iteration method is a fundamental algorithm in numerical linear algebra:\n",
    "\n",
    "- **Simplicity**: Easy to implement and understand\n",
    "- **Convergence**: Linear convergence with rate $|\\lambda_2/\\lambda_1|$\n",
    "- **Applications**: PageRank, PCA, stability analysis\n",
    "- **Foundation**: Forms the basis for more advanced methods (QR algorithm, Arnoldi iteration)\n",
    "\n",
    "Despite its limitations, power iteration remains valuable for:\n",
    "- Large sparse matrices where only the dominant eigenvalue is needed\n",
    "- Educational purposes to understand eigenvalue computation\n",
    "- As a building block for more sophisticated algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
