{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Linear regression is one of the most fundamental algorithms in statistical learning and machine learning. It models the relationship between a dependent variable $y$ and one or more independent variables $\\mathbf{x}$ by fitting a linear equation to observed data.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### The Model\n",
    "\n",
    "For simple linear regression with one predictor, the model is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $y$ is the dependent variable (response)\n",
    "- $x$ is the independent variable (predictor)\n",
    "- $\\beta_0$ is the y-intercept\n",
    "- $\\beta_1$ is the slope\n",
    "- $\\epsilon$ is the error term (assumed to be normally distributed with mean 0)\n",
    "\n",
    "### Ordinary Least Squares (OLS)\n",
    "\n",
    "The goal is to find the parameters $\\beta_0$ and $\\beta_1$ that minimize the sum of squared residuals:\n",
    "\n",
    "$$\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2$$\n",
    "\n",
    "### Derivation of OLS Estimators\n",
    "\n",
    "Taking partial derivatives and setting them to zero:\n",
    "\n",
    "$$\\frac{\\partial \\text{SSE}}{\\partial \\beta_0} = -2\\sum_{i=1}^{n}(y_i - \\beta_0 - \\beta_1 x_i) = 0$$\n",
    "\n",
    "$$\\frac{\\partial \\text{SSE}}{\\partial \\beta_1} = -2\\sum_{i=1}^{n}x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0$$\n",
    "\n",
    "Solving these normal equations yields the closed-form solutions:\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}$$\n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the sample means.\n",
    "\n",
    "### Matrix Formulation\n",
    "\n",
    "For multiple linear regression, the model can be expressed in matrix form:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$$\n",
    "\n",
    "The OLS estimator becomes:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "### Coefficient of Determination ($R^2$)\n",
    "\n",
    "The $R^2$ statistic measures the proportion of variance in $y$ explained by the model:\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$\n",
    "\n",
    "where $\\text{SST}$ is the total sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let us now implement linear regression from scratch using only NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data\n",
    "\n",
    "We create a dataset with a known linear relationship plus Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters\n",
    "true_beta_0 = 2.5  # Intercept\n",
    "true_beta_1 = 1.8  # Slope\n",
    "noise_std = 1.5    # Standard deviation of noise\n",
    "\n",
    "# Generate data\n",
    "n_samples = 100\n",
    "X = np.random.uniform(0, 10, n_samples)\n",
    "epsilon = np.random.normal(0, noise_std, n_samples)\n",
    "y = true_beta_0 + true_beta_1 * X + epsilon\n",
    "\n",
    "print(f\"Generated {n_samples} data points\")\n",
    "print(f\"True parameters: β₀ = {true_beta_0}, β₁ = {true_beta_1}\")\n",
    "print(f\"X range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"y range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Class from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionOLS:\n",
    "    \"\"\"\n",
    "    Linear Regression using Ordinary Least Squares.\n",
    "    \n",
    "    Implements both the analytical closed-form solution\n",
    "    and the matrix formulation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.beta_0 = None  # Intercept\n",
    "        self.beta_1 = None  # Slope (for simple regression)\n",
    "        self.coefficients = None  # Full coefficient vector\n",
    "        \n",
    "    def fit_simple(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit simple linear regression using closed-form OLS formulas.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples,)\n",
    "            Training data (single feature)\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        n = len(X)\n",
    "        \n",
    "        # Calculate means\n",
    "        x_mean = np.mean(X)\n",
    "        y_mean = np.mean(y)\n",
    "        \n",
    "        # Calculate slope using covariance formula\n",
    "        # β₁ = Σ(xᵢ - x̄)(yᵢ - ȳ) / Σ(xᵢ - x̄)²\n",
    "        numerator = np.sum((X - x_mean) * (y - y_mean))\n",
    "        denominator = np.sum((X - x_mean) ** 2)\n",
    "        \n",
    "        self.beta_1 = numerator / denominator\n",
    "        \n",
    "        # Calculate intercept\n",
    "        # β₀ = ȳ - β₁x̄\n",
    "        self.beta_0 = y_mean - self.beta_1 * x_mean\n",
    "        \n",
    "        self.coefficients = np.array([self.beta_0, self.beta_1])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_matrix(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear regression using matrix formulation.\n",
    "        \n",
    "        β = (X'X)⁻¹X'y\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples,) or (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Reshape X if 1D\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        \n",
    "        # Add column of ones for intercept (design matrix)\n",
    "        n_samples = X.shape[0]\n",
    "        X_design = np.column_stack([np.ones(n_samples), X])\n",
    "        \n",
    "        # Compute OLS estimator: β = (X'X)⁻¹X'y\n",
    "        XtX = X_design.T @ X_design\n",
    "        XtX_inv = np.linalg.inv(XtX)\n",
    "        Xty = X_design.T @ y\n",
    "        \n",
    "        self.coefficients = XtX_inv @ Xty\n",
    "        self.beta_0 = self.coefficients[0]\n",
    "        self.beta_1 = self.coefficients[1] if len(self.coefficients) > 1 else None\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the fitted model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Samples to predict\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        if X.ndim == 1:\n",
    "            return self.beta_0 + self.beta_1 * X\n",
    "        else:\n",
    "            X_design = np.column_stack([np.ones(X.shape[0]), X])\n",
    "            return X_design @ self.coefficients\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R² (coefficient of determination).\n",
    "        \n",
    "        R² = 1 - SSE/SST\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Test samples\n",
    "        y : array-like\n",
    "            True values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        r2 : float\n",
    "            Coefficient of determination\n",
    "        \"\"\"\n",
    "        y = np.asarray(y)\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Sum of squared errors (residuals)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        \n",
    "        # Total sum of squares\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        \n",
    "        return 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    def residuals(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate residuals (errors).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Samples\n",
    "        y : array-like\n",
    "            True values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        residuals : array\n",
    "            y - ŷ\n",
    "        \"\"\"\n",
    "        return np.asarray(y) - self.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit model using simple closed-form solution\n",
    "model_simple = LinearRegressionOLS()\n",
    "model_simple.fit_simple(X, y)\n",
    "\n",
    "print(\"Simple OLS Formula Results:\")\n",
    "print(f\"  Estimated β₀ (intercept): {model_simple.beta_0:.4f}\")\n",
    "print(f\"  Estimated β₁ (slope):     {model_simple.beta_1:.4f}\")\n",
    "print(f\"  True β₀: {true_beta_0}, True β₁: {true_beta_1}\")\n",
    "print(f\"  R² score: {model_simple.score(X, y):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Create and fit model using matrix formulation\n",
    "model_matrix = LinearRegressionOLS()\n",
    "model_matrix.fit_matrix(X, y)\n",
    "\n",
    "print(\"Matrix Formulation Results:\")\n",
    "print(f\"  Estimated β₀ (intercept): {model_matrix.beta_0:.4f}\")\n",
    "print(f\"  Estimated β₁ (slope):     {model_matrix.beta_1:.4f}\")\n",
    "print(f\"  R² score: {model_matrix.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Data and Regression Line\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(X, y, alpha=0.6, edgecolors='black', linewidth=0.5, label='Data points')\n",
    "\n",
    "# Regression line\n",
    "X_line = np.linspace(X.min(), X.max(), 100)\n",
    "y_pred_line = model_simple.predict(X_line)\n",
    "ax1.plot(X_line, y_pred_line, 'r-', linewidth=2, \n",
    "         label=f'Fitted: $y = {model_simple.beta_0:.2f} + {model_simple.beta_1:.2f}x$')\n",
    "\n",
    "# True line for comparison\n",
    "y_true_line = true_beta_0 + true_beta_1 * X_line\n",
    "ax1.plot(X_line, y_true_line, 'g--', linewidth=2, alpha=0.7,\n",
    "         label=f'True: $y = {true_beta_0} + {true_beta_1}x$')\n",
    "\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$y$')\n",
    "ax1.set_title('Linear Regression Fit')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Residuals vs Fitted Values\n",
    "ax2 = axes[0, 1]\n",
    "y_pred = model_simple.predict(X)\n",
    "residuals = model_simple.residuals(X, y)\n",
    "\n",
    "ax2.scatter(y_pred, residuals, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
    "ax2.set_xlabel('Fitted Values ($\\\\hat{y}$)')\n",
    "ax2.set_ylabel('Residuals ($y - \\\\hat{y}$)')\n",
    "ax2.set_title('Residuals vs Fitted Values')\n",
    "\n",
    "# Plot 3: Histogram of Residuals\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(residuals, bins=20, edgecolor='black', alpha=0.7, density=True)\n",
    "\n",
    "# Overlay normal distribution\n",
    "from scipy import stats\n",
    "x_norm = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "ax3.plot(x_norm, stats.norm.pdf(x_norm, 0, np.std(residuals)), \n",
    "         'r-', linewidth=2, label='Normal PDF')\n",
    "\n",
    "ax3.set_xlabel('Residuals')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Distribution of Residuals')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Q-Q Plot\n",
    "ax4 = axes[1, 1]\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax4)\n",
    "ax4.set_title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Plot saved to 'plot.png'\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional statistics\n",
    "n = len(y)\n",
    "p = 2  # Number of parameters (β₀ and β₁)\n",
    "\n",
    "# Predictions and residuals\n",
    "y_pred = model_simple.predict(X)\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Sum of squares\n",
    "SSE = np.sum(residuals ** 2)\n",
    "SST = np.sum((y - np.mean(y)) ** 2)\n",
    "SSR = SST - SSE  # Regression sum of squares\n",
    "\n",
    "# Mean squared error\n",
    "MSE = SSE / (n - p)\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "# Standard error of coefficients\n",
    "X_design = np.column_stack([np.ones(n), X])\n",
    "var_beta = MSE * np.linalg.inv(X_design.T @ X_design)\n",
    "se_beta_0 = np.sqrt(var_beta[0, 0])\n",
    "se_beta_1 = np.sqrt(var_beta[1, 1])\n",
    "\n",
    "# t-statistics\n",
    "t_beta_0 = model_simple.beta_0 / se_beta_0\n",
    "t_beta_1 = model_simple.beta_1 / se_beta_1\n",
    "\n",
    "# p-values (two-tailed)\n",
    "p_value_beta_0 = 2 * (1 - stats.t.cdf(abs(t_beta_0), n - p))\n",
    "p_value_beta_1 = 2 * (1 - stats.t.cdf(abs(t_beta_1), n - p))\n",
    "\n",
    "# F-statistic\n",
    "MSR = SSR / (p - 1)\n",
    "F_stat = MSR / MSE\n",
    "p_value_F = 1 - stats.f.cdf(F_stat, p - 1, n - p)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REGRESSION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSample size: {n}\")\n",
    "print(f\"R²: {model_simple.score(X, y):.4f}\")\n",
    "print(f\"Adjusted R²: {1 - (1 - model_simple.score(X, y)) * (n - 1) / (n - p):.4f}\")\n",
    "print(f\"RMSE: {RMSE:.4f}\")\n",
    "print(f\"\\nF-statistic: {F_stat:.4f} (p-value: {p_value_F:.4e})\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"COEFFICIENTS\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Parameter':<12} {'Estimate':<12} {'Std. Error':<12} {'t-value':<12} {'p-value':<12}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'β₀ (int.)':<12} {model_simple.beta_0:<12.4f} {se_beta_0:<12.4f} {t_beta_0:<12.4f} {p_value_beta_0:<12.4e}\")\n",
    "print(f\"{'β₁ (slope)':<12} {model_simple.beta_1:<12.4f} {se_beta_1:<12.4f} {t_beta_1:<12.4f} {p_value_beta_1:<12.4e}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Confidence intervals (95%)\n",
    "alpha = 0.05\n",
    "t_crit = stats.t.ppf(1 - alpha/2, n - p)\n",
    "\n",
    "ci_beta_0 = (model_simple.beta_0 - t_crit * se_beta_0, \n",
    "             model_simple.beta_0 + t_crit * se_beta_0)\n",
    "ci_beta_1 = (model_simple.beta_1 - t_crit * se_beta_1, \n",
    "             model_simple.beta_1 + t_crit * se_beta_1)\n",
    "\n",
    "print(f\"\\n95% Confidence Intervals:\")\n",
    "print(f\"  β₀: [{ci_beta_0[0]:.4f}, {ci_beta_0[1]:.4f}]\")\n",
    "print(f\"  β₁: [{ci_beta_1[0]:.4f}, {ci_beta_1[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Implementation\n",
    "\n",
    "As an alternative to the closed-form solution, we can also find the optimal parameters using gradient descent. This iterative approach is particularly useful when dealing with large datasets or when extending to more complex models.\n",
    "\n",
    "The gradients of the cost function $J(\\beta_0, \\beta_1) = \\frac{1}{2n}\\sum_{i=1}^{n}(y_i - \\beta_0 - \\beta_1 x_i)^2$ are:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_0} = -\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\beta_0 - \\beta_1 x_i)$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_1} = -\\frac{1}{n}\\sum_{i=1}^{n}x_i(y_i - \\beta_0 - \\beta_1 x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    \"\"\"\n",
    "    Linear Regression using Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.beta_0 = None\n",
    "        self.beta_1 = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit model using gradient descent.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        n = len(X)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.beta_0 = 0\n",
    "        self.beta_1 = 0\n",
    "        self.cost_history = []\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            # Predictions\n",
    "            y_pred = self.beta_0 + self.beta_1 * X\n",
    "            \n",
    "            # Compute cost (MSE)\n",
    "            cost = np.mean((y - y_pred) ** 2) / 2\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Compute gradients\n",
    "            d_beta_0 = -np.mean(y - y_pred)\n",
    "            d_beta_1 = -np.mean(X * (y - y_pred))\n",
    "            \n",
    "            # Update parameters\n",
    "            self.beta_0 -= self.learning_rate * d_beta_0\n",
    "            self.beta_1 -= self.learning_rate * d_beta_1\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.beta_0 + self.beta_1 * np.asarray(X)\n",
    "\n",
    "\n",
    "# Fit using gradient descent\n",
    "model_gd = LinearRegressionGD(learning_rate=0.01, n_iterations=1000)\n",
    "model_gd.fit(X, y)\n",
    "\n",
    "print(\"Gradient Descent Results:\")\n",
    "print(f\"  Estimated β₀: {model_gd.beta_0:.4f}\")\n",
    "print(f\"  Estimated β₁: {model_gd.beta_1:.4f}\")\n",
    "print(f\"  Final cost: {model_gd.cost_history[-1]:.4f}\")\n",
    "\n",
    "# Plot convergence\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(model_gd.cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (MSE/2)')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the implementation of linear regression from scratch using:\n",
    "\n",
    "1. **Closed-form OLS solution** - Direct computation using the analytical formulas derived from minimizing squared errors\n",
    "2. **Matrix formulation** - Using the normal equation $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$\n",
    "3. **Gradient descent** - Iterative optimization approach\n",
    "\n",
    "All three methods converge to the same solution, validating our implementations. The closed-form solution is computationally efficient for small to medium datasets, while gradient descent becomes preferable for very large datasets or when extending to regularized regression.\n",
    "\n",
    "Key takeaways:\n",
    "- Linear regression minimizes the sum of squared residuals\n",
    "- The $R^2$ statistic measures model fit quality\n",
    "- Residual analysis helps validate model assumptions (normality, homoscedasticity)\n",
    "- Standard errors enable hypothesis testing and confidence interval construction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
