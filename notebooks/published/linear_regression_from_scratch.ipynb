{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Linear regression is one of the most fundamental algorithms in statistical learning and machine learning. It models the relationship between a dependent variable $y$ and one or more independent variables $\\mathbf{x}$ by fitting a linear equation to observed data.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Simple Linear Regression\n",
    "\n",
    "For a single predictor variable, the linear regression model is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $y$ is the dependent variable (response)\n",
    "- $x$ is the independent variable (predictor)\n",
    "- $\\beta_0$ is the y-intercept\n",
    "- $\\beta_1$ is the slope coefficient\n",
    "- $\\epsilon$ is the error term (noise)\n",
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "For multiple predictors, we extend to matrix notation:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^n$ is the response vector\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{n \\times (p+1)}$ is the design matrix (with column of ones for intercept)\n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}$ is the coefficient vector\n",
    "- $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^n$ is the error vector\n",
    "\n",
    "## Ordinary Least Squares (OLS) Estimation\n",
    "\n",
    "The goal is to find $\\hat{\\boldsymbol{\\beta}}$ that minimizes the sum of squared residuals:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}}{\\arg\\min} \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2$$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "The cost function (Mean Squared Error) is:\n",
    "\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$$\n",
    "\n",
    "### Normal Equation\n",
    "\n",
    "Taking the derivative and setting it to zero:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\boldsymbol{\\beta}} = -\\frac{2}{n}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = 0$$\n",
    "\n",
    "Solving for $\\boldsymbol{\\beta}$:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "This is the **closed-form solution** for linear regression.\n",
    "\n",
    "### Gradient Descent Alternative\n",
    "\n",
    "For large datasets, we can use gradient descent:\n",
    "\n",
    "$$\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\alpha \\nabla J(\\boldsymbol{\\beta}^{(t)})$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and:\n",
    "\n",
    "$$\\nabla J(\\boldsymbol{\\beta}) = -\\frac{2}{n}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "We will implement a `LinearRegression` class that supports both the closed-form solution (Normal Equation) and gradient descent optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression implementation from scratch.\n",
    "    \n",
    "    Supports both closed-form (Normal Equation) and gradient descent solutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, method='closed_form'):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent\n",
    "        n_iterations : int\n",
    "            Number of iterations for gradient descent\n",
    "        method : str\n",
    "            'closed_form' or 'gradient_descent'\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.method = method\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"Add a column of ones for the intercept term.\"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def _compute_cost(self, X, y, theta):\n",
    "        \"\"\"Compute the Mean Squared Error cost function.\"\"\"\n",
    "        n = len(y)\n",
    "        predictions = X @ theta\n",
    "        cost = (1 / (2 * n)) * np.sum((predictions - y) ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model to the training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Training features of shape (n_samples, n_features)\n",
    "        y : numpy.ndarray\n",
    "            Target values of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        # Ensure y is a column vector\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Add intercept column\n",
    "        X_b = self._add_intercept(X)\n",
    "        n_samples, n_features = X_b.shape\n",
    "        \n",
    "        if self.method == 'closed_form':\n",
    "            # Normal Equation: theta = (X^T X)^(-1) X^T y\n",
    "            theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "        \n",
    "        elif self.method == 'gradient_descent':\n",
    "            # Initialize weights randomly\n",
    "            theta = np.random.randn(n_features, 1) * 0.01\n",
    "            \n",
    "            # Gradient descent\n",
    "            for i in range(self.n_iterations):\n",
    "                # Compute predictions\n",
    "                predictions = X_b @ theta\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = (1 / n_samples) * X_b.T @ (predictions - y)\n",
    "                \n",
    "                # Update weights\n",
    "                theta = theta - self.learning_rate * gradients\n",
    "                \n",
    "                # Store cost for visualization\n",
    "                cost = self._compute_cost(X_b, y, theta)\n",
    "                self.cost_history.append(cost)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'closed_form' or 'gradient_descent'\")\n",
    "        \n",
    "        # Extract bias and weights\n",
    "        self.bias = theta[0, 0]\n",
    "        self.weights = theta[1:].flatten()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for input features.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Input features of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        return X @ self.weights + self.bias\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the R² score (coefficient of determination).\n",
    "        \n",
    "        R² = 1 - SS_res / SS_tot\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Input features\n",
    "        y : numpy.ndarray\n",
    "            True target values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            R² score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "We create a dataset with a known linear relationship plus Gaussian noise:\n",
    "\n",
    "$$y = 3 + 2x_1 + 1.5x_2 + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "n_samples = 200\n",
    "n_features = 2\n",
    "\n",
    "# True parameters\n",
    "true_intercept = 3.0\n",
    "true_weights = np.array([2.0, 1.5])\n",
    "\n",
    "# Generate features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate target with noise\n",
    "noise = np.random.randn(n_samples) * 1.0\n",
    "y = true_intercept + X @ true_weights + noise\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"True intercept: {true_intercept}\")\n",
    "print(f\"True weights: {true_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "We will train two models: one using the closed-form solution and one using gradient descent, then compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with closed-form solution\n",
    "model_closed = LinearRegression(method='closed_form')\n",
    "model_closed.fit(X, y)\n",
    "\n",
    "print(\"=== Closed-Form Solution ===\")\n",
    "print(f\"Estimated intercept: {model_closed.bias:.4f}\")\n",
    "print(f\"Estimated weights: {model_closed.weights}\")\n",
    "print(f\"R² score: {model_closed.score(X, y):.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with gradient descent\n",
    "model_gd = LinearRegression(\n",
    "    method='gradient_descent',\n",
    "    learning_rate=0.1,\n",
    "    n_iterations=1000\n",
    ")\n",
    "model_gd.fit(X, y)\n",
    "\n",
    "print(\"=== Gradient Descent Solution ===\")\n",
    "print(f\"Estimated intercept: {model_gd.bias:.4f}\")\n",
    "print(f\"Estimated weights: {model_gd.weights}\")\n",
    "print(f\"R² score: {model_gd.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We will create a comprehensive visualization showing:\n",
    "1. The data and regression fit (for 1D projection)\n",
    "2. Gradient descent convergence\n",
    "3. Residual analysis\n",
    "4. Parameter comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Data and regression line (using first feature)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(X[:, 0], y, alpha=0.6, label='Data points', color='steelblue')\n",
    "\n",
    "# Create prediction line for first feature (holding second at mean)\n",
    "x_line = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "X_pred = np.column_stack([x_line, np.full_like(x_line, X[:, 1].mean())])\n",
    "y_line = model_closed.predict(X_pred)\n",
    "ax1.plot(x_line, y_line, 'r-', linewidth=2, label='Regression line')\n",
    "ax1.set_xlabel('$x_1$', fontsize=12)\n",
    "ax1.set_ylabel('$y$', fontsize=12)\n",
    "ax1.set_title('Linear Regression Fit (Feature 1)', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Gradient descent convergence\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(model_gd.cost_history, 'b-', linewidth=1.5)\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Cost (MSE)', fontsize=12)\n",
    "ax2.set_title('Gradient Descent Convergence', fontsize=14)\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residual analysis\n",
    "ax3 = axes[1, 0]\n",
    "y_pred = model_closed.predict(X)\n",
    "residuals = y - y_pred\n",
    "ax3.scatter(y_pred, residuals, alpha=0.6, color='green')\n",
    "ax3.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax3.set_xlabel('Predicted values $\\hat{y}$', fontsize=12)\n",
    "ax3.set_ylabel('Residuals $(y - \\hat{y})$', fontsize=12)\n",
    "ax3.set_title('Residual Plot', fontsize=14)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Parameter comparison\n",
    "ax4 = axes[1, 1]\n",
    "params = ['$\\\\beta_0$ (intercept)', '$\\\\beta_1$ (weight 1)', '$\\\\beta_2$ (weight 2)']\n",
    "true_values = [true_intercept, true_weights[0], true_weights[1]]\n",
    "closed_values = [model_closed.bias, model_closed.weights[0], model_closed.weights[1]]\n",
    "gd_values = [model_gd.bias, model_gd.weights[0], model_gd.weights[1]]\n",
    "\n",
    "x_pos = np.arange(len(params))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax4.bar(x_pos - width, true_values, width, label='True', color='darkgreen', alpha=0.8)\n",
    "bars2 = ax4.bar(x_pos, closed_values, width, label='Closed-form', color='steelblue', alpha=0.8)\n",
    "bars3 = ax4.bar(x_pos + width, gd_values, width, label='Gradient descent', color='coral', alpha=0.8)\n",
    "\n",
    "ax4.set_xlabel('Parameters', fontsize=12)\n",
    "ax4.set_ylabel('Value', fontsize=12)\n",
    "ax4.set_title('Parameter Estimation Comparison', fontsize=14)\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(params)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics\n",
    "\n",
    "Let's compute additional metrics to evaluate our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute regression evaluation metrics.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing MSE, RMSE, MAE, and R²\n",
    "    \"\"\"\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2\n",
    "    }\n",
    "\n",
    "# Compute metrics for closed-form solution\n",
    "y_pred_closed = model_closed.predict(X)\n",
    "metrics_closed = compute_metrics(y, y_pred_closed)\n",
    "\n",
    "# Compute metrics for gradient descent solution\n",
    "y_pred_gd = model_gd.predict(X)\n",
    "metrics_gd = compute_metrics(y, y_pred_gd)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Model Evaluation Metrics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n{'Metric':<10} {'Closed-Form':>15} {'Gradient Descent':>18}\")\n",
    "print(\"-\" * 45)\n",
    "for metric in ['MSE', 'RMSE', 'MAE', 'R²']:\n",
    "    print(f\"{metric:<10} {metrics_closed[metric]:>15.4f} {metrics_gd[metric]:>18.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented linear regression from scratch using two methods:\n",
    "\n",
    "1. **Closed-form solution (Normal Equation)**: Provides an exact analytical solution in one step\n",
    "   - Advantages: Fast for small datasets, exact solution\n",
    "   - Disadvantages: Computationally expensive for large datasets ($O(n^3)$ for matrix inversion)\n",
    "\n",
    "2. **Gradient descent**: Iteratively optimizes the cost function\n",
    "   - Advantages: Scales well to large datasets, memory efficient\n",
    "   - Disadvantages: Requires tuning hyperparameters (learning rate, iterations)\n",
    "\n",
    "Both methods successfully recovered the true parameters with high accuracy, demonstrating the effectiveness of linear regression for modeling linear relationships in data.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The R² score close to 1 indicates that the model explains most of the variance in the data\n",
    "- The residual plot shows randomly scattered residuals around zero, indicating a good fit\n",
    "- Both optimization methods converge to nearly identical solutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
