{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-b134ec7e-d090-4ded-bb88-9ae85ee19980.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_backend_state":1763845624216,"last_ipynb_save":1763845635250,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1763845624243,"exec_count":3,"id":"94410e","input":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# Set random seed for reproducibility\nnp.random.seed(42)","kernel":"python3","pos":2,"start":1763845624230,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845624255,"exec_count":4,"id":"480467","input":"def generate_dataset(n_samples=200, noise=0.1):\n    \"\"\"\n    Generate a synthetic binary classification dataset.\n    \n    Parameters:\n    -----------\n    n_samples : int\n        Total number of samples\n    noise : float\n        Standard deviation of Gaussian noise\n    \n    Returns:\n    --------\n    X : ndarray of shape (n_samples, 2)\n        Feature matrix\n    y : ndarray of shape (n_samples,)\n        Binary labels\n    \"\"\"\n    n_per_class = n_samples // 2\n    \n    # Class 0: centered at (-1, -1)\n    X0 = np.random.randn(n_per_class, 2) * 0.8 + np.array([-1, -1])\n    \n    # Class 1: centered at (1, 1)\n    X1 = np.random.randn(n_per_class, 2) * 0.8 + np.array([1, 1])\n    \n    X = np.vstack([X0, X1])\n    y = np.hstack([np.zeros(n_per_class), np.ones(n_per_class)])\n    \n    # Shuffle the dataset\n    indices = np.random.permutation(n_samples)\n    X = X[indices]\n    y = y[indices]\n    \n    return X, y\n\n# Generate dataset\nX, y = generate_dataset(n_samples=300)\n\nprint(f\"Dataset shape: X={X.shape}, y={y.shape}\")\nprint(f\"Class distribution: {np.sum(y==0):.0f} samples in class 0, {np.sum(y==1):.0f} samples in class 1\")","kernel":"python3","output":{"0":{"name":"stdout","text":"Dataset shape: X=(300, 2), y=(300,)\nClass distribution: 150 samples in class 0, 150 samples in class 1\n"}},"pos":4,"start":1763845624251,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845624268,"exec_count":5,"id":"c01b0d","input":"class LogisticRegression:\n    \"\"\"\n    Logistic Regression classifier using gradient descent.\n    \n    Parameters:\n    -----------\n    learning_rate : float\n        Learning rate for gradient descent\n    n_iterations : int\n        Number of training iterations\n    \"\"\"\n    \n    def __init__(self, learning_rate=0.1, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n        self.loss_history = []\n    \n    def _sigmoid(self, z):\n        \"\"\"Compute the sigmoid function.\"\"\"\n        # Clip values to avoid overflow\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n    \n    def _compute_loss(self, y, y_pred):\n        \"\"\"Compute binary cross-entropy loss.\"\"\"\n        m = len(y)\n        # Add small epsilon to avoid log(0)\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        return loss\n    \n    def fit(self, X, y):\n        \"\"\"\n        Train the logistic regression model.\n        \n        Parameters:\n        -----------\n        X : ndarray of shape (n_samples, n_features)\n            Training features\n        y : ndarray of shape (n_samples,)\n            Training labels\n        \"\"\"\n        n_samples, n_features = X.shape\n        \n        # Initialize parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        self.loss_history = []\n        \n        # Gradient descent\n        for i in range(self.n_iterations):\n            # Forward pass\n            z = np.dot(X, self.weights) + self.bias\n            y_pred = self._sigmoid(z)\n            \n            # Compute gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n            db = (1 / n_samples) * np.sum(y_pred - y)\n            \n            # Update parameters\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n            \n            # Record loss\n            loss = self._compute_loss(y, y_pred)\n            self.loss_history.append(loss)\n        \n        return self\n    \n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities.\"\"\"\n        z = np.dot(X, self.weights) + self.bias\n        return self._sigmoid(z)\n    \n    def predict(self, X, threshold=0.5):\n        \"\"\"Predict class labels.\"\"\"\n        return (self.predict_proba(X) >= threshold).astype(int)","kernel":"python3","pos":6,"start":1763845624264,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845624280,"exec_count":6,"id":"db9bff","input":"def train_test_split(X, y, test_size=0.2, random_state=None):\n    \"\"\"\n    Split dataset into training and test sets.\n    \n    Parameters:\n    -----------\n    X : ndarray\n        Features\n    y : ndarray\n        Labels\n    test_size : float\n        Proportion of test set\n    random_state : int\n        Random seed\n    \n    Returns:\n    --------\n    X_train, X_test, y_train, y_test\n    \"\"\"\n    if random_state is not None:\n        np.random.seed(random_state)\n    \n    n_samples = len(y)\n    n_test = int(n_samples * test_size)\n    \n    indices = np.random.permutation(n_samples)\n    test_indices = indices[:n_test]\n    train_indices = indices[n_test:]\n    \n    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")","kernel":"python3","output":{"0":{"name":"stdout","text":"Training set: 240 samples\nTest set: 60 samples\n"}},"pos":8,"start":1763845624274,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845624309,"exec_count":7,"id":"6b1e87","input":"# Create and train the model\nmodel = LogisticRegression(learning_rate=0.5, n_iterations=500)\nmodel.fit(X_train, y_train)\n\n# Display learned parameters\nprint(f\"Learned weights: w = [{model.weights[0]:.4f}, {model.weights[1]:.4f}]\")\nprint(f\"Learned bias: b = {model.bias:.4f}\")","kernel":"python3","output":{"0":{"name":"stdout","text":"Learned weights: w = [3.5958, 3.4944]\nLearned bias: b = -0.1908\n"}},"pos":10,"start":1763845624286,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845624325,"exec_count":8,"id":"1e4cdf","input":"def compute_metrics(y_true, y_pred):\n    \"\"\"\n    Compute classification metrics.\n    \n    Returns:\n    --------\n    dict : Dictionary containing accuracy, precision, recall, and F1-score\n    \"\"\"\n    # True positives, false positives, etc.\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'confusion_matrix': np.array([[tn, fp], [fn, tp]])\n    }\n\n# Evaluate on training set\ny_train_pred = model.predict(X_train)\ntrain_metrics = compute_metrics(y_train, y_train_pred)\n\n# Evaluate on test set\ny_test_pred = model.predict(X_test)\ntest_metrics = compute_metrics(y_test, y_test_pred)\n\nprint(\"Training Set Metrics:\")\nprint(f\"  Accuracy:  {train_metrics['accuracy']:.4f}\")\nprint(f\"  Precision: {train_metrics['precision']:.4f}\")\nprint(f\"  Recall:    {train_metrics['recall']:.4f}\")\nprint(f\"  F1-Score:  {train_metrics['f1_score']:.4f}\")\n\nprint(\"\\nTest Set Metrics:\")\nprint(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\nprint(f\"  Precision: {test_metrics['precision']:.4f}\")\nprint(f\"  Recall:    {test_metrics['recall']:.4f}\")\nprint(f\"  F1-Score:  {test_metrics['f1_score']:.4f}\")\n\nprint(\"\\nConfusion Matrix (Test Set):\")\nprint(test_metrics['confusion_matrix'])","kernel":"python3","output":{"0":{"name":"stdout","text":"Training Set Metrics:\n  Accuracy:  0.9750\n  Precision: 0.9750\n  Recall:    0.9750\n  F1-Score:  0.9750\n\nTest Set Metrics:\n  Accuracy:  0.9000\n  Precision: 0.9286\n  Recall:    0.8667\n  F1-Score:  0.8966\n\nConfusion Matrix (Test Set):\n[[28  2]\n [ 4 26]]\n"}},"pos":12,"start":1763845624317,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845626938,"exec_count":9,"id":"42c7ce","input":"# Create figure with subplots\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\n# Plot 1: Decision Boundary\nax1 = axes[0, 0]\n\n# Create mesh grid\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                     np.linspace(y_min, y_max, 200))\n\n# Predict on mesh grid\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot decision regions\nax1.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)\nax1.contour(xx, yy, Z, colors='k', linewidths=0.5)\n\n# Plot data points\nscatter = ax1.scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n                      cmap=cmap_bold, edgecolors='k', alpha=0.7, s=50, label='Train')\nax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n            cmap=cmap_bold, edgecolors='k', alpha=0.7, s=100, marker='s', label='Test')\n\n# Plot decision boundary line\nw1, w2 = model.weights\nb = model.bias\nx_boundary = np.linspace(x_min, x_max, 100)\ny_boundary = -(w1 * x_boundary + b) / w2\nax1.plot(x_boundary, y_boundary, 'k-', linewidth=2, label='Decision Boundary')\n\nax1.set_xlabel('$x_1$', fontsize=12)\nax1.set_ylabel('$x_2$', fontsize=12)\nax1.set_title('Decision Boundary', fontsize=14)\nax1.legend(loc='upper left')\nax1.set_xlim(x_min, x_max)\nax1.set_ylim(y_min, y_max)\n\n# Plot 2: Training Loss History\nax2 = axes[0, 1]\nax2.plot(model.loss_history, 'b-', linewidth=1.5)\nax2.set_xlabel('Iteration', fontsize=12)\nax2.set_ylabel('Cross-Entropy Loss', fontsize=12)\nax2.set_title('Training Loss History', fontsize=14)\nax2.grid(True, alpha=0.3)\nax2.set_xlim(0, len(model.loss_history))\n\n# Plot 3: Probability Contours\nax3 = axes[1, 0]\n\n# Compute probability on mesh grid\nZ_prob = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\nZ_prob = Z_prob.reshape(xx.shape)\n\n# Plot probability contours\ncontour = ax3.contourf(xx, yy, Z_prob, levels=20, cmap='RdBu_r', alpha=0.8)\nax3.contour(xx, yy, Z_prob, levels=[0.5], colors='k', linewidths=2)\nplt.colorbar(contour, ax=ax3, label='$P(y=1|\\mathbf{x})$')\n\n# Plot data points\nax3.scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n            cmap=cmap_bold, edgecolors='k', alpha=0.7, s=50)\nax3.scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n            cmap=cmap_bold, edgecolors='k', alpha=0.7, s=100, marker='s')\n\nax3.set_xlabel('$x_1$', fontsize=12)\nax3.set_ylabel('$x_2$', fontsize=12)\nax3.set_title('Probability Contours', fontsize=14)\nax3.set_xlim(x_min, x_max)\nax3.set_ylim(y_min, y_max)\n\n# Plot 4: Sigmoid Function and Decision\nax4 = axes[1, 1]\n\n# Plot sigmoid function\nz_range = np.linspace(-6, 6, 200)\nsigmoid_values = 1 / (1 + np.exp(-z_range))\nax4.plot(z_range, sigmoid_values, 'b-', linewidth=2, label='$\\sigma(z)$')\nax4.axhline(y=0.5, color='r', linestyle='--', linewidth=1, label='Threshold = 0.5')\nax4.axvline(x=0, color='gray', linestyle=':', linewidth=1)\n\n# Highlight regions\nax4.fill_between(z_range, 0, sigmoid_values, where=(z_range < 0), \n                 alpha=0.3, color='red', label='Predict Class 0')\nax4.fill_between(z_range, 0, sigmoid_values, where=(z_range >= 0), \n                 alpha=0.3, color='blue', label='Predict Class 1')\n\nax4.set_xlabel('$z = \\mathbf{w}^T\\mathbf{x} + b$', fontsize=12)\nax4.set_ylabel('$\\sigma(z)$', fontsize=12)\nax4.set_title('Sigmoid Function', fontsize=14)\nax4.legend(loc='upper left', fontsize=9)\nax4.grid(True, alpha=0.3)\nax4.set_xlim(-6, 6)\nax4.set_ylim(-0.05, 1.05)\n\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('plot.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nFigure saved as 'plot.png'\")","kernel":"python3","output":{"0":{"data":{"image/png":"b10ce0b55460e0584fa0c88ce0463baa13f95c7a","text/plain":"<Figure size 1400x1200 with 5 Axes>"},"metadata":{"image/png":{"height":1187,"width":1388}}},"1":{"name":"stdout","text":"\nFigure saved as 'plot.png'\n"}},"pos":14,"start":1763845624348,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"1e334f","input":"## Conclusion\n\nLogistic regression remains a fundamental tool in machine learning due to its interpretability, efficiency, and probabilistic framework. This implementation demonstrates the core concepts of maximum likelihood estimation, gradient descent optimization, and the sigmoid function. Understanding logistic regression provides a solid foundation for more advanced classification methods, including neural networks where the sigmoid (or softmax) function serves as the output activation.","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"2169d3","input":"## Discussion\n\n### Key Observations\n\n1. **Linear Decision Boundary**: Logistic regression creates a linear decision boundary in feature space. The equation $\\mathbf{w}^T \\mathbf{x} + b = 0$ defines this hyperplane.\n\n2. **Probabilistic Output**: Unlike hard classifiers, logistic regression provides calibrated probability estimates, which are valuable for risk assessment and decision-making under uncertainty.\n\n3. **Convex Optimization**: The cross-entropy loss function is convex, guaranteeing convergence to the global minimum with gradient descent.\n\n4. **Interpretability**: Each weight $w_j$ represents the change in log-odds for a one-unit increase in feature $x_j$, holding other features constant.\n\n### Limitations\n\n1. **Linear Separability**: Logistic regression assumes data is linearly separable, which limits its expressiveness for complex decision boundaries.\n\n2. **Feature Engineering**: Performance heavily depends on feature selection and transformation.\n\n3. **Class Imbalance**: Standard logistic regression can be biased toward the majority class in imbalanced datasets.\n\n### Extensions\n\n1. **Regularization**: L1 (Lasso) and L2 (Ridge) penalties can be added to prevent overfitting:\n   $$J_{reg} = J + \\frac{\\lambda}{2m} \\|\\mathbf{w}\\|^2_2$$\n\n2. **Multiclass Classification**: The softmax function generalizes logistic regression to multiple classes (multinomial logistic regression).\n\n3. **Kernel Methods**: Applying kernel tricks can create nonlinear decision boundaries while maintaining the probabilistic framework.","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"457da4","input":"### Generate Synthetic Dataset\n\nWe create a two-class dataset with two features for visualization purposes.","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"4cc690","input":"### Visualization\n\nWe create a comprehensive visualization showing the decision boundary, training history, and probability contours.","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"586801","input":"### Train-Test Split","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"5b992b","input":"# Logistic Regression Classification\n\n## Introduction\n\nLogistic regression is a fundamental classification algorithm that models the probability of a binary outcome. Despite its name, it is a classification method rather than a regression technique. It serves as a cornerstone of statistical learning and provides an excellent foundation for understanding more complex classification algorithms.\n\n## Mathematical Foundation\n\n### The Logistic Function (Sigmoid)\n\nThe logistic function transforms any real-valued input into a probability between 0 and 1:\n\n$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n\nwhere $z$ is the linear combination of features:\n\n$$z = \\mathbf{w}^T \\mathbf{x} + b = \\sum_{i=1}^{n} w_i x_i + b$$\n\n### Probability Model\n\nFor binary classification with classes $y \\in \\{0, 1\\}$, logistic regression models:\n\n$$P(y=1 | \\mathbf{x}; \\mathbf{w}, b) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$$\n\n$$P(y=0 | \\mathbf{x}; \\mathbf{w}, b) = 1 - \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$$\n\n### Log-Odds (Logit) Interpretation\n\nThe logit function is the inverse of the sigmoid:\n\n$$\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\mathbf{w}^T \\mathbf{x} + b$$\n\nThis shows that logistic regression models the log-odds as a linear function of the features.\n\n### Maximum Likelihood Estimation\n\nGiven a dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{m}$, the likelihood function is:\n\n$$L(\\mathbf{w}, b) = \\prod_{i=1}^{m} \\hat{p}_i^{y_i} (1 - \\hat{p}_i)^{1-y_i}$$\n\nwhere $\\hat{p}_i = \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b)$.\n\n### Cross-Entropy Loss Function\n\nTaking the negative log-likelihood gives the binary cross-entropy loss:\n\n$$J(\\mathbf{w}, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\ln(\\hat{p}_i) + (1 - y_i) \\ln(1 - \\hat{p}_i) \\right]$$\n\n### Gradient Descent Update Rules\n\nThe gradients of the loss function are:\n\n$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{p}_i - y_i) x_{ij}$$\n\n$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{p}_i - y_i)$$\n\nThe parameters are updated as:\n\n$$\\mathbf{w} := \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} J$$\n\n$$b := b - \\alpha \\frac{\\partial J}{\\partial b}$$\n\nwhere $\\alpha$ is the learning rate.","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"a8bf1d","input":"### Logistic Regression Class Implementation","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"c73115","input":"### Model Training","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"c8f75d","input":"## Implementation\n\nWe will implement logistic regression from scratch and apply it to a synthetic binary classification dataset.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"f0e0a6","input":"### Model Evaluation","pos":11,"type":"cell"}
{"id":0,"time":1763845621325,"type":"user"}
{"last_load":1763845621748,"type":"file"}