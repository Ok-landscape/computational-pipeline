{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Annealing Optimization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Simulated Annealing (SA) is a probabilistic metaheuristic optimization algorithm inspired by the annealing process in metallurgy. In metallurgy, annealing involves heating a material to a high temperature and then slowly cooling it to decrease defects and reach a low-energy crystalline state. Similarly, the SA algorithm explores the solution space by accepting both improving and worsening solutions, with the probability of accepting worse solutions decreasing as the algorithm progresses.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "Given an objective function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ that we wish to minimize, SA seeks to find:\n",
    "\n",
    "$$\\mathbf{x}^* = \\arg\\min_{\\mathbf{x} \\in \\mathcal{S}} f(\\mathbf{x})$$\n",
    "\n",
    "where $\\mathcal{S}$ is the search space.\n",
    "\n",
    "### Metropolis Acceptance Criterion\n",
    "\n",
    "At each iteration, a candidate solution $\\mathbf{x}'$ is generated from the current solution $\\mathbf{x}$ via a perturbation. The acceptance probability follows the Metropolis criterion:\n",
    "\n",
    "$$P(\\text{accept}) = \\begin{cases} 1 & \\text{if } \\Delta E \\leq 0 \\\\ \\exp\\left(-\\frac{\\Delta E}{T}\\right) & \\text{if } \\Delta E > 0 \\end{cases}$$\n",
    "\n",
    "where:\n",
    "- $\\Delta E = f(\\mathbf{x}') - f(\\mathbf{x})$ is the change in energy (objective function value)\n",
    "- $T$ is the current temperature\n",
    "\n",
    "### Cooling Schedule\n",
    "\n",
    "The temperature decreases according to a cooling schedule. Common schedules include:\n",
    "\n",
    "**Geometric cooling:**\n",
    "$$T_{k+1} = \\alpha \\cdot T_k$$\n",
    "\n",
    "where $\\alpha \\in (0, 1)$ is the cooling rate (typically $0.85 \\leq \\alpha \\leq 0.99$).\n",
    "\n",
    "**Logarithmic cooling:**\n",
    "$$T_k = \\frac{T_0}{\\ln(1 + k)}$$\n",
    "\n",
    "### Convergence Properties\n",
    "\n",
    "Under suitable conditions, SA converges to a global optimum. Specifically, if the temperature decreases sufficiently slowly (logarithmic cooling), the probability of being in the global optimum state approaches 1:\n",
    "\n",
    "$$\\lim_{k \\rightarrow \\infty} P(\\mathbf{x}_k = \\mathbf{x}^*) = 1$$\n",
    "\n",
    "### Boltzmann Distribution\n",
    "\n",
    "At thermal equilibrium for temperature $T$, the probability of the system being in state $\\mathbf{x}$ follows the Boltzmann distribution:\n",
    "\n",
    "$$P(\\mathbf{x}) = \\frac{\\exp\\left(-\\frac{f(\\mathbf{x})}{T}\\right)}{Z(T)}$$\n",
    "\n",
    "where $Z(T) = \\sum_{\\mathbf{x} \\in \\mathcal{S}} \\exp\\left(-\\frac{f(\\mathbf{x})}{T}\\right)$ is the partition function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement SA to minimize the Rastrigin function, a classic benchmark for optimization algorithms due to its highly multimodal landscape:\n",
    "\n",
    "$$f(\\mathbf{x}) = An + \\sum_{i=1}^{n} \\left[ x_i^2 - A\\cos(2\\pi x_i) \\right]$$\n",
    "\n",
    "where $A = 10$ and typically $x_i \\in [-5.12, 5.12]$. The global minimum is at $\\mathbf{x}^* = \\mathbf{0}$ with $f(\\mathbf{x}^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rastrigin(x):\n",
    "    \"\"\"\n",
    "    Rastrigin function - a non-convex function used as a performance test problem\n",
    "    for optimization algorithms.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Input vector\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Function value at x\n",
    "    \"\"\"\n",
    "    A = 10\n",
    "    n = len(x)\n",
    "    return A * n + np.sum(x**2 - A * np.cos(2 * np.pi * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_annealing(objective, bounds, n_iterations=10000, \n",
    "                        initial_temp=100.0, cooling_rate=0.995):\n",
    "    \"\"\"\n",
    "    Simulated Annealing optimization algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    objective : callable\n",
    "        The objective function to minimize\n",
    "    bounds : list of tuples\n",
    "        Bounds for each dimension [(min, max), ...]\n",
    "    n_iterations : int\n",
    "        Number of iterations\n",
    "    initial_temp : float\n",
    "        Initial temperature\n",
    "    cooling_rate : float\n",
    "        Geometric cooling rate (alpha)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (best_solution, best_score, history)\n",
    "    \"\"\"\n",
    "    n_dims = len(bounds)\n",
    "    \n",
    "    # Initialize current solution randomly within bounds\n",
    "    current = np.array([np.random.uniform(b[0], b[1]) for b in bounds])\n",
    "    current_score = objective(current)\n",
    "    \n",
    "    # Track best solution found\n",
    "    best = current.copy()\n",
    "    best_score = current_score\n",
    "    \n",
    "    # History for visualization\n",
    "    history = {\n",
    "        'temperature': [],\n",
    "        'current_score': [],\n",
    "        'best_score': [],\n",
    "        'acceptance_rate': [],\n",
    "        'positions': []\n",
    "    }\n",
    "    \n",
    "    temperature = initial_temp\n",
    "    accepted = 0\n",
    "    window_size = 100\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Generate candidate solution via Gaussian perturbation\n",
    "        # Step size decreases with temperature\n",
    "        step_size = 0.5 * (temperature / initial_temp)\n",
    "        candidate = current + np.random.randn(n_dims) * step_size\n",
    "        \n",
    "        # Ensure candidate is within bounds\n",
    "        candidate = np.clip(candidate, \n",
    "                           [b[0] for b in bounds], \n",
    "                           [b[1] for b in bounds])\n",
    "        \n",
    "        candidate_score = objective(candidate)\n",
    "        \n",
    "        # Compute energy difference\n",
    "        delta_e = candidate_score - current_score\n",
    "        \n",
    "        # Metropolis acceptance criterion\n",
    "        if delta_e < 0 or np.random.random() < np.exp(-delta_e / temperature):\n",
    "            current = candidate\n",
    "            current_score = candidate_score\n",
    "            accepted += 1\n",
    "            \n",
    "            # Update best if improved\n",
    "            if current_score < best_score:\n",
    "                best = current.copy()\n",
    "                best_score = current_score\n",
    "        \n",
    "        # Record history\n",
    "        history['temperature'].append(temperature)\n",
    "        history['current_score'].append(current_score)\n",
    "        history['best_score'].append(best_score)\n",
    "        history['positions'].append(current.copy())\n",
    "        \n",
    "        # Calculate acceptance rate over sliding window\n",
    "        if (i + 1) % window_size == 0:\n",
    "            history['acceptance_rate'].append(accepted / window_size)\n",
    "            accepted = 0\n",
    "        \n",
    "        # Cool down\n",
    "        temperature *= cooling_rate\n",
    "    \n",
    "    return best, best_score, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the Objective Function\n",
    "\n",
    "Let us first visualize the Rastrigin function in 2D to understand its multimodal nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meshgrid for visualization\n",
    "x = np.linspace(-5.12, 5.12, 200)\n",
    "y = np.linspace(-5.12, 5.12, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Compute Rastrigin function values\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = rastrigin(np.array([X[i, j], Y[i, j]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulated Annealing Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search bounds\n",
    "bounds = [(-5.12, 5.12), (-5.12, 5.12)]\n",
    "\n",
    "# Run optimization\n",
    "best_solution, best_score, history = simulated_annealing(\n",
    "    rastrigin, \n",
    "    bounds, \n",
    "    n_iterations=10000,\n",
    "    initial_temp=100.0,\n",
    "    cooling_rate=0.9995\n",
    ")\n",
    "\n",
    "print(f\"Best solution found: x = [{best_solution[0]:.6f}, {best_solution[1]:.6f}]\")\n",
    "print(f\"Best objective value: f(x) = {best_score:.6f}\")\n",
    "print(f\"Global optimum: f(0, 0) = 0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot 1: 3D surface of Rastrigin function\n",
    "ax1 = fig.add_subplot(2, 2, 1, projection='3d')\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, \n",
    "                        linewidth=0, antialiased=True)\n",
    "ax1.scatter([best_solution[0]], [best_solution[1]], [best_score], \n",
    "           color='red', s=100, marker='*', label='Best found')\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_zlabel('$f(x)$')\n",
    "ax1.set_title('Rastrigin Function Surface')\n",
    "ax1.view_init(elev=30, azim=45)\n",
    "\n",
    "# Plot 2: Contour plot with optimization path\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "contour = ax2.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax2, label='$f(x)$')\n",
    "\n",
    "# Plot optimization trajectory\n",
    "positions = np.array(history['positions'])\n",
    "# Subsample for clarity\n",
    "step = max(1, len(positions) // 500)\n",
    "ax2.plot(positions[::step, 0], positions[::step, 1], 'w-', \n",
    "         alpha=0.5, linewidth=0.5, label='Path')\n",
    "ax2.scatter(positions[0, 0], positions[0, 1], color='yellow', \n",
    "           s=100, marker='o', edgecolors='black', label='Start')\n",
    "ax2.scatter(best_solution[0], best_solution[1], color='red', \n",
    "           s=100, marker='*', edgecolors='black', label='Best')\n",
    "ax2.scatter(0, 0, color='lime', s=100, marker='x', \n",
    "           linewidths=3, label='Global min')\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_title('Optimization Trajectory')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Plot 3: Convergence history\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "iterations = np.arange(len(history['best_score']))\n",
    "ax3.semilogy(iterations, history['current_score'], 'b-', \n",
    "            alpha=0.3, label='Current', linewidth=0.5)\n",
    "ax3.semilogy(iterations, history['best_score'], 'r-', \n",
    "            linewidth=2, label='Best')\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Objective Value (log scale)')\n",
    "ax3.set_title('Convergence History')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Temperature and acceptance rate\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "ax4_twin = ax4.twinx()\n",
    "\n",
    "# Temperature\n",
    "ln1 = ax4.semilogy(iterations, history['temperature'], 'b-', \n",
    "                   linewidth=2, label='Temperature')\n",
    "ax4.set_xlabel('Iteration')\n",
    "ax4.set_ylabel('Temperature (log scale)', color='blue')\n",
    "ax4.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Acceptance rate\n",
    "if history['acceptance_rate']:\n",
    "    ar_iterations = np.arange(100, len(history['best_score']) + 1, 100)[:len(history['acceptance_rate'])]\n",
    "    ln2 = ax4_twin.plot(ar_iterations, history['acceptance_rate'], 'r-', \n",
    "                       linewidth=2, label='Acceptance Rate')\n",
    "    ax4_twin.set_ylabel('Acceptance Rate', color='red')\n",
    "    ax4_twin.tick_params(axis='y', labelcolor='red')\n",
    "    ax4_twin.set_ylim(0, 1)\n",
    "\n",
    "ax4.set_title('Temperature Schedule & Acceptance Rate')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Discussion\n",
    "\n",
    "### Algorithm Behavior\n",
    "\n",
    "The visualization demonstrates several key characteristics of Simulated Annealing:\n",
    "\n",
    "1. **Exploration vs. Exploitation**: At high temperatures, the algorithm freely explores the search space, accepting uphill moves that help escape local minima. As temperature decreases, it gradually shifts toward exploitation, refining the solution in promising regions.\n",
    "\n",
    "2. **Acceptance Rate Decay**: The acceptance rate naturally decreases as temperature drops, reflecting the transition from exploration to exploitation.\n",
    "\n",
    "3. **Convergence**: The best score monotonically improves (or stays constant), while the current score fluctuates due to accepted uphill moves.\n",
    "\n",
    "### Parameter Sensitivity\n",
    "\n",
    "- **Initial Temperature ($T_0$)**: Should be high enough to allow significant exploration initially\n",
    "- **Cooling Rate ($\\alpha$)**: Slower cooling (higher $\\alpha$) improves solution quality but increases computation time\n",
    "- **Number of Iterations**: Must be sufficient to allow convergence at low temperatures\n",
    "\n",
    "### Comparison with Other Methods\n",
    "\n",
    "Unlike gradient-based methods, SA:\n",
    "- Does not require gradient information\n",
    "- Can escape local minima\n",
    "- Works on non-differentiable functions\n",
    "- Provides probabilistic guarantees of global convergence\n",
    "\n",
    "### Applications\n",
    "\n",
    "Simulated Annealing finds applications in:\n",
    "- Combinatorial optimization (TSP, scheduling)\n",
    "- Machine learning hyperparameter tuning\n",
    "- Circuit design and VLSI placement\n",
    "- Protein folding simulations\n",
    "- Financial portfolio optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
