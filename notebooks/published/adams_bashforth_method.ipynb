{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adams-Bashforth Method for Ordinary Differential Equations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **Adams-Bashforth methods** are a family of explicit multi-step numerical methods used to solve ordinary differential equations (ODEs). Unlike single-step methods such as Euler or Runge-Kutta, multi-step methods utilize information from several previous time steps to compute the solution at the next step, offering improved efficiency for smooth problems.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Consider the initial value problem (IVP):\n",
    "\n",
    "$$\\frac{dy}{dt} = f(t, y), \\quad y(t_0) = y_0$$\n",
    "\n",
    "We seek to approximate $y(t)$ at discrete points $t_n = t_0 + nh$ where $h$ is the step size.\n",
    "\n",
    "### Derivation via Polynomial Interpolation\n",
    "\n",
    "The Adams-Bashforth method is derived by integrating the ODE:\n",
    "\n",
    "$$y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt$$\n",
    "\n",
    "The key idea is to approximate $f(t, y(t))$ by a polynomial $P(t)$ that interpolates the values $f_j = f(t_j, y_j)$ at previous time points $t_{n}, t_{n-1}, \\ldots, t_{n-k+1}$.\n",
    "\n",
    "Using **backward Newton interpolation** with the interpolating polynomial:\n",
    "\n",
    "$$P(t) = \\sum_{j=0}^{k-1} \\binom{-s}{j} (-1)^j \\nabla^j f_n$$\n",
    "\n",
    "where $s = (t - t_n)/h$ and $\\nabla^j f_n$ are backward differences.\n",
    "\n",
    "### Adams-Bashforth Formulas\n",
    "\n",
    "**First-order (AB1 - Forward Euler):**\n",
    "$$y_{n+1} = y_n + h f_n$$\n",
    "\n",
    "**Second-order (AB2):**\n",
    "$$y_{n+1} = y_n + \\frac{h}{2}(3f_n - f_{n-1})$$\n",
    "\n",
    "**Third-order (AB3):**\n",
    "$$y_{n+1} = y_n + \\frac{h}{12}(23f_n - 16f_{n-1} + 5f_{n-2})$$\n",
    "\n",
    "**Fourth-order (AB4):**\n",
    "$$y_{n+1} = y_n + \\frac{h}{24}(55f_n - 59f_{n-1} + 37f_{n-2} - 9f_{n-3})$$\n",
    "\n",
    "### Local Truncation Error\n",
    "\n",
    "The local truncation error (LTE) for the $k$-th order Adams-Bashforth method is $O(h^{k+1})$, making it a $k$-th order method. Specifically:\n",
    "\n",
    "- AB1: LTE = $O(h^2)$\n",
    "- AB2: LTE = $O(h^3)$\n",
    "- AB3: LTE = $O(h^4)$\n",
    "- AB4: LTE = $O(h^5)$\n",
    "\n",
    "### Stability Considerations\n",
    "\n",
    "Adams-Bashforth methods have smaller stability regions compared to implicit methods. The stability region shrinks as the order increases, which may require smaller step sizes for stiff problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement the Adams-Bashforth methods of orders 1 through 4 and test them on a known ODE problem. Since multi-step methods require values at previous time steps, we use the 4th-order Runge-Kutta method (RK4) to bootstrap the initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "def rk4_step(f: Callable, t: float, y: float, h: float) -> float:\n",
    "    \"\"\"Single step of the 4th-order Runge-Kutta method.\"\"\"\n",
    "    k1 = f(t, y)\n",
    "    k2 = f(t + h/2, y + h*k1/2)\n",
    "    k3 = f(t + h/2, y + h*k2/2)\n",
    "    k4 = f(t + h, y + h*k3)\n",
    "    return y + (h/6) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "\n",
    "def adams_bashforth(f: Callable, t_span: Tuple[float, float], y0: float, \n",
    "                    h: float, order: int = 4) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Solve ODE using Adams-Bashforth method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    f : callable\n",
    "        Function f(t, y) defining dy/dt = f(t, y)\n",
    "    t_span : tuple\n",
    "        (t_start, t_end)\n",
    "    y0 : float\n",
    "        Initial condition\n",
    "    h : float\n",
    "        Step size\n",
    "    order : int\n",
    "        Order of Adams-Bashforth method (1, 2, 3, or 4)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    t : ndarray\n",
    "        Time points\n",
    "    y : ndarray\n",
    "        Solution values\n",
    "    \"\"\"\n",
    "    t_start, t_end = t_span\n",
    "    n_steps = int((t_end - t_start) / h)\n",
    "    \n",
    "    t = np.linspace(t_start, t_start + n_steps * h, n_steps + 1)\n",
    "    y = np.zeros(n_steps + 1)\n",
    "    y[0] = y0\n",
    "    \n",
    "    # Store function evaluations\n",
    "    f_vals = np.zeros(n_steps + 1)\n",
    "    f_vals[0] = f(t[0], y[0])\n",
    "    \n",
    "    # Bootstrap using RK4 for the first (order-1) steps\n",
    "    for i in range(min(order - 1, n_steps)):\n",
    "        y[i + 1] = rk4_step(f, t[i], y[i], h)\n",
    "        f_vals[i + 1] = f(t[i + 1], y[i + 1])\n",
    "    \n",
    "    # Adams-Bashforth coefficients\n",
    "    if order == 1:\n",
    "        coeffs = [1]\n",
    "    elif order == 2:\n",
    "        coeffs = [3/2, -1/2]\n",
    "    elif order == 3:\n",
    "        coeffs = [23/12, -16/12, 5/12]\n",
    "    elif order == 4:\n",
    "        coeffs = [55/24, -59/24, 37/24, -9/24]\n",
    "    else:\n",
    "        raise ValueError(\"Order must be 1, 2, 3, or 4\")\n",
    "    \n",
    "    # Main Adams-Bashforth iteration\n",
    "    for n in range(order - 1, n_steps):\n",
    "        # Compute weighted sum of previous function values\n",
    "        increment = sum(coeffs[j] * f_vals[n - j] for j in range(order))\n",
    "        y[n + 1] = y[n] + h * increment\n",
    "        f_vals[n + 1] = f(t[n + 1], y[n + 1])\n",
    "    \n",
    "    return t, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Problem: Exponential Decay\n",
    "\n",
    "We test our implementation on the exponential decay equation:\n",
    "\n",
    "$$\\frac{dy}{dt} = -\\lambda y, \\quad y(0) = y_0$$\n",
    "\n",
    "The exact solution is:\n",
    "\n",
    "$$y(t) = y_0 e^{-\\lambda t}$$\n",
    "\n",
    "This allows us to compute the error and verify the convergence order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test problem\n",
    "lambda_decay = 2.0  # Decay constant\n",
    "y0 = 1.0  # Initial condition\n",
    "\n",
    "def f_decay(t: float, y: float) -> float:\n",
    "    \"\"\"ODE: dy/dt = -lambda * y\"\"\"\n",
    "    return -lambda_decay * y\n",
    "\n",
    "def exact_solution(t: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Exact solution: y = y0 * exp(-lambda * t)\"\"\"\n",
    "    return y0 * np.exp(-lambda_decay * t)\n",
    "\n",
    "# Solve with different orders\n",
    "t_span = (0.0, 3.0)\n",
    "h = 0.1\n",
    "\n",
    "results = {}\n",
    "for order in [1, 2, 3, 4]:\n",
    "    t, y = adams_bashforth(f_decay, t_span, y0, h, order=order)\n",
    "    results[order] = (t, y)\n",
    "\n",
    "# Compute exact solution\n",
    "t_exact = np.linspace(t_span[0], t_span[1], 200)\n",
    "y_exact = exact_solution(t_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Solution comparison\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(t_exact, y_exact, 'k-', linewidth=2, label='Exact')\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3']\n",
    "markers = ['o', 's', '^', 'd']\n",
    "for i, order in enumerate([1, 2, 3, 4]):\n",
    "    t, y = results[order]\n",
    "    ax1.plot(t, y, f'{markers[i]}-', color=colors[i], \n",
    "             markersize=4, linewidth=1, alpha=0.8, label=f'AB{order}')\n",
    "ax1.set_xlabel('Time $t$', fontsize=11)\n",
    "ax1.set_ylabel('$y(t)$', fontsize=11)\n",
    "ax1.set_title('Adams-Bashforth Methods: Solution Comparison', fontsize=12)\n",
    "ax1.legend(loc='upper right', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error over time\n",
    "ax2 = axes[0, 1]\n",
    "for i, order in enumerate([1, 2, 3, 4]):\n",
    "    t, y = results[order]\n",
    "    error = np.abs(y - exact_solution(t))\n",
    "    ax2.semilogy(t, error + 1e-16, f'{markers[i]}-', color=colors[i], \n",
    "                 markersize=4, linewidth=1, alpha=0.8, label=f'AB{order}')\n",
    "ax2.set_xlabel('Time $t$', fontsize=11)\n",
    "ax2.set_ylabel('Absolute Error', fontsize=11)\n",
    "ax2.set_title('Error Evolution Over Time', fontsize=12)\n",
    "ax2.legend(loc='lower right', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Convergence study\n",
    "ax3 = axes[1, 0]\n",
    "step_sizes = [0.2, 0.1, 0.05, 0.025, 0.0125]\n",
    "errors = {order: [] for order in [1, 2, 3, 4]}\n",
    "\n",
    "for h_test in step_sizes:\n",
    "    for order in [1, 2, 3, 4]:\n",
    "        t, y = adams_bashforth(f_decay, t_span, y0, h_test, order=order)\n",
    "        max_error = np.max(np.abs(y - exact_solution(t)))\n",
    "        errors[order].append(max_error)\n",
    "\n",
    "for i, order in enumerate([1, 2, 3, 4]):\n",
    "    ax3.loglog(step_sizes, errors[order], f'{markers[i]}-', color=colors[i], \n",
    "               markersize=6, linewidth=1.5, label=f'AB{order}')\n",
    "\n",
    "# Add reference slopes\n",
    "h_ref = np.array([0.2, 0.0125])\n",
    "for p, ls in [(1, ':'), (2, '--'), (3, '-.'), (4, '-')]:\n",
    "    scale = errors[p][0] / h_ref[0]**p\n",
    "    ax3.loglog(h_ref, scale * h_ref**p, ls, color='gray', alpha=0.5, linewidth=1)\n",
    "\n",
    "ax3.set_xlabel('Step size $h$', fontsize=11)\n",
    "ax3.set_ylabel('Maximum Error', fontsize=11)\n",
    "ax3.set_title('Convergence: Error vs Step Size', fontsize=12)\n",
    "ax3.legend(loc='lower right', fontsize=9)\n",
    "ax3.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Plot 4: Estimated convergence orders\n",
    "ax4 = axes[1, 1]\n",
    "h_arr = np.array(step_sizes)\n",
    "for i, order in enumerate([1, 2, 3, 4]):\n",
    "    err_arr = np.array(errors[order])\n",
    "    # Compute numerical order of convergence\n",
    "    p_est = np.log(err_arr[:-1] / err_arr[1:]) / np.log(h_arr[:-1] / h_arr[1:])\n",
    "    ax4.plot(range(1, len(p_est) + 1), p_est, f'{markers[i]}-', color=colors[i], \n",
    "             markersize=6, linewidth=1.5, label=f'AB{order} (expected: {order})')\n",
    "    ax4.axhline(y=order, color=colors[i], linestyle=':', alpha=0.5)\n",
    "\n",
    "ax4.set_xlabel('Refinement Level', fontsize=11)\n",
    "ax4.set_ylabel('Estimated Order $p$', fontsize=11)\n",
    "ax4.set_title('Numerical Convergence Order Estimation', fontsize=12)\n",
    "ax4.legend(loc='lower right', fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_ylim([0, 5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Discussion\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Convergence Order**: Each Adams-Bashforth method achieves its theoretical convergence order. AB1 is $O(h)$, AB2 is $O(h^2)$, AB3 is $O(h^3)$, and AB4 is $O(h^4)$.\n",
    "\n",
    "2. **Efficiency**: Higher-order methods require fewer function evaluations per step compared to equivalent-order Runge-Kutta methods, making them computationally efficient for smooth problems.\n",
    "\n",
    "3. **Bootstrap Requirement**: Multi-step methods need starting values from a single-step method (here RK4), which is a minor overhead.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Only **one function evaluation per step** after initialization\n",
    "- High accuracy for smooth solutions\n",
    "- Easy to implement and extend to higher orders\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Smaller stability regions compared to implicit methods\n",
    "- Not suitable for stiff equations without modification\n",
    "- Requires uniform step sizes (adaptive variants exist but are more complex)\n",
    "\n",
    "### Extensions\n",
    "\n",
    "- **Adams-Moulton methods**: Implicit variants with larger stability regions\n",
    "- **Predictor-corrector schemes**: Combine Adams-Bashforth (predictor) with Adams-Moulton (corrector)\n",
    "- **Variable step-size methods**: Adapt $h$ based on local error estimates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
