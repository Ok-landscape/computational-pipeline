{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarm Optimization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Particle Swarm Optimization (PSO) is a computational method that optimizes a problem by iteratively improving candidate solutions with regard to a given measure of quality. Introduced by Kennedy and Eberhart in 1995, PSO is inspired by the social behavior of bird flocking and fish schooling.\n",
    "\n",
    "PSO belongs to the class of metaheuristic optimization algorithms and is particularly effective for continuous nonlinear optimization problems where gradient information is unavailable or unreliable.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Particle Representation\n",
    "\n",
    "Consider a swarm of $N$ particles in a $D$-dimensional search space. Each particle $i$ is characterized by:\n",
    "\n",
    "- **Position vector**: $\\mathbf{x}_i = (x_{i1}, x_{i2}, \\ldots, x_{iD})$\n",
    "- **Velocity vector**: $\\mathbf{v}_i = (v_{i1}, v_{i2}, \\ldots, v_{iD})$\n",
    "- **Personal best position**: $\\mathbf{p}_i = (p_{i1}, p_{i2}, \\ldots, p_{iD})$\n",
    "\n",
    "The swarm also maintains a **global best position**:\n",
    "$$\\mathbf{g} = (g_1, g_2, \\ldots, g_D)$$\n",
    "\n",
    "### Velocity and Position Update Equations\n",
    "\n",
    "At each iteration $t$, the velocity and position of each particle are updated according to:\n",
    "\n",
    "$$v_{id}^{(t+1)} = w \\cdot v_{id}^{(t)} + c_1 r_1 (p_{id} - x_{id}^{(t)}) + c_2 r_2 (g_d - x_{id}^{(t)})$$\n",
    "\n",
    "$$x_{id}^{(t+1)} = x_{id}^{(t)} + v_{id}^{(t+1)}$$\n",
    "\n",
    "where:\n",
    "- $w$ is the **inertia weight** controlling the influence of previous velocity\n",
    "- $c_1$ is the **cognitive coefficient** (personal learning factor)\n",
    "- $c_2$ is the **social coefficient** (global learning factor)\n",
    "- $r_1, r_2 \\sim \\mathcal{U}(0, 1)$ are random numbers drawn from uniform distribution\n",
    "\n",
    "### Inertia Weight Strategies\n",
    "\n",
    "A common approach is to use a **linearly decreasing inertia weight**:\n",
    "\n",
    "$$w^{(t)} = w_{\\max} - \\frac{(w_{\\max} - w_{\\min}) \\cdot t}{T_{\\max}}$$\n",
    "\n",
    "where $T_{\\max}$ is the maximum number of iterations. This strategy promotes exploration in early iterations and exploitation in later iterations.\n",
    "\n",
    "### Personal and Global Best Updates\n",
    "\n",
    "The personal best is updated as:\n",
    "$$\\mathbf{p}_i^{(t+1)} = \\begin{cases} \\mathbf{x}_i^{(t+1)} & \\text{if } f(\\mathbf{x}_i^{(t+1)}) < f(\\mathbf{p}_i^{(t)}) \\\\ \\mathbf{p}_i^{(t)} & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "The global best is:\n",
    "$$\\mathbf{g}^{(t+1)} = \\arg\\min_{\\mathbf{p}_i^{(t+1)}} f(\\mathbf{p}_i^{(t+1)}), \\quad i = 1, 2, \\ldots, N$$\n",
    "\n",
    "### Velocity Clamping\n",
    "\n",
    "To prevent particles from leaving the search space, velocities are typically clamped:\n",
    "$$v_{id} = \\text{clip}(v_{id}, -v_{\\max}, v_{\\max})$$\n",
    "\n",
    "where $v_{\\max}$ is often set as a fraction of the search space range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement PSO to minimize the **Rastrigin function**, a standard benchmark for optimization algorithms:\n",
    "\n",
    "$$f(\\mathbf{x}) = An + \\sum_{i=1}^{n} \\left[ x_i^2 - A\\cos(2\\pi x_i) \\right]$$\n",
    "\n",
    "where $A = 10$. This function is highly multimodal with a global minimum at $\\mathbf{x}^* = \\mathbf{0}$ where $f(\\mathbf{x}^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rastrigin(x):\n",
    "    \"\"\"\n",
    "    Rastrigin function - a standard optimization benchmark.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : ndarray\n",
    "        Input vector of shape (D,) or (N, D)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float or ndarray\n",
    "        Function value(s)\n",
    "    \"\"\"\n",
    "    A = 10\n",
    "    if x.ndim == 1:\n",
    "        n = len(x)\n",
    "        return A * n + np.sum(x**2 - A * np.cos(2 * np.pi * x))\n",
    "    else:\n",
    "        n = x.shape[1]\n",
    "        return A * n + np.sum(x**2 - A * np.cos(2 * np.pi * x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleSwarmOptimizer:\n",
    "    \"\"\"\n",
    "    Particle Swarm Optimization implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    objective_func : callable\n",
    "        The objective function to minimize\n",
    "    n_particles : int\n",
    "        Number of particles in the swarm\n",
    "    n_dimensions : int\n",
    "        Dimensionality of the search space\n",
    "    bounds : tuple\n",
    "        (lower_bound, upper_bound) for all dimensions\n",
    "    w_max : float\n",
    "        Maximum inertia weight\n",
    "    w_min : float\n",
    "        Minimum inertia weight\n",
    "    c1 : float\n",
    "        Cognitive coefficient\n",
    "    c2 : float\n",
    "        Social coefficient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, objective_func, n_particles=30, n_dimensions=2,\n",
    "                 bounds=(-5.12, 5.12), w_max=0.9, w_min=0.4, c1=2.0, c2=2.0):\n",
    "        self.objective_func = objective_func\n",
    "        self.n_particles = n_particles\n",
    "        self.n_dimensions = n_dimensions\n",
    "        self.bounds = bounds\n",
    "        self.w_max = w_max\n",
    "        self.w_min = w_min\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        \n",
    "        # Velocity limits\n",
    "        self.v_max = (bounds[1] - bounds[0]) * 0.2\n",
    "        \n",
    "        # Initialize particles\n",
    "        self._initialize_swarm()\n",
    "        \n",
    "        # History for visualization\n",
    "        self.history = {\n",
    "            'global_best_fitness': [],\n",
    "            'positions': [],\n",
    "            'global_best_position': []\n",
    "        }\n",
    "    \n",
    "    def _initialize_swarm(self):\n",
    "        \"\"\"Initialize particle positions and velocities randomly.\"\"\"\n",
    "        lb, ub = self.bounds\n",
    "        \n",
    "        # Random initial positions\n",
    "        self.positions = np.random.uniform(lb, ub, \n",
    "                                           (self.n_particles, self.n_dimensions))\n",
    "        \n",
    "        # Random initial velocities\n",
    "        self.velocities = np.random.uniform(-self.v_max, self.v_max,\n",
    "                                            (self.n_particles, self.n_dimensions))\n",
    "        \n",
    "        # Evaluate initial fitness\n",
    "        self.fitness = self.objective_func(self.positions)\n",
    "        \n",
    "        # Initialize personal bests\n",
    "        self.personal_best_positions = self.positions.copy()\n",
    "        self.personal_best_fitness = self.fitness.copy()\n",
    "        \n",
    "        # Initialize global best\n",
    "        best_idx = np.argmin(self.personal_best_fitness)\n",
    "        self.global_best_position = self.personal_best_positions[best_idx].copy()\n",
    "        self.global_best_fitness = self.personal_best_fitness[best_idx]\n",
    "    \n",
    "    def optimize(self, max_iterations=100, verbose=True):\n",
    "        \"\"\"\n",
    "        Run the PSO algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_iterations : int\n",
    "            Maximum number of iterations\n",
    "        verbose : bool\n",
    "            Print progress information\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (best_position, best_fitness)\n",
    "        \"\"\"\n",
    "        for t in range(max_iterations):\n",
    "            # Calculate inertia weight (linearly decreasing)\n",
    "            w = self.w_max - (self.w_max - self.w_min) * t / max_iterations\n",
    "            \n",
    "            # Update velocities and positions\n",
    "            r1 = np.random.random((self.n_particles, self.n_dimensions))\n",
    "            r2 = np.random.random((self.n_particles, self.n_dimensions))\n",
    "            \n",
    "            # Velocity update equation\n",
    "            cognitive = self.c1 * r1 * (self.personal_best_positions - self.positions)\n",
    "            social = self.c2 * r2 * (self.global_best_position - self.positions)\n",
    "            self.velocities = w * self.velocities + cognitive + social\n",
    "            \n",
    "            # Velocity clamping\n",
    "            self.velocities = np.clip(self.velocities, -self.v_max, self.v_max)\n",
    "            \n",
    "            # Position update\n",
    "            self.positions = self.positions + self.velocities\n",
    "            \n",
    "            # Boundary handling (reflection)\n",
    "            lb, ub = self.bounds\n",
    "            below_lb = self.positions < lb\n",
    "            above_ub = self.positions > ub\n",
    "            self.positions[below_lb] = lb + (lb - self.positions[below_lb])\n",
    "            self.positions[above_ub] = ub - (self.positions[above_ub] - ub)\n",
    "            self.positions = np.clip(self.positions, lb, ub)\n",
    "            \n",
    "            # Reverse velocity for reflected particles\n",
    "            self.velocities[below_lb | above_ub] *= -0.5\n",
    "            \n",
    "            # Evaluate fitness\n",
    "            self.fitness = self.objective_func(self.positions)\n",
    "            \n",
    "            # Update personal bests\n",
    "            improved = self.fitness < self.personal_best_fitness\n",
    "            self.personal_best_positions[improved] = self.positions[improved]\n",
    "            self.personal_best_fitness[improved] = self.fitness[improved]\n",
    "            \n",
    "            # Update global best\n",
    "            best_idx = np.argmin(self.personal_best_fitness)\n",
    "            if self.personal_best_fitness[best_idx] < self.global_best_fitness:\n",
    "                self.global_best_position = self.personal_best_positions[best_idx].copy()\n",
    "                self.global_best_fitness = self.personal_best_fitness[best_idx]\n",
    "            \n",
    "            # Record history\n",
    "            self.history['global_best_fitness'].append(self.global_best_fitness)\n",
    "            self.history['positions'].append(self.positions.copy())\n",
    "            self.history['global_best_position'].append(self.global_best_position.copy())\n",
    "            \n",
    "            if verbose and (t + 1) % 10 == 0:\n",
    "                print(f\"Iteration {t+1:3d}: Best fitness = {self.global_best_fitness:.6e}\")\n",
    "        \n",
    "        return self.global_best_position, self.global_best_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PSO optimizer\n",
    "pso = ParticleSwarmOptimizer(\n",
    "    objective_func=rastrigin,\n",
    "    n_particles=40,\n",
    "    n_dimensions=2,\n",
    "    bounds=(-5.12, 5.12),\n",
    "    w_max=0.9,\n",
    "    w_min=0.4,\n",
    "    c1=2.0,\n",
    "    c2=2.0\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "best_position, best_fitness = pso.optimize(max_iterations=100)\n",
    "\n",
    "print(f\"\\nOptimization Results:\")\n",
    "print(f\"Best position: [{best_position[0]:.6f}, {best_position[1]:.6f}]\")\n",
    "print(f\"Best fitness: {best_fitness:.6e}\")\n",
    "print(f\"True optimum: [0.0, 0.0] with fitness 0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We create a comprehensive visualization showing:\n",
    "1. The Rastrigin function landscape with the optimization path\n",
    "2. Convergence curve of the best fitness over iterations\n",
    "3. Particle swarm evolution at different stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with multiple subplots\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Subplot 1: 3D surface plot of Rastrigin function\n",
    "ax1 = fig.add_subplot(2, 2, 1, projection='3d')\n",
    "x = np.linspace(-5.12, 5.12, 100)\n",
    "y = np.linspace(-5.12, 5.12, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = 10 * 2 + (X**2 - 10 * np.cos(2 * np.pi * X)) + (Y**2 - 10 * np.cos(2 * np.pi * Y))\n",
    "\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.7, linewidth=0)\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_zlabel('$f(x_1, x_2)$')\n",
    "ax1.set_title('Rastrigin Function Landscape')\n",
    "ax1.view_init(elev=30, azim=45)\n",
    "\n",
    "# Subplot 2: Convergence curve\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "iterations = range(1, len(pso.history['global_best_fitness']) + 1)\n",
    "ax2.semilogy(iterations, pso.history['global_best_fitness'], 'b-', linewidth=2)\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Best Fitness (log scale)')\n",
    "ax2.set_title('Convergence Curve')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0.01, color='r', linestyle='--', label='Tolerance = 0.01')\n",
    "ax2.legend()\n",
    "\n",
    "# Subplot 3: Contour plot with particle trajectories\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "contour = ax3.contour(X, Y, Z, levels=20, cmap=cm.viridis)\n",
    "ax3.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Plot particle positions at different iterations\n",
    "iterations_to_plot = [0, 24, 49, 99]\n",
    "colors = ['red', 'orange', 'yellow', 'lime']\n",
    "for idx, it in enumerate(iterations_to_plot):\n",
    "    if it < len(pso.history['positions']):\n",
    "        positions = pso.history['positions'][it]\n",
    "        ax3.scatter(positions[:, 0], positions[:, 1], c=colors[idx], \n",
    "                   s=20, alpha=0.6, label=f'Iteration {it+1}')\n",
    "\n",
    "# Plot global best trajectory\n",
    "gbest_history = np.array(pso.history['global_best_position'])\n",
    "ax3.plot(gbest_history[:, 0], gbest_history[:, 1], 'k--', linewidth=1, alpha=0.5)\n",
    "ax3.scatter(best_position[0], best_position[1], c='black', s=100, \n",
    "           marker='*', label='Final Best', zorder=5)\n",
    "ax3.scatter(0, 0, c='white', s=100, marker='x', linewidths=2,\n",
    "           label='True Optimum', zorder=5)\n",
    "\n",
    "ax3.set_xlabel('$x_1$')\n",
    "ax3.set_ylabel('$x_2$')\n",
    "ax3.set_title('Particle Swarm Evolution')\n",
    "ax3.legend(loc='upper right', fontsize=8)\n",
    "ax3.set_xlim(-5.12, 5.12)\n",
    "ax3.set_ylim(-5.12, 5.12)\n",
    "\n",
    "# Subplot 4: Swarm diversity over iterations\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "# Calculate swarm diversity (average distance from swarm center)\n",
    "diversity = []\n",
    "for positions in pso.history['positions']:\n",
    "    center = np.mean(positions, axis=0)\n",
    "    distances = np.linalg.norm(positions - center, axis=1)\n",
    "    diversity.append(np.mean(distances))\n",
    "\n",
    "ax4.plot(iterations, diversity, 'g-', linewidth=2)\n",
    "ax4.set_xlabel('Iteration')\n",
    "ax4.set_ylabel('Swarm Diversity')\n",
    "ax4.set_title('Swarm Diversity Over Time')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Discussion\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Convergence Behavior**: The convergence curve shows rapid initial improvement followed by gradual refinement. This is characteristic of PSO's exploration-exploitation balance.\n",
    "\n",
    "2. **Swarm Diversity**: The diversity metric decreases over iterations, indicating that particles converge toward the global best. The linearly decreasing inertia weight facilitates this transition from exploration to exploitation.\n",
    "\n",
    "3. **Parameter Sensitivity**: The performance of PSO is sensitive to parameter choices:\n",
    "   - Higher $c_1$ promotes individual exploration\n",
    "   - Higher $c_2$ promotes social learning\n",
    "   - Inertia weight $w$ balances momentum and responsiveness\n",
    "\n",
    "### Advantages of PSO\n",
    "\n",
    "- **Simplicity**: Few parameters to tune compared to other metaheuristics\n",
    "- **No gradient required**: Suitable for non-differentiable or noisy functions\n",
    "- **Parallelizable**: Particle evaluations are independent\n",
    "- **Memory**: Particles retain knowledge of personal and global bests\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Premature convergence**: May converge to local optima in highly multimodal landscapes\n",
    "- **No convergence guarantee**: Unlike gradient methods, no theoretical convergence proof\n",
    "- **Parameter sensitivity**: Performance depends on proper tuning of $w$, $c_1$, $c_2$\n",
    "\n",
    "### Extensions\n",
    "\n",
    "Several variants address these limitations:\n",
    "- **Adaptive PSO**: Dynamically adjusts parameters based on swarm behavior\n",
    "- **Topology variants**: Ring, von Neumann, or dynamic topologies instead of global best\n",
    "- **Hybrid methods**: Combine PSO with local search or other optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Particle Swarm Optimization provides an elegant and effective approach to continuous optimization problems. Its biologically-inspired mechanics—balancing individual experience with social information sharing—lead to robust search behavior across diverse problem landscapes.\n",
    "\n",
    "The algorithm successfully minimized the Rastrigin function, a challenging multimodal benchmark, demonstrating its capability to escape local optima through swarm intelligence. The visualization reveals the characteristic behavior of PSO: initial exploration across the search space, followed by convergence as particles are attracted to promising regions.\n",
    "\n",
    "For practitioners, PSO offers a good balance between simplicity and effectiveness, making it a valuable tool in the optimization toolkit alongside gradient-based methods and other metaheuristics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
