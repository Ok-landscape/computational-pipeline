{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference: A Computational Introduction\n",
    "\n",
    "## 1. Theoretical Foundation\n",
    "\n",
    "Bayesian inference is a method of statistical inference that uses Bayes' theorem to update the probability of a hypothesis as more evidence becomes available. Unlike frequentist approaches, Bayesian methods treat parameters as random variables with probability distributions.\n",
    "\n",
    "### 1.1 Bayes' Theorem\n",
    "\n",
    "The cornerstone of Bayesian inference is **Bayes' theorem**:\n",
    "\n",
    "$$P(\\theta | D) = \\frac{P(D | \\theta) \\cdot P(\\theta)}{P(D)}$$\n",
    "\n",
    "Where:\n",
    "- $P(\\theta | D)$ is the **posterior distribution** — our updated belief about parameter $\\theta$ after observing data $D$\n",
    "- $P(D | \\theta)$ is the **likelihood** — the probability of observing data $D$ given parameter $\\theta$\n",
    "- $P(\\theta)$ is the **prior distribution** — our initial belief about $\\theta$ before seeing data\n",
    "- $P(D)$ is the **marginal likelihood** (evidence) — a normalizing constant\n",
    "\n",
    "### 1.2 Conjugate Priors\n",
    "\n",
    "A prior is said to be **conjugate** to a likelihood if the posterior distribution belongs to the same family as the prior. This property enables analytical solutions.\n",
    "\n",
    "#### Beta-Binomial Conjugacy\n",
    "\n",
    "For binomial likelihood with parameter $\\theta$ (probability of success):\n",
    "\n",
    "$$X | \\theta \\sim \\text{Binomial}(n, \\theta)$$\n",
    "\n",
    "With a Beta prior:\n",
    "\n",
    "$$\\theta \\sim \\text{Beta}(\\alpha, \\beta)$$\n",
    "\n",
    "The posterior is also Beta:\n",
    "\n",
    "$$\\theta | X \\sim \\text{Beta}(\\alpha + X, \\beta + n - X)$$\n",
    "\n",
    "The Beta distribution PDF is:\n",
    "\n",
    "$$f(\\theta; \\alpha, \\beta) = \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)}$$\n",
    "\n",
    "where $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the Beta function.\n",
    "\n",
    "### 1.3 Posterior Predictive Distribution\n",
    "\n",
    "To predict future observations $\\tilde{X}$, we integrate over the posterior:\n",
    "\n",
    "$$P(\\tilde{X} | D) = \\int P(\\tilde{X} | \\theta) P(\\theta | D) \\, d\\theta$$\n",
    "\n",
    "### 1.4 Credible Intervals\n",
    "\n",
    "A $(1-\\alpha)$ **credible interval** $[a, b]$ satisfies:\n",
    "\n",
    "$$P(a \\leq \\theta \\leq b | D) = 1 - \\alpha$$\n",
    "\n",
    "Unlike frequentist confidence intervals, credible intervals have a direct probabilistic interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation\n",
    "\n",
    "We will demonstrate Bayesian inference using the Beta-Binomial model to estimate the probability of success in a series of Bernoulli trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import beta as beta_func\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generate Synthetic Data\n",
    "\n",
    "We simulate coin flips from a biased coin with true probability $\\theta_{\\text{true}} = 0.7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameter (unknown in practice)\n",
    "theta_true = 0.7\n",
    "\n",
    "# Generate data: n Bernoulli trials\n",
    "n_trials = 100\n",
    "data = np.random.binomial(1, theta_true, n_trials)\n",
    "\n",
    "# Summary statistics\n",
    "n_successes = np.sum(data)\n",
    "n_failures = n_trials - n_successes\n",
    "\n",
    "print(f\"Number of trials: {n_trials}\")\n",
    "print(f\"Number of successes: {n_successes}\")\n",
    "print(f\"Sample proportion: {n_successes/n_trials:.3f}\")\n",
    "print(f\"True probability: {theta_true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define Prior, Likelihood, and Posterior\n",
    "\n",
    "We compare three different priors:\n",
    "1. **Uniform prior**: $\\text{Beta}(1, 1)$ — no prior information\n",
    "2. **Informative prior**: $\\text{Beta}(5, 5)$ — centered at 0.5\n",
    "3. **Strong prior**: $\\text{Beta}(30, 10)$ — prior belief that $\\theta \\approx 0.75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define priors\n",
    "priors = {\n",
    "    'Uniform (1, 1)': (1, 1),\n",
    "    'Weakly Informative (5, 5)': (5, 5),\n",
    "    'Strong Prior (30, 10)': (30, 10)\n",
    "}\n",
    "\n",
    "# Compute posteriors using conjugate update\n",
    "posteriors = {}\n",
    "for name, (alpha_prior, beta_prior) in priors.items():\n",
    "    alpha_post = alpha_prior + n_successes\n",
    "    beta_post = beta_prior + n_failures\n",
    "    posteriors[name] = (alpha_post, beta_post)\n",
    "    \n",
    "    # Compute posterior statistics\n",
    "    post_mean = alpha_post / (alpha_post + beta_post)\n",
    "    post_mode = (alpha_post - 1) / (alpha_post + beta_post - 2) if alpha_post > 1 and beta_post > 1 else np.nan\n",
    "    post_var = (alpha_post * beta_post) / ((alpha_post + beta_post)**2 * (alpha_post + beta_post + 1))\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Prior: Beta({alpha_prior}, {beta_prior})\")\n",
    "    print(f\"  Posterior: Beta({alpha_post}, {beta_post})\")\n",
    "    print(f\"  Posterior mean: {post_mean:.4f}\")\n",
    "    print(f\"  Posterior mode: {post_mode:.4f}\")\n",
    "    print(f\"  Posterior std: {np.sqrt(post_var):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualization of Bayesian Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create theta grid for plotting\n",
    "theta = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Colors for different priors\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "\n",
    "# Plot 1: Prior distributions\n",
    "ax1 = axes[0, 0]\n",
    "for (name, (alpha, beta)), color in zip(priors.items(), colors):\n",
    "    pdf = stats.beta.pdf(theta, alpha, beta)\n",
    "    ax1.plot(theta, pdf, label=name, color=color, linewidth=2)\n",
    "ax1.axvline(theta_true, color='black', linestyle='--', linewidth=1.5, label=f'True θ = {theta_true}')\n",
    "ax1.set_xlabel(r'$\\theta$')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Prior Distributions')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_xlim(0, 1)\n",
    "\n",
    "# Plot 2: Posterior distributions\n",
    "ax2 = axes[0, 1]\n",
    "for (name, (alpha, beta)), color in zip(posteriors.items(), colors):\n",
    "    pdf = stats.beta.pdf(theta, alpha, beta)\n",
    "    ax2.plot(theta, pdf, label=name, color=color, linewidth=2)\n",
    "ax2.axvline(theta_true, color='black', linestyle='--', linewidth=1.5, label=f'True θ = {theta_true}')\n",
    "ax2.set_xlabel(r'$\\theta$')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title(f'Posterior Distributions (n={n_trials}, successes={n_successes})')\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.set_xlim(0, 1)\n",
    "\n",
    "# Plot 3: Sequential Bayesian updating\n",
    "ax3 = axes[1, 0]\n",
    "alpha_seq, beta_seq = 1, 1  # Start with uniform prior\n",
    "update_points = [0, 10, 25, 50, 100]\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "for i, n in enumerate(update_points):\n",
    "    if n == 0:\n",
    "        alpha_n, beta_n = alpha_seq, beta_seq\n",
    "    else:\n",
    "        successes_n = np.sum(data[:n])\n",
    "        alpha_n = alpha_seq + successes_n\n",
    "        beta_n = beta_seq + (n - successes_n)\n",
    "    \n",
    "    pdf = stats.beta.pdf(theta, alpha_n, beta_n)\n",
    "    color = cmap(i / len(update_points))\n",
    "    ax3.plot(theta, pdf, label=f'n = {n}', color=color, linewidth=2)\n",
    "\n",
    "ax3.axvline(theta_true, color='red', linestyle='--', linewidth=1.5, label=f'True θ')\n",
    "ax3.set_xlabel(r'$\\theta$')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Sequential Bayesian Updating')\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.set_xlim(0, 1)\n",
    "\n",
    "# Plot 4: 95% Credible intervals\n",
    "ax4 = axes[1, 1]\n",
    "y_positions = np.arange(len(posteriors))\n",
    "\n",
    "for i, ((name, (alpha, beta)), color) in enumerate(zip(posteriors.items(), colors)):\n",
    "    # Compute 95% credible interval\n",
    "    ci_lower = stats.beta.ppf(0.025, alpha, beta)\n",
    "    ci_upper = stats.beta.ppf(0.975, alpha, beta)\n",
    "    mean = alpha / (alpha + beta)\n",
    "    \n",
    "    # Plot credible interval\n",
    "    ax4.hlines(y=i, xmin=ci_lower, xmax=ci_upper, color=color, linewidth=4, alpha=0.7)\n",
    "    ax4.plot(mean, i, 'o', color=color, markersize=10)\n",
    "    \n",
    "    # Annotate\n",
    "    ax4.text(ci_upper + 0.02, i, f'[{ci_lower:.3f}, {ci_upper:.3f}]', \n",
    "             va='center', fontsize=9)\n",
    "\n",
    "ax4.axvline(theta_true, color='black', linestyle='--', linewidth=1.5, label=f'True θ = {theta_true}')\n",
    "ax4.set_yticks(y_positions)\n",
    "ax4.set_yticklabels([name.split('(')[0].strip() for name in posteriors.keys()])\n",
    "ax4.set_xlabel(r'$\\theta$')\n",
    "ax4.set_title('95% Credible Intervals')\n",
    "ax4.set_xlim(0, 1.15)\n",
    "ax4.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Posterior Predictive Distribution\n",
    "\n",
    "We compute the probability of observing $k$ successes in the next $m$ trials, marginalizing over the posterior uncertainty in $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_binomial_pmf(k, n, alpha, beta):\n",
    "    \"\"\"\n",
    "    Compute the Beta-Binomial PMF (posterior predictive for Binomial likelihood).\n",
    "    \n",
    "    P(k | n, alpha, beta) = C(n,k) * B(k+alpha, n-k+beta) / B(alpha, beta)\n",
    "    \"\"\"\n",
    "    from scipy.special import comb, betaln\n",
    "    log_pmf = (np.log(comb(n, k, exact=False)) + \n",
    "               betaln(k + alpha, n - k + beta) - \n",
    "               betaln(alpha, beta))\n",
    "    return np.exp(log_pmf)\n",
    "\n",
    "# Predict next m trials\n",
    "m = 20\n",
    "k_values = np.arange(0, m + 1)\n",
    "\n",
    "# Using uniform prior posterior\n",
    "alpha_post, beta_post = posteriors['Uniform (1, 1)']\n",
    "predictive_pmf = [beta_binomial_pmf(k, m, alpha_post, beta_post) for k in k_values]\n",
    "\n",
    "# Compute expected value and variance\n",
    "expected_k = m * alpha_post / (alpha_post + beta_post)\n",
    "variance_k = (m * alpha_post * beta_post * (alpha_post + beta_post + m)) / \\\n",
    "             ((alpha_post + beta_post)**2 * (alpha_post + beta_post + 1))\n",
    "\n",
    "print(f\"Posterior Predictive for next {m} trials:\")\n",
    "print(f\"  Expected successes: {expected_k:.2f}\")\n",
    "print(f\"  Standard deviation: {np.sqrt(variance_k):.2f}\")\n",
    "print(f\"  Based on true θ = {theta_true}: expected = {m * theta_true:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize posterior predictive\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(k_values, predictive_pmf, color='#2E86AB', alpha=0.7, edgecolor='black')\n",
    "plt.axvline(expected_k, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Expected = {expected_k:.2f}')\n",
    "plt.axvline(m * theta_true, color='green', linestyle=':', linewidth=2,\n",
    "            label=f'True expected = {m * theta_true:.2f}')\n",
    "plt.xlabel('Number of Successes (k)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Posterior Predictive Distribution for Next {m} Trials')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison: Bayes Factor\n",
    "\n",
    "The **Bayes factor** compares two models $M_1$ and $M_2$:\n",
    "\n",
    "$$BF_{12} = \\frac{P(D | M_1)}{P(D | M_2)}$$\n",
    "\n",
    "For Beta-Binomial, the marginal likelihood is:\n",
    "\n",
    "$$P(D | \\alpha, \\beta) = \\binom{n}{k} \\frac{B(k + \\alpha, n - k + \\beta)}{B(\\alpha, \\beta)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import betaln, gammaln\n",
    "\n",
    "def log_marginal_likelihood(k, n, alpha, beta):\n",
    "    \"\"\"\n",
    "    Compute log marginal likelihood for Beta-Binomial model.\n",
    "    \"\"\"\n",
    "    log_binom = gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)\n",
    "    log_ml = log_binom + betaln(k + alpha, n - k + beta) - betaln(alpha, beta)\n",
    "    return log_ml\n",
    "\n",
    "# Compare priors\n",
    "print(\"Bayes Factor Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "log_mls = {}\n",
    "for name, (alpha, beta) in priors.items():\n",
    "    log_ml = log_marginal_likelihood(n_successes, n_trials, alpha, beta)\n",
    "    log_mls[name] = log_ml\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Log marginal likelihood: {log_ml:.4f}\")\n",
    "\n",
    "# Compute Bayes factors relative to uniform prior\n",
    "print(\"\\nBayes Factors (relative to Uniform prior):\")\n",
    "baseline_log_ml = log_mls['Uniform (1, 1)']\n",
    "for name, log_ml in log_mls.items():\n",
    "    bf = np.exp(log_ml - baseline_log_ml)\n",
    "    print(f\"  {name}: BF = {bf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Numerical Integration: Grid Approximation\n",
    "\n",
    "For non-conjugate models, we can use grid approximation to compute the posterior numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_approximation(data, prior_func, n_grid=1000):\n",
    "    \"\"\"\n",
    "    Compute posterior using grid approximation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Binary outcomes (0s and 1s)\n",
    "    prior_func : callable\n",
    "        Function that returns prior density at theta\n",
    "    n_grid : int\n",
    "        Number of grid points\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    theta_grid : array\n",
    "        Grid of theta values\n",
    "    posterior : array\n",
    "        Normalized posterior probabilities\n",
    "    \"\"\"\n",
    "    # Create grid (avoiding 0 and 1 for numerical stability)\n",
    "    theta_grid = np.linspace(0.001, 0.999, n_grid)\n",
    "    \n",
    "    # Compute prior\n",
    "    prior = np.array([prior_func(t) for t in theta_grid])\n",
    "    \n",
    "    # Compute likelihood\n",
    "    k = np.sum(data)\n",
    "    n = len(data)\n",
    "    likelihood = theta_grid**k * (1 - theta_grid)**(n - k)\n",
    "    \n",
    "    # Compute unnormalized posterior\n",
    "    posterior_unnorm = likelihood * prior\n",
    "    \n",
    "    # Normalize (numerical integration via trapezoid rule)\n",
    "    posterior = posterior_unnorm / np.trapz(posterior_unnorm, theta_grid)\n",
    "    \n",
    "    return theta_grid, posterior\n",
    "\n",
    "# Define a custom non-standard prior (mixture of Betas)\n",
    "def mixture_prior(theta):\n",
    "    \"\"\"Mixture of two Beta distributions.\"\"\"\n",
    "    return 0.5 * stats.beta.pdf(theta, 2, 8) + 0.5 * stats.beta.pdf(theta, 8, 2)\n",
    "\n",
    "# Compute grid approximation\n",
    "theta_grid, posterior_grid = grid_approximation(data, mixture_prior, n_grid=1000)\n",
    "\n",
    "# Compute posterior mean and mode\n",
    "posterior_mean_grid = np.trapz(theta_grid * posterior_grid, theta_grid)\n",
    "posterior_mode_grid = theta_grid[np.argmax(posterior_grid)]\n",
    "\n",
    "print(\"Grid Approximation with Mixture Prior:\")\n",
    "print(f\"  Posterior mean: {posterior_mean_grid:.4f}\")\n",
    "print(f\"  Posterior mode: {posterior_mode_grid:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid approximation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot mixture prior\n",
    "prior_values = [mixture_prior(t) for t in theta_grid]\n",
    "axes[0].plot(theta_grid, prior_values, 'b-', linewidth=2, label='Mixture Prior')\n",
    "axes[0].fill_between(theta_grid, prior_values, alpha=0.3)\n",
    "axes[0].set_xlabel(r'$\\theta$')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Bimodal Mixture Prior')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot posterior from grid approximation\n",
    "axes[1].plot(theta_grid, posterior_grid, 'r-', linewidth=2, label='Posterior (Grid Approx)')\n",
    "axes[1].fill_between(theta_grid, posterior_grid, alpha=0.3, color='red')\n",
    "axes[1].axvline(theta_true, color='black', linestyle='--', linewidth=1.5, \n",
    "                label=f'True θ = {theta_true}')\n",
    "axes[1].axvline(posterior_mean_grid, color='green', linestyle=':', linewidth=1.5,\n",
    "                label=f'Mean = {posterior_mean_grid:.3f}')\n",
    "axes[1].set_xlabel(r'$\\theta$')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Posterior via Grid Approximation')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "This notebook demonstrated key concepts in Bayesian inference:\n",
    "\n",
    "1. **Bayes' theorem** provides a principled framework for updating beliefs given data\n",
    "2. **Conjugate priors** (Beta-Binomial) enable analytical posterior computation\n",
    "3. **Prior sensitivity**: Different priors lead to different posteriors, but with sufficient data, posteriors converge\n",
    "4. **Credible intervals** provide direct probabilistic statements about parameter values\n",
    "5. **Posterior predictive** distributions account for parameter uncertainty in predictions\n",
    "6. **Bayes factors** enable formal model comparison\n",
    "7. **Grid approximation** extends Bayesian methods to non-conjugate models\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Bayesian methods naturally quantify uncertainty through probability distributions\n",
    "- The choice of prior matters most when data are limited\n",
    "- Conjugacy provides computational convenience but is not always realistic\n",
    "- Numerical methods (grid, MCMC) extend Bayesian inference to complex models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
