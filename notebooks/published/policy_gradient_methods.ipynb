{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods in Reinforcement Learning\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy without requiring a value function. Unlike value-based methods (e.g., Q-learning), policy gradient methods parameterize the policy and update it by gradient ascent on expected cumulative reward.\n",
    "\n",
    "## 2. Theoretical Foundation\n",
    "\n",
    "### 2.1 Policy Parameterization\n",
    "\n",
    "We define a stochastic policy $\\pi_\\theta(a|s)$ parameterized by $\\theta$, which gives the probability of taking action $a$ in state $s$. The objective is to maximize the expected return:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t \\right]$$\n",
    "\n",
    "where $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\ldots)$ is a trajectory, and $\\gamma \\in [0, 1]$ is the discount factor.\n",
    "\n",
    "### 2.2 Policy Gradient Theorem\n",
    "\n",
    "The policy gradient theorem provides a tractable expression for the gradient of $J(\\theta)$:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t \\right]$$\n",
    "\n",
    "where $G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$ is the return from time step $t$.\n",
    "\n",
    "### 2.3 REINFORCE Algorithm\n",
    "\n",
    "The REINFORCE algorithm (Williams, 1992) is a Monte Carlo policy gradient method:\n",
    "\n",
    "1. Sample trajectory $\\tau$ using policy $\\pi_\\theta$\n",
    "2. Compute returns $G_t$ for each time step\n",
    "3. Update: $\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "\n",
    "### 2.4 Variance Reduction with Baseline\n",
    "\n",
    "To reduce variance, we subtract a baseline $b(s_t)$ from the return:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot (G_t - b(s_t)) \\right]$$\n",
    "\n",
    "A common choice is the average return: $b = \\frac{1}{N} \\sum_{i=1}^{N} G_0^{(i)}$\n",
    "\n",
    "The term $(G_t - b(s_t))$ is called the **advantage**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "We will implement the REINFORCE algorithm to solve a simple continuous control problem: balancing a pole (CartPole-like environment simulated from scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Environment: Simplified CartPole\n",
    "\n",
    "We implement a simplified CartPole environment with discrete actions (left/right force)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnv:\n",
    "    \"\"\"Simplified CartPole environment.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Physical parameters\n",
    "        self.gravity = 9.8\n",
    "        self.cart_mass = 1.0\n",
    "        self.pole_mass = 0.1\n",
    "        self.total_mass = self.cart_mass + self.pole_mass\n",
    "        self.pole_length = 0.5\n",
    "        self.pole_mass_length = self.pole_mass * self.pole_length\n",
    "        self.force_mag = 10.0\n",
    "        self.dt = 0.02\n",
    "        \n",
    "        # Thresholds for termination\n",
    "        self.x_threshold = 2.4\n",
    "        self.theta_threshold = 12 * np.pi / 180  # 12 degrees\n",
    "        \n",
    "        self.state = None\n",
    "        self.steps = 0\n",
    "        self.max_steps = 200\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment.\"\"\"\n",
    "        self.state = np.random.uniform(-0.05, 0.05, size=4)\n",
    "        self.steps = 0\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment.\"\"\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        \n",
    "        # Apply force based on action\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        \n",
    "        # Physics simulation\n",
    "        cos_theta = np.cos(theta)\n",
    "        sin_theta = np.sin(theta)\n",
    "        \n",
    "        temp = (force + self.pole_mass_length * theta_dot**2 * sin_theta) / self.total_mass\n",
    "        theta_acc = (self.gravity * sin_theta - cos_theta * temp) / \\\n",
    "                    (self.pole_length * (4/3 - self.pole_mass * cos_theta**2 / self.total_mass))\n",
    "        x_acc = temp - self.pole_mass_length * theta_acc * cos_theta / self.total_mass\n",
    "        \n",
    "        # Euler integration\n",
    "        x = x + self.dt * x_dot\n",
    "        x_dot = x_dot + self.dt * x_acc\n",
    "        theta = theta + self.dt * theta_dot\n",
    "        theta_dot = theta_dot + self.dt * theta_acc\n",
    "        \n",
    "        self.state = np.array([x, x_dot, theta, theta_dot])\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Check termination\n",
    "        done = bool(\n",
    "            x < -self.x_threshold or x > self.x_threshold or\n",
    "            theta < -self.theta_threshold or theta > self.theta_threshold or\n",
    "            self.steps >= self.max_steps\n",
    "        )\n",
    "        \n",
    "        reward = 1.0 if not done else 0.0\n",
    "        \n",
    "        return self.state.copy(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Policy Network\n",
    "\n",
    "We use a simple linear policy with softmax action selection:\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\text{softmax}(W \\cdot s + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork:\n",
    "    \"\"\"Linear policy network with softmax output.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W = np.random.randn(action_dim, state_dim) * 0.1\n",
    "        self.b = np.zeros(action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Compute action probabilities.\"\"\"\n",
    "        logits = np.dot(self.W, state) + self.b\n",
    "        probs = softmax(logits)\n",
    "        return probs\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Sample action from policy.\"\"\"\n",
    "        probs = self.forward(state)\n",
    "        action = np.random.choice(self.action_dim, p=probs)\n",
    "        return action, probs\n",
    "    \n",
    "    def compute_log_prob_grad(self, state, action, probs):\n",
    "        \"\"\"Compute gradient of log probability.\"\"\"\n",
    "        # Gradient of log softmax\n",
    "        # d/dθ log π(a|s) = d/dθ (logits[a] - logsumexp(logits))\n",
    "        # = indicator(a) - softmax(logits)\n",
    "        \n",
    "        grad_logits = -probs.copy()\n",
    "        grad_logits[action] += 1\n",
    "        \n",
    "        # Gradient w.r.t. W and b\n",
    "        grad_W = np.outer(grad_logits, state)\n",
    "        grad_b = grad_logits\n",
    "        \n",
    "        return grad_W, grad_b\n",
    "    \n",
    "    def update(self, grad_W, grad_b, learning_rate):\n",
    "        \"\"\"Update policy parameters.\"\"\"\n",
    "        self.W += learning_rate * grad_W\n",
    "        self.b += learning_rate * grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 REINFORCE Algorithm with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy, num_episodes=1000, gamma=0.99, learning_rate=0.01):\n",
    "    \"\"\"REINFORCE algorithm with baseline.\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    baseline = 0  # Running average of returns\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Collect trajectory\n",
    "        states, actions, rewards, probs_list = [], [], [], []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, probs = policy.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            probs_list.append(probs)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Compute returns\n",
    "        T = len(rewards)\n",
    "        returns = np.zeros(T)\n",
    "        G = 0\n",
    "        for t in reversed(range(T)):\n",
    "            G = rewards[t] + gamma * G\n",
    "            returns[t] = G\n",
    "        \n",
    "        # Update baseline (moving average)\n",
    "        episode_return = returns[0]\n",
    "        baseline = 0.9 * baseline + 0.1 * episode_return\n",
    "        \n",
    "        # Compute policy gradient\n",
    "        total_grad_W = np.zeros_like(policy.W)\n",
    "        total_grad_b = np.zeros_like(policy.b)\n",
    "        \n",
    "        for t in range(T):\n",
    "            advantage = returns[t] - baseline\n",
    "            grad_W, grad_b = policy.compute_log_prob_grad(\n",
    "                states[t], actions[t], probs_list[t]\n",
    "            )\n",
    "            total_grad_W += advantage * grad_W\n",
    "            total_grad_b += advantage * grad_b\n",
    "        \n",
    "        # Update policy\n",
    "        policy.update(total_grad_W / T, total_grad_b / T, learning_rate)\n",
    "        \n",
    "        episode_rewards.append(sum(rewards))\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}, Avg Reward (last 100): {avg_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and policy\n",
    "env = CartPoleEnv()\n",
    "policy = PolicyNetwork(state_dim=4, action_dim=2)\n",
    "\n",
    "# Train using REINFORCE\n",
    "print(\"Training REINFORCE with Baseline...\\n\")\n",
    "rewards = reinforce(env, policy, num_episodes=1000, gamma=0.99, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute moving average\n",
    "window = 50\n",
    "moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Learning Curve\n",
    "axes[0].plot(rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "axes[0].plot(range(window-1, len(rewards)), moving_avg, color='red', \n",
    "             linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "axes[0].axhline(y=195, color='green', linestyle='--', label='Solved Threshold (195)')\n",
    "axes[0].set_xlabel('Episode', fontsize=12)\n",
    "axes[0].set_ylabel('Total Reward', fontsize=12)\n",
    "axes[0].set_title('REINFORCE Learning Curve', fontsize=14)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Policy Visualization\n",
    "# Visualize learned policy over state space (theta vs theta_dot)\n",
    "theta_range = np.linspace(-0.2, 0.2, 50)\n",
    "theta_dot_range = np.linspace(-2, 2, 50)\n",
    "policy_map = np.zeros((50, 50))\n",
    "\n",
    "for i, theta in enumerate(theta_range):\n",
    "    for j, theta_dot in enumerate(theta_dot_range):\n",
    "        state = np.array([0, 0, theta, theta_dot])  # x=0, x_dot=0\n",
    "        probs = policy.forward(state)\n",
    "        policy_map[j, i] = probs[1]  # Probability of action 1 (right)\n",
    "\n",
    "im = axes[1].imshow(policy_map, extent=[-0.2, 0.2, -2, 2], \n",
    "                    origin='lower', aspect='auto', cmap='RdBu')\n",
    "axes[1].set_xlabel(r'Pole Angle $\\theta$ (rad)', fontsize=12)\n",
    "axes[1].set_ylabel(r'Angular Velocity $\\dot{\\theta}$ (rad/s)', fontsize=12)\n",
    "axes[1].set_title('Learned Policy: P(action=right)', fontsize=14)\n",
    "cbar = plt.colorbar(im, ax=axes[1])\n",
    "cbar.set_label('Probability', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal average reward (last 100 episodes): {np.mean(rewards[-100:]):.2f}\")\n",
    "print(\"Plot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis\n",
    "\n",
    "### 5.1 Observations\n",
    "\n",
    "1. **Learning Dynamics**: The REINFORCE algorithm shows high variance in episode rewards due to its Monte Carlo nature. The baseline helps reduce this variance.\n",
    "\n",
    "2. **Policy Interpretation**: The learned policy shows intuitive behavior:\n",
    "   - When $\\theta > 0$ (pole falling right), the agent applies rightward force\n",
    "   - When $\\dot{\\theta} > 0$ (pole rotating right), similar corrective action\n",
    "   - The policy learns a smooth interpolation between these cases\n",
    "\n",
    "### 5.2 Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Can learn stochastic policies\n",
    "- Naturally handles continuous action spaces\n",
    "- Convergence guarantees to local optima\n",
    "\n",
    "**Limitations:**\n",
    "- High variance in gradient estimates\n",
    "- Sample inefficient (requires many trajectories)\n",
    "- Sensitive to hyperparameters\n",
    "\n",
    "### 5.3 Extensions\n",
    "\n",
    "Modern policy gradient methods address these limitations:\n",
    "- **Actor-Critic**: Use learned value function as baseline\n",
    "- **PPO/TRPO**: Constrain policy updates for stability\n",
    "- **A3C/A2C**: Parallel sampling for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Policy gradient methods provide a principled approach to directly optimizing policies in reinforcement learning. The REINFORCE algorithm, while simple, demonstrates the core ideas:\n",
    "\n",
    "1. **Direct policy optimization** via gradient ascent on expected return\n",
    "2. **Variance reduction** through baseline subtraction\n",
    "3. **Stochastic exploration** naturally encoded in the policy\n",
    "\n",
    "These foundations underpin modern deep RL algorithms like PPO, SAC, and TD3 that achieve state-of-the-art performance in complex control tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
