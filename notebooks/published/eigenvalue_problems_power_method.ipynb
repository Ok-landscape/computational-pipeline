{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue Problems: The Power Method\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Eigenvalue problems are fundamental in computational science, appearing in applications ranging from quantum mechanics to principal component analysis. Given a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, we seek scalar $\\lambda$ and non-zero vector $\\mathbf{v}$ satisfying:\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "## The Power Method\n",
    "\n",
    "The **Power Method** (also known as power iteration) is an iterative algorithm for finding the **dominant eigenvalue** $\\lambda_1$ (the eigenvalue with largest absolute value) and its corresponding eigenvector.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Starting with an initial guess $\\mathbf{v}^{(0)}$, the iteration proceeds as:\n",
    "\n",
    "$$\\mathbf{w}^{(k+1)} = \\mathbf{A}\\mathbf{v}^{(k)}$$\n",
    "\n",
    "$$\\mathbf{v}^{(k+1)} = \\frac{\\mathbf{w}^{(k+1)}}{\\|\\mathbf{w}^{(k+1)}\\|}$$\n",
    "\n",
    "The eigenvalue estimate at iteration $k$ is obtained via the **Rayleigh quotient**:\n",
    "\n",
    "$$\\lambda^{(k)} = \\frac{(\\mathbf{v}^{(k)})^T \\mathbf{A} \\mathbf{v}^{(k)}}{(\\mathbf{v}^{(k)})^T \\mathbf{v}^{(k)}}$$\n",
    "\n",
    "### Convergence Analysis\n",
    "\n",
    "Let $\\mathbf{A}$ have eigenvalues $|\\lambda_1| > |\\lambda_2| \\geq \\ldots \\geq |\\lambda_n|$ with corresponding eigenvectors $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_n$. If the initial vector has a non-zero component in the direction of $\\mathbf{u}_1$:\n",
    "\n",
    "$$\\mathbf{v}^{(0)} = c_1 \\mathbf{u}_1 + c_2 \\mathbf{u}_2 + \\ldots + c_n \\mathbf{u}_n, \\quad c_1 \\neq 0$$\n",
    "\n",
    "Then after $k$ iterations:\n",
    "\n",
    "$$\\mathbf{A}^k \\mathbf{v}^{(0)} = \\lambda_1^k \\left[ c_1 \\mathbf{u}_1 + c_2 \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k \\mathbf{u}_2 + \\ldots \\right]$$\n",
    "\n",
    "The convergence rate is determined by the ratio $|\\lambda_2/\\lambda_1|$. The error decreases as:\n",
    "\n",
    "$$\\|\\mathbf{v}^{(k)} - \\mathbf{u}_1\\| = O\\left(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|^k\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm, eig\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the Power Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(A, v0=None, max_iter=100, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Power method for finding the dominant eigenvalue and eigenvector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : ndarray\n",
    "        Square matrix (n x n)\n",
    "    v0 : ndarray, optional\n",
    "        Initial guess for eigenvector\n",
    "    max_iter : int\n",
    "        Maximum number of iterations\n",
    "    tol : float\n",
    "        Convergence tolerance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    eigenvalue : float\n",
    "        Dominant eigenvalue estimate\n",
    "    eigenvector : ndarray\n",
    "        Corresponding eigenvector\n",
    "    eigenvalue_history : list\n",
    "        Eigenvalue estimates at each iteration\n",
    "    error_history : list\n",
    "        Convergence error at each iteration\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # Initialize with random vector if not provided\n",
    "    if v0 is None:\n",
    "        v = np.random.rand(n)\n",
    "    else:\n",
    "        v = v0.copy()\n",
    "    \n",
    "    v = v / norm(v)\n",
    "    \n",
    "    eigenvalue_history = []\n",
    "    error_history = []\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        # Power iteration step\n",
    "        w = A @ v\n",
    "        \n",
    "        # Rayleigh quotient for eigenvalue estimate\n",
    "        eigenvalue = v @ w\n",
    "        eigenvalue_history.append(eigenvalue)\n",
    "        \n",
    "        # Normalize\n",
    "        v_new = w / norm(w)\n",
    "        \n",
    "        # Check convergence\n",
    "        error = norm(v_new - v)\n",
    "        error_history.append(error)\n",
    "        \n",
    "        v = v_new\n",
    "        \n",
    "        if error < tol:\n",
    "            break\n",
    "    \n",
    "    return eigenvalue, v, eigenvalue_history, error_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Symmetric Positive Definite Matrix\n",
    "\n",
    "We first test on a symmetric positive definite matrix with well-separated eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a symmetric matrix with known eigenvalues\n",
    "# We construct A = Q * D * Q^T where D is diagonal\n",
    "n = 5\n",
    "eigenvalues_true = np.array([10.0, 5.0, 2.0, 1.0, 0.5])\n",
    "D = np.diag(eigenvalues_true)\n",
    "\n",
    "# Random orthogonal matrix via QR decomposition\n",
    "Q, _ = np.linalg.qr(np.random.randn(n, n))\n",
    "A = Q @ D @ Q.T\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A.round(4))\n",
    "print(\"\\nTrue eigenvalues:\", eigenvalues_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply power method\n",
    "lambda_est, v_est, lambda_hist, error_hist = power_method(A, max_iter=50)\n",
    "\n",
    "print(f\"Estimated dominant eigenvalue: {lambda_est:.10f}\")\n",
    "print(f\"True dominant eigenvalue: {eigenvalues_true[0]:.10f}\")\n",
    "print(f\"Absolute error: {abs(lambda_est - eigenvalues_true[0]):.2e}\")\n",
    "print(f\"\\nConverged in {len(error_hist)} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Convergence Rate Analysis\n",
    "\n",
    "The theoretical convergence rate is $|\\lambda_2/\\lambda_1|^k$. Let's verify this numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify eigenvalue using numpy\n",
    "eigenvalues_np, eigenvectors_np = eig(A)\n",
    "idx = np.argsort(np.abs(eigenvalues_np))[::-1]\n",
    "eigenvalues_np = eigenvalues_np[idx]\n",
    "\n",
    "# Theoretical convergence ratio\n",
    "ratio = abs(eigenvalues_np[1] / eigenvalues_np[0])\n",
    "print(f\"Ratio |λ₂/λ₁| = {ratio:.4f}\")\n",
    "print(f\"Expected convergence rate: {ratio:.4f}^k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigenvalue errors\n",
    "eigenvalue_errors = np.abs(np.array(lambda_hist) - eigenvalues_true[0])\n",
    "\n",
    "# Theoretical error decay\n",
    "k_values = np.arange(len(lambda_hist))\n",
    "theoretical_decay = eigenvalue_errors[0] * (ratio ** k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Effect of Eigenvalue Separation\n",
    "\n",
    "We compare convergence for matrices with different eigenvalue separations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different eigenvalue ratios\n",
    "ratios_to_test = [0.1, 0.5, 0.9, 0.99]\n",
    "results = {}\n",
    "\n",
    "for r in ratios_to_test:\n",
    "    # Create matrix with specified ratio\n",
    "    eigenvalues_test = np.array([10.0, 10.0 * r, 1.0, 0.5, 0.1])\n",
    "    D_test = np.diag(eigenvalues_test)\n",
    "    A_test = Q @ D_test @ Q.T\n",
    "    \n",
    "    # Run power method\n",
    "    _, _, lambda_hist_test, _ = power_method(A_test, max_iter=100)\n",
    "    \n",
    "    # Store errors\n",
    "    errors = np.abs(np.array(lambda_hist_test) - eigenvalues_test[0])\n",
    "    results[r] = errors\n",
    "\n",
    "print(\"Convergence analysis complete for ratios:\", ratios_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Eigenvalue convergence\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(lambda_hist, 'b-o', markersize=4, label='Power Method')\n",
    "ax1.axhline(y=eigenvalues_true[0], color='r', linestyle='--', label=f'True λ₁ = {eigenvalues_true[0]}')\n",
    "ax1.set_xlabel('Iteration', fontsize=11)\n",
    "ax1.set_ylabel('Eigenvalue Estimate', fontsize=11)\n",
    "ax1.set_title('Convergence of Eigenvalue Estimate', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error decay (log scale)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.semilogy(eigenvalue_errors, 'b-o', markersize=4, label='Actual Error')\n",
    "ax2.semilogy(theoretical_decay, 'r--', linewidth=2, label=f'Theoretical: {ratio:.2f}^k')\n",
    "ax2.set_xlabel('Iteration', fontsize=11)\n",
    "ax2.set_ylabel('|λ_est - λ_true|', fontsize=11)\n",
    "ax2.set_title('Eigenvalue Error vs Theoretical Rate', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Effect of eigenvalue separation\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['green', 'blue', 'orange', 'red']\n",
    "for (r, errors), c in zip(results.items(), colors):\n",
    "    # Only plot non-zero errors\n",
    "    mask = errors > 1e-16\n",
    "    if np.any(mask):\n",
    "        ax3.semilogy(np.where(mask)[0], errors[mask], '-o', markersize=3, \n",
    "                    color=c, label=f'|λ₂/λ₁| = {r}')\n",
    "ax3.set_xlabel('Iteration', fontsize=11)\n",
    "ax3.set_ylabel('Eigenvalue Error', fontsize=11)\n",
    "ax3.set_title('Convergence for Different Eigenvalue Ratios', fontsize=12)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Eigenvector component convergence\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Run power method tracking eigenvector components\n",
    "v = np.random.rand(n)\n",
    "v = v / norm(v)\n",
    "v_history = [v.copy()]\n",
    "\n",
    "for _ in range(30):\n",
    "    w = A @ v\n",
    "    v = w / norm(w)\n",
    "    v_history.append(v.copy())\n",
    "\n",
    "v_history = np.array(v_history)\n",
    "\n",
    "for i in range(n):\n",
    "    ax4.plot(np.abs(v_history[:, i]), '-', linewidth=2, label=f'|v_{i+1}|')\n",
    "\n",
    "ax4.set_xlabel('Iteration', fontsize=11)\n",
    "ax4.set_ylabel('|Component|', fontsize=11)\n",
    "ax4.set_title('Eigenvector Component Convergence', fontsize=12)\n",
    "ax4.legend(loc='right')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Power Method provides a simple and robust approach for computing the dominant eigenvalue:\n",
    "\n",
    "1. **Simplicity**: Only requires matrix-vector multiplication\n",
    "2. **Convergence**: Linear convergence with rate $|\\lambda_2/\\lambda_1|$\n",
    "3. **Memory efficient**: Only stores vectors, not the full matrix decomposition\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Only finds the **dominant** eigenvalue\n",
    "- Convergence can be slow when $|\\lambda_2| \\approx |\\lambda_1|$\n",
    "- Fails if the dominant eigenvalue is complex or has multiplicity > 1\n",
    "\n",
    "### Extensions\n",
    "\n",
    "- **Inverse Power Method**: Find smallest eigenvalue by applying power method to $\\mathbf{A}^{-1}$\n",
    "- **Shifted Inverse Iteration**: Find eigenvalue closest to a shift $\\sigma$ using $(\\mathbf{A} - \\sigma \\mathbf{I})^{-1}$\n",
    "- **Rayleigh Quotient Iteration**: Use adaptive shifts for cubic convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
