{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM): Theory and Implementation\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Support Vector Machines (SVMs) are powerful supervised learning algorithms used for classification and regression tasks. Introduced by Vapnik and colleagues in the 1990s, SVMs have become one of the most robust and well-studied classification methods in machine learning.\n",
    "\n",
    "The fundamental idea behind SVMs is to find the optimal hyperplane that maximally separates data points belonging to different classes.\n",
    "\n",
    "## 2. Mathematical Foundation\n",
    "\n",
    "### 2.1 Linear SVM for Separable Data\n",
    "\n",
    "Consider a binary classification problem with training data $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n}$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, +1\\}$.\n",
    "\n",
    "A hyperplane in $\\mathbb{R}^d$ can be written as:\n",
    "\n",
    "$$\\mathbf{w}^T \\mathbf{x} + b = 0$$\n",
    "\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^d$ is the normal vector to the hyperplane and $b \\in \\mathbb{R}$ is the bias term.\n",
    "\n",
    "### 2.2 The Margin\n",
    "\n",
    "The **margin** is defined as the perpendicular distance between the hyperplane and the nearest data points from either class. For a correctly classified point, we require:\n",
    "\n",
    "$$y_i(\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1$$\n",
    "\n",
    "The geometric margin (distance from a point to the hyperplane) is:\n",
    "\n",
    "$$\\gamma_i = \\frac{y_i(\\mathbf{w}^T \\mathbf{x}_i + b)}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "The total margin width is $\\frac{2}{\\|\\mathbf{w}\\|}$.\n",
    "\n",
    "### 2.3 Optimization Problem\n",
    "\n",
    "To maximize the margin, we minimize $\\|\\mathbf{w}\\|^2$. The **primal optimization problem** is:\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$y_i(\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1, \\quad i = 1, \\ldots, n$$\n",
    "\n",
    "### 2.4 Lagrangian Dual Formulation\n",
    "\n",
    "Introducing Lagrange multipliers $\\alpha_i \\geq 0$, the Lagrangian is:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}) = \\frac{1}{2}\\|\\mathbf{w}\\|^2 - \\sum_{i=1}^{n} \\alpha_i [y_i(\\mathbf{w}^T \\mathbf{x}_i + b) - 1]$$\n",
    "\n",
    "Setting $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = 0$ and $\\frac{\\partial \\mathcal{L}}{\\partial b} = 0$ yields:\n",
    "\n",
    "$$\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i$$\n",
    "\n",
    "$$\\sum_{i=1}^{n} \\alpha_i y_i = 0$$\n",
    "\n",
    "The **dual problem** becomes:\n",
    "\n",
    "$$\\max_{\\boldsymbol{\\alpha}} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\\alpha_i \\geq 0, \\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0$$\n",
    "\n",
    "### 2.5 Support Vectors\n",
    "\n",
    "Points with $\\alpha_i > 0$ are called **support vectors**. These are the critical points that lie on the margin boundaries and fully determine the decision boundary.\n",
    "\n",
    "### 2.6 Soft Margin SVM\n",
    "\n",
    "For non-separable data, we introduce slack variables $\\xi_i \\geq 0$:\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$y_i(\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0$$\n",
    "\n",
    "The parameter $C > 0$ controls the trade-off between margin maximization and classification error.\n",
    "\n",
    "### 2.7 The Kernel Trick\n",
    "\n",
    "For non-linearly separable data, we map inputs to a higher-dimensional feature space via $\\phi: \\mathbb{R}^d \\rightarrow \\mathcal{H}$. The **kernel function** computes inner products in this space:\n",
    "\n",
    "$$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j)$$\n",
    "\n",
    "Common kernels include:\n",
    "\n",
    "- **Linear:** $K(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^T \\mathbf{z}$\n",
    "- **Polynomial:** $K(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^T \\mathbf{z} + c)^d$\n",
    "- **RBF (Gaussian):** $K(\\mathbf{x}, \\mathbf{z}) = \\exp\\left(-\\gamma \\|\\mathbf{x} - \\mathbf{z}\\|^2\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "We will implement a simple SVM from scratch using quadratic programming, and visualize the decision boundary on synthetic 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Generate Synthetic Data\n",
    "\n",
    "We create two linearly separable clusters in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=100, separation=2.0):\n",
    "    \"\"\"\n",
    "    Generate linearly separable 2D data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples per class\n",
    "    separation : float\n",
    "        Distance between class centers\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray of shape (2*n_samples, 2)\n",
    "        Feature matrix\n",
    "    y : ndarray of shape (2*n_samples,)\n",
    "        Labels in {-1, +1}\n",
    "    \"\"\"\n",
    "    # Class +1: centered at (separation/2, separation/2)\n",
    "    X_pos = np.random.randn(n_samples, 2) * 0.8 + np.array([separation/2, separation/2])\n",
    "    \n",
    "    # Class -1: centered at (-separation/2, -separation/2)\n",
    "    X_neg = np.random.randn(n_samples, 2) * 0.8 + np.array([-separation/2, -separation/2])\n",
    "    \n",
    "    X = np.vstack([X_pos, X_neg])\n",
    "    y = np.hstack([np.ones(n_samples), -np.ones(n_samples)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_data(n_samples=50, separation=3.0)\n",
    "\n",
    "print(f\"Dataset shape: X = {X.shape}, y = {y.shape}\")\n",
    "print(f\"Class distribution: +1 = {np.sum(y == 1)}, -1 = {np.sum(y == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 SVM Implementation\n",
    "\n",
    "We solve the dual optimization problem using scipy's minimize function with the SLSQP method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM:\n",
    "    \"\"\"\n",
    "    Linear Support Vector Machine classifier.\n",
    "    \n",
    "    Implements the hard-margin SVM by solving the dual optimization problem.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        C : float\n",
    "            Regularization parameter (for soft margin)\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.alphas = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the SVM model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Labels in {-1, +1}\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Compute the Gram matrix\n",
    "        K = np.dot(X, X.T)\n",
    "        \n",
    "        # Objective function: -sum(alpha) + 0.5 * alpha^T Q alpha\n",
    "        # where Q_ij = y_i * y_j * K_ij\n",
    "        Q = np.outer(y, y) * K\n",
    "        \n",
    "        def objective(alpha):\n",
    "            return 0.5 * np.dot(alpha, np.dot(Q, alpha)) - np.sum(alpha)\n",
    "        \n",
    "        def gradient(alpha):\n",
    "            return np.dot(Q, alpha) - np.ones(n_samples)\n",
    "        \n",
    "        # Constraints: sum(alpha_i * y_i) = 0\n",
    "        constraints = {'type': 'eq', 'fun': lambda a: np.dot(a, y), 'jac': lambda a: y}\n",
    "        \n",
    "        # Bounds: 0 <= alpha_i <= C\n",
    "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
    "        \n",
    "        # Initial guess\n",
    "        alpha0 = np.zeros(n_samples)\n",
    "        \n",
    "        # Solve the optimization problem\n",
    "        result = minimize(\n",
    "            objective, \n",
    "            alpha0, \n",
    "            method='SLSQP', \n",
    "            jac=gradient,\n",
    "            bounds=bounds, \n",
    "            constraints=constraints,\n",
    "            options={'ftol': 1e-10, 'maxiter': 1000}\n",
    "        )\n",
    "        \n",
    "        self.alphas = result.x\n",
    "        \n",
    "        # Support vectors have alpha > threshold\n",
    "        sv_threshold = 1e-5\n",
    "        sv_mask = self.alphas > sv_threshold\n",
    "        \n",
    "        self.support_vectors = X[sv_mask]\n",
    "        self.support_vector_labels = y[sv_mask]\n",
    "        self.support_alphas = self.alphas[sv_mask]\n",
    "        \n",
    "        # Compute weight vector: w = sum(alpha_i * y_i * x_i)\n",
    "        self.w = np.sum((self.alphas * y)[:, np.newaxis] * X, axis=0)\n",
    "        \n",
    "        # Compute bias: b = y_s - w^T x_s for any support vector\n",
    "        # Average over all support vectors for numerical stability\n",
    "        self.b = np.mean(\n",
    "            self.support_vector_labels - np.dot(self.support_vectors, self.w)\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute the signed distance to the hyperplane.\"\"\"\n",
    "        return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        return np.sign(self.decision_function(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the SVM\n",
    "svm = LinearSVM(C=100.0)  # Large C for near-hard margin\n",
    "svm.fit(X, y)\n",
    "\n",
    "print(f\"Weight vector w = {svm.w}\")\n",
    "print(f\"Bias term b = {svm.b:.4f}\")\n",
    "print(f\"Number of support vectors: {len(svm.support_vectors)}\")\n",
    "\n",
    "# Compute training accuracy\n",
    "predictions = svm.predict(X)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"Training accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualization\n",
    "\n",
    "We visualize the decision boundary, margin boundaries, and support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_decision_boundary(svm, X, y, title=\"SVM Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the SVM decision boundary with margins and support vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    svm : LinearSVM\n",
    "        Trained SVM model\n",
    "    X : ndarray\n",
    "        Feature matrix\n",
    "    y : ndarray\n",
    "        Labels\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Create mesh grid for decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Compute decision function on mesh\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision regions with soft colors\n",
    "    cmap_light = ListedColormap(['#FFCCCC', '#CCCCFF'])\n",
    "    ax.contourf(xx, yy, Z, levels=[-np.inf, 0, np.inf], \n",
    "                colors=['#FFCCCC', '#CCCCFF'], alpha=0.4)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(xx, yy, Z, levels=[-1, 0, 1], \n",
    "               colors=['#CC0000', 'black', '#0000CC'],\n",
    "               linestyles=['--', '-', '--'], linewidths=[1.5, 2, 1.5])\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter_pos = ax.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "                             c='blue', marker='o', s=60, \n",
    "                             edgecolors='navy', linewidths=1,\n",
    "                             label='Class +1', zorder=3)\n",
    "    scatter_neg = ax.scatter(X[y == -1, 0], X[y == -1, 1], \n",
    "                             c='red', marker='s', s=60, \n",
    "                             edgecolors='darkred', linewidths=1,\n",
    "                             label='Class -1', zorder=3)\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    ax.scatter(svm.support_vectors[:, 0], svm.support_vectors[:, 1],\n",
    "               s=200, facecolors='none', edgecolors='green', \n",
    "               linewidths=2.5, label='Support Vectors', zorder=4)\n",
    "    \n",
    "    # Add annotations\n",
    "    margin = 2 / np.linalg.norm(svm.w)\n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.set_title(f'{title}\\nMargin width = {margin:.3f}, '\n",
    "                 f'Support Vectors = {len(svm.support_vectors)}', fontsize=14)\n",
    "    ax.legend(loc='upper left', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Add text box with equation\n",
    "    textstr = f'Decision boundary:\\n$w_1 x_1 + w_2 x_2 + b = 0$\\n'\n",
    "    textstr += f'$w = [{svm.w[0]:.3f}, {svm.w[1]:.3f}]$\\n$b = {svm.b:.3f}$'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "    ax.text(0.98, 0.02, textstr, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='bottom', horizontalalignment='right', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# Create the visualization\n",
    "fig, ax = plot_svm_decision_boundary(svm, X, y, \n",
    "                                      title=\"Linear SVM Classification\")\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "print(\"Plot saved to 'plot.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis of Results\n",
    "\n",
    "### 4.1 Understanding the Decision Boundary\n",
    "\n",
    "The solid black line represents the optimal separating hyperplane:\n",
    "\n",
    "$$w_1 x_1 + w_2 x_2 + b = 0$$\n",
    "\n",
    "The dashed lines represent the margin boundaries where $\\mathbf{w}^T \\mathbf{x} + b = \\pm 1$.\n",
    "\n",
    "### 4.2 Support Vectors\n",
    "\n",
    "The circled points are support vectors - they are the only points that influence the position of the decision boundary. All other points could be moved or removed without changing the classifier, as long as they remain on the correct side of the margin.\n",
    "\n",
    "### 4.3 Margin Interpretation\n",
    "\n",
    "The margin width is $\\frac{2}{\\|\\mathbf{w}\\|}$. A larger margin generally indicates better generalization capability, according to statistical learning theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: examine support vectors\n",
    "print(\"Support Vector Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nNumber of support vectors: {len(svm.support_vectors)}\")\n",
    "print(f\"Percentage of training data: {100 * len(svm.support_vectors) / len(X):.1f}%\")\n",
    "print(f\"\\nMargin width: {2 / np.linalg.norm(svm.w):.4f}\")\n",
    "print(f\"||w||: {np.linalg.norm(svm.w):.4f}\")\n",
    "\n",
    "# Verify KKT conditions for support vectors\n",
    "print(\"\\nVerifying KKT conditions for support vectors:\")\n",
    "for i, (sv, label, alpha) in enumerate(zip(svm.support_vectors, \n",
    "                                            svm.support_vector_labels,\n",
    "                                            svm.support_alphas)):\n",
    "    margin_distance = label * (np.dot(svm.w, sv) + svm.b)\n",
    "    print(f\"  SV {i+1}: y*(wÂ·x + b) = {margin_distance:.6f}, alpha = {alpha:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook demonstrated the mathematical foundations and practical implementation of Support Vector Machines:\n",
    "\n",
    "1. **Theoretical Framework**: We derived the primal and dual optimization problems, showing how SVMs maximize the margin between classes.\n",
    "\n",
    "2. **Implementation**: A working linear SVM was implemented from scratch using quadratic programming via scipy's SLSQP optimizer.\n",
    "\n",
    "3. **Visualization**: The decision boundary, margins, and support vectors were visualized, demonstrating how only a small subset of training points (support vectors) determine the classifier.\n",
    "\n",
    "Key takeaways:\n",
    "- SVMs find the maximum-margin hyperplane, providing good generalization\n",
    "- The solution depends only on support vectors (sparse representation)\n",
    "- The kernel trick enables non-linear classification in the original feature space\n",
    "- The regularization parameter $C$ controls the bias-variance trade-off\n",
    "\n",
    "### References\n",
    "\n",
    "1. Vapnik, V. N. (1995). *The Nature of Statistical Learning Theory*. Springer.\n",
    "2. Cortes, C., & Vapnik, V. (1995). Support-vector networks. *Machine Learning*, 20(3), 273-297.\n",
    "3. Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
