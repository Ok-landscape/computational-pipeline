{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR Decomposition: Theory and Implementation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "QR decomposition (also called QR factorization) is a fundamental matrix factorization technique in numerical linear algebra. It decomposes a matrix $A$ into the product of two matrices:\n",
    "\n",
    "$$A = QR$$\n",
    "\n",
    "where:\n",
    "- $Q$ is an **orthogonal matrix** (i.e., $Q^T Q = I$)\n",
    "- $R$ is an **upper triangular matrix**\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Orthogonal Matrices\n",
    "\n",
    "A matrix $Q \\in \\mathbb{R}^{m \\times n}$ is orthogonal if its columns form an orthonormal set:\n",
    "\n",
    "$$\\mathbf{q}_i^T \\mathbf{q}_j = \\delta_{ij} = \\begin{cases} 1 & \\text{if } i = j \\\\ 0 & \\text{if } i \\neq j \\end{cases}$$\n",
    "\n",
    "This implies $Q^T Q = I$, and for square matrices, $Q^{-1} = Q^T$.\n",
    "\n",
    "### The Gram-Schmidt Process\n",
    "\n",
    "The classical Gram-Schmidt process constructs an orthonormal basis from linearly independent vectors. Given vectors $\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\}$, we compute:\n",
    "\n",
    "$$\\mathbf{u}_k = \\mathbf{a}_k - \\sum_{j=1}^{k-1} \\text{proj}_{\\mathbf{q}_j}(\\mathbf{a}_k)$$\n",
    "\n",
    "$$\\mathbf{q}_k = \\frac{\\mathbf{u}_k}{\\|\\mathbf{u}_k\\|}$$\n",
    "\n",
    "where the projection is:\n",
    "\n",
    "$$\\text{proj}_{\\mathbf{q}_j}(\\mathbf{a}_k) = \\frac{\\mathbf{q}_j^T \\mathbf{a}_k}{\\mathbf{q}_j^T \\mathbf{q}_j} \\mathbf{q}_j = (\\mathbf{q}_j^T \\mathbf{a}_k) \\mathbf{q}_j$$\n",
    "\n",
    "### Construction of R\n",
    "\n",
    "The upper triangular matrix $R$ contains the projection coefficients:\n",
    "\n",
    "$$r_{ij} = \\begin{cases} \\mathbf{q}_i^T \\mathbf{a}_j & \\text{if } i \\leq j \\\\ 0 & \\text{if } i > j \\end{cases}$$\n",
    "\n",
    "## Applications\n",
    "\n",
    "QR decomposition is essential for:\n",
    "- **Solving least squares problems**: $\\min_x \\|Ax - b\\|_2$\n",
    "- **Computing eigenvalues**: QR algorithm iteratively applies QR decomposition\n",
    "- **Solving linear systems**: More numerically stable than LU for ill-conditioned matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Classical Gram-Schmidt\n",
    "\n",
    "We implement the classical Gram-Schmidt algorithm to compute the QR decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_gram_schmidt(A):\n",
    "    \"\"\"\n",
    "    Compute QR decomposition using Classical Gram-Schmidt.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A : ndarray of shape (m, n)\n",
    "        Input matrix with m >= n\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Q : ndarray of shape (m, n)\n",
    "        Orthogonal matrix\n",
    "    R : ndarray of shape (n, n)\n",
    "        Upper triangular matrix\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    Q = np.zeros((m, n))\n",
    "    R = np.zeros((n, n))\n",
    "    \n",
    "    for j in range(n):\n",
    "        # Start with the j-th column of A\n",
    "        v = A[:, j].copy()\n",
    "        \n",
    "        # Subtract projections onto previous q vectors\n",
    "        for i in range(j):\n",
    "            R[i, j] = np.dot(Q[:, i], A[:, j])\n",
    "            v = v - R[i, j] * Q[:, i]\n",
    "        \n",
    "        # Normalize\n",
    "        R[j, j] = np.linalg.norm(v)\n",
    "        if R[j, j] > 1e-10:\n",
    "            Q[:, j] = v / R[j, j]\n",
    "        else:\n",
    "            Q[:, j] = v\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "# Example: 4x3 matrix\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 7],\n",
    "    [10, 11, 12]\n",
    "], dtype=float)\n",
    "\n",
    "print(\"Original Matrix A:\")\n",
    "print(A)\n",
    "print(f\"\\nShape: {A.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute QR decomposition\n",
    "Q, R = classical_gram_schmidt(A)\n",
    "\n",
    "print(\"Orthogonal Matrix Q:\")\n",
    "print(Q)\n",
    "print(f\"\\nUpper Triangular Matrix R:\")\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of Orthogonality\n",
    "\n",
    "We verify that $Q^T Q = I$ and $A = QR$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check orthogonality: Q^T Q should be identity\n",
    "QtQ = Q.T @ Q\n",
    "print(\"Q^T Q (should be identity):\")\n",
    "print(QtQ)\n",
    "\n",
    "print(f\"\\nOrthogonality error: {np.linalg.norm(QtQ - np.eye(3)):.2e}\")\n",
    "\n",
    "# Check reconstruction: A = QR\n",
    "A_reconstructed = Q @ R\n",
    "print(f\"\\nReconstruction error ||A - QR||: {np.linalg.norm(A - A_reconstructed):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with NumPy's Implementation\n",
    "\n",
    "NumPy uses Householder reflections, which is more numerically stable than Gram-Schmidt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy's QR (uses Householder reflections)\n",
    "Q_np, R_np = np.linalg.qr(A)\n",
    "\n",
    "print(\"NumPy Q:\")\n",
    "print(Q_np)\n",
    "print(\"\\nNumPy R:\")\n",
    "print(R_np)\n",
    "\n",
    "# Note: Signs may differ (both are valid QR decompositions)\n",
    "print(f\"\\nNumPy reconstruction error: {np.linalg.norm(A - Q_np @ R_np):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Solving Least Squares\n",
    "\n",
    "The least squares problem $\\min_x \\|Ax - b\\|_2$ can be solved efficiently using QR decomposition:\n",
    "\n",
    "$$Ax = b \\implies QRx = b \\implies Rx = Q^T b$$\n",
    "\n",
    "Since $R$ is upper triangular, we can solve by back substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_least_squares_qr(A, b):\n",
    "    \"\"\"\n",
    "    Solve least squares problem using QR decomposition.\n",
    "    \"\"\"\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    # Solve Rx = Q^T b by back substitution\n",
    "    y = Q.T @ b\n",
    "    x = np.linalg.solve(R, y)\n",
    "    return x\n",
    "\n",
    "# Generate overdetermined system\n",
    "m, n = 100, 3\n",
    "A_ls = np.column_stack([np.ones(m), np.linspace(0, 10, m), np.linspace(0, 10, m)**2])\n",
    "x_true = np.array([1, 2, -0.5])\n",
    "b_ls = A_ls @ x_true + 0.5 * np.random.randn(m)\n",
    "\n",
    "# Solve using QR\n",
    "x_qr = solve_least_squares_qr(A_ls, b_ls)\n",
    "print(f\"True coefficients: {x_true}\")\n",
    "print(f\"Estimated coefficients: {x_qr}\")\n",
    "print(f\"Error: {np.linalg.norm(x_true - x_qr):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We visualize the QR decomposition in 3D, showing how the original column vectors are transformed into orthonormal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 3x3 matrix for visualization\n",
    "A_3d = np.array([\n",
    "    [2, 1, 1],\n",
    "    [1, 3, 1],\n",
    "    [1, 1, 4]\n",
    "], dtype=float)\n",
    "\n",
    "Q_3d, R_3d = np.linalg.qr(A_3d)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Matrix structure visualization\n",
    "ax1 = fig.add_subplot(131)\n",
    "matrices = [A_3d, Q_3d, R_3d]\n",
    "titles = ['A (Original)', 'Q (Orthogonal)', 'R (Upper Triangular)']\n",
    "\n",
    "# Combined heatmap\n",
    "combined = np.zeros((3, 9))\n",
    "combined[:, 0:3] = A_3d\n",
    "combined[:, 3:6] = Q_3d\n",
    "combined[:, 6:9] = R_3d\n",
    "\n",
    "im = ax1.imshow(combined, cmap='RdBu', aspect='equal', vmin=-4, vmax=4)\n",
    "ax1.set_xticks([1, 4, 7])\n",
    "ax1.set_xticklabels(['A', 'Q', 'R'])\n",
    "ax1.set_yticks([])\n",
    "ax1.axvline(x=2.5, color='black', linewidth=2)\n",
    "ax1.axvline(x=5.5, color='black', linewidth=2)\n",
    "plt.colorbar(im, ax=ax1, fraction=0.046)\n",
    "ax1.set_title('QR Decomposition: A = QR')\n",
    "\n",
    "# Plot 2: Orthogonality verification (Q^T Q)\n",
    "ax2 = fig.add_subplot(132)\n",
    "QtQ_3d = Q_3d.T @ Q_3d\n",
    "im2 = ax2.imshow(QtQ_3d, cmap='RdBu', vmin=-0.5, vmax=1.5)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax2.text(j, i, f'{QtQ_3d[i, j]:.2f}', ha='center', va='center', fontsize=10)\n",
    "ax2.set_title(r'$Q^T Q$ (Identity Matrix)')\n",
    "ax2.set_xticks(range(3))\n",
    "ax2.set_yticks(range(3))\n",
    "plt.colorbar(im2, ax=ax2, fraction=0.046)\n",
    "\n",
    "# Plot 3: Least squares fit\n",
    "ax3 = fig.add_subplot(133)\n",
    "t = np.linspace(0, 10, m)\n",
    "ax3.scatter(t, b_ls, alpha=0.5, s=20, label='Noisy data')\n",
    "y_fit = A_ls @ x_qr\n",
    "ax3.plot(t, y_fit, 'r-', linewidth=2, label='QR least squares fit')\n",
    "y_true = A_ls @ x_true\n",
    "ax3.plot(t, y_true, 'g--', linewidth=1.5, label='True curve')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_title('Least Squares via QR Decomposition')\n",
    "ax3.legend(fontsize=8)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Stability Analysis\n",
    "\n",
    "The classical Gram-Schmidt algorithm suffers from numerical instability. Let's compare it with the Modified Gram-Schmidt algorithm on an ill-conditioned matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_gram_schmidt(A):\n",
    "    \"\"\"\n",
    "    Compute QR decomposition using Modified Gram-Schmidt.\n",
    "    More numerically stable than classical Gram-Schmidt.\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    Q = A.copy().astype(float)\n",
    "    R = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        R[i, i] = np.linalg.norm(Q[:, i])\n",
    "        if R[i, i] > 1e-10:\n",
    "            Q[:, i] = Q[:, i] / R[i, i]\n",
    "        \n",
    "        for j in range(i + 1, n):\n",
    "            R[i, j] = np.dot(Q[:, i], Q[:, j])\n",
    "            Q[:, j] = Q[:, j] - R[i, j] * Q[:, i]\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "# Create ill-conditioned matrix (Hilbert matrix)\n",
    "def hilbert_matrix(n):\n",
    "    return np.array([[1/(i + j + 1) for j in range(n)] for i in range(n)])\n",
    "\n",
    "n = 8\n",
    "H = hilbert_matrix(n)\n",
    "print(f\"Condition number of {n}x{n} Hilbert matrix: {np.linalg.cond(H):.2e}\")\n",
    "\n",
    "# Compare orthogonality errors\n",
    "Q_cgs, R_cgs = classical_gram_schmidt(H)\n",
    "Q_mgs, R_mgs = modified_gram_schmidt(H)\n",
    "Q_np, R_np = np.linalg.qr(H)\n",
    "\n",
    "err_cgs = np.linalg.norm(Q_cgs.T @ Q_cgs - np.eye(n))\n",
    "err_mgs = np.linalg.norm(Q_mgs.T @ Q_mgs - np.eye(n))\n",
    "err_np = np.linalg.norm(Q_np.T @ Q_np - np.eye(n))\n",
    "\n",
    "print(f\"\\nOrthogonality error ||Q^T Q - I||:\")\n",
    "print(f\"  Classical Gram-Schmidt: {err_cgs:.2e}\")\n",
    "print(f\"  Modified Gram-Schmidt:  {err_mgs:.2e}\")\n",
    "print(f\"  NumPy (Householder):    {err_np:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "QR decomposition is a powerful tool in numerical linear algebra with key properties:\n",
    "\n",
    "1. **Uniqueness**: For a matrix $A$ with full column rank, the QR decomposition with positive diagonal entries in $R$ is unique\n",
    "\n",
    "2. **Numerical Stability**: \n",
    "   - Modified Gram-Schmidt is more stable than classical Gram-Schmidt\n",
    "   - Householder reflections (used by NumPy) are the most stable\n",
    "\n",
    "3. **Computational Cost**: $\\mathcal{O}(mn^2)$ for an $m \\times n$ matrix\n",
    "\n",
    "4. **Applications**: Solving least squares, computing eigenvalues (QR algorithm), and orthogonalization of vectors\n",
    "\n",
    "The choice of algorithm depends on the trade-off between computational efficiency and numerical stability required for the specific application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
