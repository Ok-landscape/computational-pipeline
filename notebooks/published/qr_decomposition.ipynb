{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR Decomposition\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "QR decomposition (also called QR factorization) is a fundamental matrix factorization technique in numerical linear algebra. Given an $m \\times n$ matrix $A$ with $m \\geq n$, the QR decomposition expresses $A$ as the product of two matrices:\n",
    "\n",
    "$$A = QR$$\n",
    "\n",
    "where:\n",
    "- $Q$ is an $m \\times m$ orthogonal matrix (i.e., $Q^T Q = QQ^T = I$)\n",
    "- $R$ is an $m \\times n$ upper triangular matrix\n",
    "\n",
    "### Properties of Orthogonal Matrices\n",
    "\n",
    "An orthogonal matrix $Q$ satisfies:\n",
    "- $Q^{-1} = Q^T$\n",
    "- $\\|Qx\\|_2 = \\|x\\|_2$ for any vector $x$ (norm preservation)\n",
    "- $\\det(Q) = \\pm 1$\n",
    "\n",
    "### The Gram-Schmidt Process\n",
    "\n",
    "One classical method to compute QR decomposition is the **Gram-Schmidt orthogonalization** process. Given the columns of $A$ as $\\{a_1, a_2, \\ldots, a_n\\}$, we construct orthonormal vectors $\\{q_1, q_2, \\ldots, q_n\\}$ as follows:\n",
    "\n",
    "For $k = 1, 2, \\ldots, n$:\n",
    "\n",
    "$$u_k = a_k - \\sum_{j=1}^{k-1} \\text{proj}_{q_j}(a_k)$$\n",
    "\n",
    "where the projection is:\n",
    "\n",
    "$$\\text{proj}_{q_j}(a_k) = \\frac{\\langle a_k, q_j \\rangle}{\\langle q_j, q_j \\rangle} q_j = \\langle a_k, q_j \\rangle q_j$$\n",
    "\n",
    "Then normalize:\n",
    "\n",
    "$$q_k = \\frac{u_k}{\\|u_k\\|_2}$$\n",
    "\n",
    "The elements of $R$ are computed as:\n",
    "\n",
    "$$r_{jk} = \\langle a_k, q_j \\rangle \\quad \\text{for } j \\leq k$$\n",
    "\n",
    "### Applications\n",
    "\n",
    "QR decomposition is used extensively in:\n",
    "1. **Solving least squares problems**: For overdetermined systems $Ax = b$\n",
    "2. **Eigenvalue algorithms**: The QR algorithm for computing eigenvalues\n",
    "3. **Computing orthonormal bases**: For subspaces spanned by column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Classical Gram-Schmidt\n",
    "\n",
    "We first implement the classical Gram-Schmidt algorithm to understand the underlying mechanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_gram_schmidt(A):\n",
    "    \"\"\"\n",
    "    Compute QR decomposition using Classical Gram-Schmidt.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A : ndarray\n",
    "        Input matrix of shape (m, n) with m >= n\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Q : ndarray\n",
    "        Orthogonal matrix of shape (m, n)\n",
    "    R : ndarray\n",
    "        Upper triangular matrix of shape (n, n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    Q = np.zeros((m, n))\n",
    "    R = np.zeros((n, n))\n",
    "    \n",
    "    for k in range(n):\n",
    "        # Start with the k-th column of A\n",
    "        u = A[:, k].copy()\n",
    "        \n",
    "        # Subtract projections onto previous q vectors\n",
    "        for j in range(k):\n",
    "            R[j, k] = np.dot(Q[:, j], A[:, k])\n",
    "            u = u - R[j, k] * Q[:, j]\n",
    "        \n",
    "        # Normalize\n",
    "        R[k, k] = np.linalg.norm(u)\n",
    "        if R[k, k] > 1e-10:\n",
    "            Q[:, k] = u / R[k, k]\n",
    "        else:\n",
    "            Q[:, k] = u\n",
    "    \n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Modified Gram-Schmidt\n",
    "\n",
    "The Modified Gram-Schmidt (MGS) algorithm improves numerical stability by updating the vector $u_k$ immediately after each projection is subtracted, rather than computing all projections from the original $a_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_gram_schmidt(A):\n",
    "    \"\"\"\n",
    "    Compute QR decomposition using Modified Gram-Schmidt.\n",
    "    \n",
    "    This version has better numerical stability than Classical GS.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A : ndarray\n",
    "        Input matrix of shape (m, n) with m >= n\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Q : ndarray\n",
    "        Orthogonal matrix of shape (m, n)\n",
    "    R : ndarray\n",
    "        Upper triangular matrix of shape (n, n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    Q = A.astype(float).copy()\n",
    "    R = np.zeros((n, n))\n",
    "    \n",
    "    for k in range(n):\n",
    "        # Compute the norm of the current column\n",
    "        R[k, k] = np.linalg.norm(Q[:, k])\n",
    "        \n",
    "        if R[k, k] > 1e-10:\n",
    "            Q[:, k] = Q[:, k] / R[k, k]\n",
    "        \n",
    "        # Orthogonalize remaining columns against q_k\n",
    "        for j in range(k + 1, n):\n",
    "            R[k, j] = np.dot(Q[:, k], Q[:, j])\n",
    "            Q[:, j] = Q[:, j] - R[k, j] * Q[:, k]\n",
    "    \n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration: Basic QR Decomposition\n",
    "\n",
    "Let's create a test matrix and compute its QR decomposition using our implementations and compare with NumPy's built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test matrix\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 7],\n",
    "    [10, 11, 12]\n",
    "], dtype=float)\n",
    "\n",
    "print(\"Original Matrix A:\")\n",
    "print(A)\n",
    "print(f\"\\nShape: {A.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute QR using different methods\n",
    "Q_cgs, R_cgs = classical_gram_schmidt(A)\n",
    "Q_mgs, R_mgs = modified_gram_schmidt(A)\n",
    "Q_np, R_np = np.linalg.qr(A)\n",
    "\n",
    "print(\"Classical Gram-Schmidt Q:\")\n",
    "print(Q_cgs)\n",
    "print(\"\\nClassical Gram-Schmidt R:\")\n",
    "print(R_cgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the decomposition: A = QR\n",
    "print(\"Verification: ||A - QR|| (should be ~0)\")\n",
    "print(f\"Classical GS: {np.linalg.norm(A - Q_cgs @ R_cgs):.2e}\")\n",
    "print(f\"Modified GS:  {np.linalg.norm(A - Q_mgs @ R_mgs):.2e}\")\n",
    "print(f\"NumPy QR:     {np.linalg.norm(A - Q_np @ R_np):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify orthogonality: Q^T Q should be identity\n",
    "print(\"\\nOrthogonality check: ||Q^T Q - I|| (should be ~0)\")\n",
    "n = A.shape[1]\n",
    "print(f\"Classical GS: {np.linalg.norm(Q_cgs.T @ Q_cgs - np.eye(n)):.2e}\")\n",
    "print(f\"Modified GS:  {np.linalg.norm(Q_mgs.T @ Q_mgs - np.eye(n)):.2e}\")\n",
    "print(f\"NumPy QR:     {np.linalg.norm(Q_np.T @ Q_np - np.eye(n)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Stability Analysis\n",
    "\n",
    "A key difference between Classical and Modified Gram-Schmidt appears when dealing with ill-conditioned matrices. Let's investigate this with a Hilbert matrix, which is notoriously ill-conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hilbert_matrix(n):\n",
    "    \"\"\"Generate an n x n Hilbert matrix.\"\"\"\n",
    "    H = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            H[i, j] = 1.0 / (i + j + 1)\n",
    "    return H\n",
    "\n",
    "# Test with different sizes of Hilbert matrices\n",
    "sizes = [5, 8, 10, 12]\n",
    "cgs_errors = []\n",
    "mgs_errors = []\n",
    "np_errors = []\n",
    "condition_numbers = []\n",
    "\n",
    "for n in sizes:\n",
    "    H = hilbert_matrix(n)\n",
    "    cond = np.linalg.cond(H)\n",
    "    condition_numbers.append(cond)\n",
    "    \n",
    "    # Compute QR decompositions\n",
    "    Q_cgs, R_cgs = classical_gram_schmidt(H)\n",
    "    Q_mgs, R_mgs = modified_gram_schmidt(H)\n",
    "    Q_np, R_np = np.linalg.qr(H)\n",
    "    \n",
    "    # Measure loss of orthogonality\n",
    "    cgs_errors.append(np.linalg.norm(Q_cgs.T @ Q_cgs - np.eye(n)))\n",
    "    mgs_errors.append(np.linalg.norm(Q_mgs.T @ Q_mgs - np.eye(n)))\n",
    "    np_errors.append(np.linalg.norm(Q_np.T @ Q_np - np.eye(n)))\n",
    "    \n",
    "    print(f\"n={n}: cond(H)={cond:.2e}, CGS={cgs_errors[-1]:.2e}, MGS={mgs_errors[-1]:.2e}, NumPy={np_errors[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Solving Least Squares Problems\n",
    "\n",
    "QR decomposition provides a numerically stable method for solving least squares problems. Given an overdetermined system $Ax = b$, we seek $x$ that minimizes $\\|Ax - b\\|_2$.\n",
    "\n",
    "Using $A = QR$, the solution is:\n",
    "\n",
    "$$x = R^{-1} Q^T b$$\n",
    "\n",
    "Since $R$ is upper triangular, this can be solved efficiently via back-substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for least squares fitting\n",
    "m = 100  # Number of data points\n",
    "n = 4    # Polynomial degree + 1\n",
    "\n",
    "# True coefficients\n",
    "true_coeffs = np.array([2.0, -1.5, 0.5, 0.1])\n",
    "\n",
    "# Generate noisy data\n",
    "x_data = np.linspace(0, 3, m)\n",
    "noise = 0.5 * np.random.randn(m)\n",
    "\n",
    "# y = c0 + c1*x + c2*x^2 + c3*x^3 + noise\n",
    "y_data = np.polyval(true_coeffs[::-1], x_data) + noise\n",
    "\n",
    "# Build Vandermonde matrix\n",
    "A_ls = np.vander(x_data, n, increasing=True)\n",
    "\n",
    "print(f\"Design matrix A shape: {A_ls.shape}\")\n",
    "print(f\"True coefficients: {true_coeffs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_least_squares_qr(A, b):\n",
    "    \"\"\"\n",
    "    Solve least squares problem Ax = b using QR decomposition.\n",
    "    \"\"\"\n",
    "    Q, R = modified_gram_schmidt(A)\n",
    "    \n",
    "    # Compute Q^T b\n",
    "    Qtb = Q.T @ b\n",
    "    \n",
    "    # Back-substitution to solve Rx = Q^T b\n",
    "    n = R.shape[1]\n",
    "    x = np.zeros(n)\n",
    "    \n",
    "    for i in range(n - 1, -1, -1):\n",
    "        x[i] = (Qtb[i] - np.dot(R[i, i+1:], x[i+1:])) / R[i, i]\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Solve using our QR method\n",
    "coeffs_qr = solve_least_squares_qr(A_ls, y_data)\n",
    "\n",
    "# Compare with NumPy's lstsq\n",
    "coeffs_np, _, _, _ = np.linalg.lstsq(A_ls, y_data, rcond=None)\n",
    "\n",
    "print(f\"True coefficients:   {true_coeffs}\")\n",
    "print(f\"QR solution:         {coeffs_qr}\")\n",
    "print(f\"NumPy lstsq:         {coeffs_np}\")\n",
    "print(f\"\\nDifference QR vs NumPy: {np.linalg.norm(coeffs_qr - coeffs_np):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's create a comprehensive visualization showing:\n",
    "1. The numerical stability comparison between CGS and MGS\n",
    "2. The least squares polynomial fit\n",
    "3. The structure of Q and R matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Numerical stability comparison\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax1.semilogy(sizes, cgs_errors, 'ro-', label='Classical GS', linewidth=2, markersize=8)\n",
    "ax1.semilogy(sizes, mgs_errors, 'bs-', label='Modified GS', linewidth=2, markersize=8)\n",
    "ax1.semilogy(sizes, np_errors, 'g^-', label='NumPy (Householder)', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Matrix Size n', fontsize=12)\n",
    "ax1.set_ylabel(r'$\\|Q^T Q - I\\|_2$', fontsize=12)\n",
    "ax1.set_title('Loss of Orthogonality (Hilbert Matrix)', fontsize=14)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Least squares fit\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax2.scatter(x_data, y_data, alpha=0.5, s=20, label='Noisy data')\n",
    "x_smooth = np.linspace(0, 3, 200)\n",
    "y_fit = np.polyval(coeffs_qr[::-1], x_smooth)\n",
    "y_true = np.polyval(true_coeffs[::-1], x_smooth)\n",
    "ax2.plot(x_smooth, y_fit, 'r-', linewidth=2, label='QR fit')\n",
    "ax2.plot(x_smooth, y_true, 'g--', linewidth=2, label='True curve')\n",
    "ax2.set_xlabel('x', fontsize=12)\n",
    "ax2.set_ylabel('y', fontsize=12)\n",
    "ax2.set_title('Least Squares Polynomial Fit via QR', fontsize=14)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Structure of Q matrix\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "A_demo = np.random.randn(8, 5)\n",
    "Q_demo, R_demo = modified_gram_schmidt(A_demo)\n",
    "im3 = ax3.imshow(Q_demo, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "ax3.set_xlabel('Column Index', fontsize=12)\n",
    "ax3.set_ylabel('Row Index', fontsize=12)\n",
    "ax3.set_title('Q Matrix (Orthonormal Columns)', fontsize=14)\n",
    "plt.colorbar(im3, ax=ax3)\n",
    "\n",
    "# Plot 4: Structure of R matrix\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "im4 = ax4.imshow(R_demo, cmap='RdBu', aspect='auto')\n",
    "ax4.set_xlabel('Column Index', fontsize=12)\n",
    "ax4.set_ylabel('Row Index', fontsize=12)\n",
    "ax4.set_title('R Matrix (Upper Triangular)', fontsize=14)\n",
    "plt.colorbar(im4, ax=ax4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored QR decomposition:\n",
    "\n",
    "1. **Theory**: QR factorization expresses a matrix as $A = QR$ where $Q$ is orthogonal and $R$ is upper triangular.\n",
    "\n",
    "2. **Algorithms**: We implemented both Classical and Modified Gram-Schmidt, demonstrating that MGS has superior numerical stability.\n",
    "\n",
    "3. **Numerical Stability**: Using Hilbert matrices, we showed that CGS suffers from loss of orthogonality for ill-conditioned matrices, while MGS maintains better stability.\n",
    "\n",
    "4. **Applications**: We demonstrated solving least squares problems using QR decomposition, achieving results comparable to optimized library functions.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Modified Gram-Schmidt is preferred over Classical Gram-Schmidt for numerical stability\n",
    "- For production code, use library implementations (e.g., `numpy.linalg.qr`) which typically use Householder reflections\n",
    "- QR decomposition is the preferred method for solving least squares problems due to its numerical stability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
