{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "A **Variational Autoencoder (VAE)** is a generative model that combines deep learning with Bayesian inference. Unlike traditional autoencoders that learn deterministic encodings, VAEs learn a probabilistic latent representation, enabling them to generate new samples from the learned distribution.\n",
    "\n",
    "## 2. Theoretical Foundation\n",
    "\n",
    "### 2.1 Generative Model Perspective\n",
    "\n",
    "VAEs model the joint distribution of observed data $\\mathbf{x}$ and latent variables $\\mathbf{z}$:\n",
    "\n",
    "$$p(\\mathbf{x}, \\mathbf{z}) = p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})$$\n",
    "\n",
    "where:\n",
    "- $p(\\mathbf{z})$ is the prior distribution over latent variables (typically $\\mathcal{N}(0, I)$)\n",
    "- $p(\\mathbf{x}|\\mathbf{z})$ is the likelihood (decoder network)\n",
    "\n",
    "### 2.2 The Intractability Problem\n",
    "\n",
    "The posterior distribution $p(\\mathbf{z}|\\mathbf{x})$ is intractable because computing the marginal likelihood requires integration:\n",
    "\n",
    "$$p(\\mathbf{x}) = \\int p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})d\\mathbf{z}$$\n",
    "\n",
    "### 2.3 Variational Inference\n",
    "\n",
    "We approximate the intractable posterior with a variational distribution $q_\\phi(\\mathbf{z}|\\mathbf{x})$ (encoder network). The goal is to minimize the Kullback-Leibler divergence:\n",
    "\n",
    "$$D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}|\\mathbf{x}))$$\n",
    "\n",
    "### 2.4 Evidence Lower Bound (ELBO)\n",
    "\n",
    "Minimizing the KL divergence is equivalent to maximizing the Evidence Lower Bound:\n",
    "\n",
    "$$\\mathcal{L}(\\theta, \\phi; \\mathbf{x}) = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))$$\n",
    "\n",
    "This decomposes into:\n",
    "- **Reconstruction term**: $\\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})]$ - measures how well the decoder reconstructs data\n",
    "- **Regularization term**: $-D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))$ - ensures the latent distribution stays close to the prior\n",
    "\n",
    "### 2.5 Reparameterization Trick\n",
    "\n",
    "To enable backpropagation through the sampling operation, we use the reparameterization trick. For a Gaussian encoder:\n",
    "\n",
    "$$\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "where $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$ are outputs of the encoder network.\n",
    "\n",
    "### 2.6 KL Divergence for Gaussians\n",
    "\n",
    "For Gaussian distributions, the KL divergence has a closed form:\n",
    "\n",
    "$$D_{KL}(\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\| \\mathcal{N}(0, I)) = -\\frac{1}{2}\\sum_{j=1}^{J}(1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2)$$\n",
    "\n",
    "where $J$ is the dimension of the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "We will implement a VAE from scratch using NumPy to demonstrate the core concepts. Our VAE will:\n",
    "1. Learn to encode/decode 2D data points\n",
    "2. Use a 2D latent space for visualization\n",
    "3. Generate new samples from the learned distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic 2D data: mixture of Gaussians\n",
    "def generate_data(n_samples=1000):\n",
    "    \"\"\"Generate 2D data from a mixture of Gaussians.\"\"\"\n",
    "    n_per_cluster = n_samples // 4\n",
    "    \n",
    "    # Four clusters\n",
    "    centers = [(-2, -2), (-2, 2), (2, -2), (2, 2)]\n",
    "    data = []\n",
    "    \n",
    "    for center in centers:\n",
    "        cluster = np.random.randn(n_per_cluster, 2) * 0.5 + np.array(center)\n",
    "        data.append(cluster)\n",
    "    \n",
    "    return np.vstack(data)\n",
    "\n",
    "# Generate training data\n",
    "X_train = generate_data(2000)\n",
    "print(f\"Training data shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    \"\"\"\n",
    "    Variational Autoencoder with single hidden layer.\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: input_dim -> hidden_dim -> (mu, log_var) of latent_dim\n",
    "    - Decoder: latent_dim -> hidden_dim -> output_dim\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=16, latent_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Initialize weights with Xavier initialization\n",
    "        scale = 0.1\n",
    "        \n",
    "        # Encoder weights\n",
    "        self.W_enc1 = np.random.randn(input_dim, hidden_dim) * scale\n",
    "        self.b_enc1 = np.zeros(hidden_dim)\n",
    "        self.W_mu = np.random.randn(hidden_dim, latent_dim) * scale\n",
    "        self.b_mu = np.zeros(latent_dim)\n",
    "        self.W_logvar = np.random.randn(hidden_dim, latent_dim) * scale\n",
    "        self.b_logvar = np.zeros(latent_dim)\n",
    "        \n",
    "        # Decoder weights\n",
    "        self.W_dec1 = np.random.randn(latent_dim, hidden_dim) * scale\n",
    "        self.b_dec1 = np.zeros(hidden_dim)\n",
    "        self.W_dec2 = np.random.randn(hidden_dim, input_dim) * scale\n",
    "        self.b_dec2 = np.zeros(input_dim)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent distribution parameters.\"\"\"\n",
    "        self.h_enc = self.relu(x @ self.W_enc1 + self.b_enc1)\n",
    "        mu = self.h_enc @ self.W_mu + self.b_mu\n",
    "        log_var = self.h_enc @ self.W_logvar + self.b_logvar\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"Sample z using reparameterization trick.\"\"\"\n",
    "        std = np.exp(0.5 * log_var)\n",
    "        eps = np.random.randn(*mu.shape)\n",
    "        return mu + eps * std, eps\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent vector to reconstruction.\"\"\"\n",
    "        self.h_dec = self.relu(z @ self.W_dec1 + self.b_dec1)\n",
    "        x_recon = self.h_dec @ self.W_dec2 + self.b_dec2\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the VAE.\"\"\"\n",
    "        mu, log_var = self.encode(x)\n",
    "        z, eps = self.reparameterize(mu, log_var)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, log_var, z, eps\n",
    "    \n",
    "    def compute_loss(self, x, x_recon, mu, log_var):\n",
    "        \"\"\"Compute VAE loss: reconstruction + KL divergence.\"\"\"\n",
    "        # Reconstruction loss (MSE)\n",
    "        recon_loss = np.mean(np.sum((x - x_recon)**2, axis=1))\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * np.mean(np.sum(1 + log_var - mu**2 - np.exp(log_var), axis=1))\n",
    "        \n",
    "        return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "    \n",
    "    def backward(self, x, x_recon, mu, log_var, z, eps, lr=0.001):\n",
    "        \"\"\"Backward pass with gradient descent.\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        std = np.exp(0.5 * log_var)\n",
    "        \n",
    "        # Gradient of reconstruction loss w.r.t. x_recon\n",
    "        d_recon = 2 * (x_recon - x) / batch_size\n",
    "        \n",
    "        # Decoder gradients\n",
    "        d_W_dec2 = self.h_dec.T @ d_recon\n",
    "        d_b_dec2 = np.sum(d_recon, axis=0)\n",
    "        \n",
    "        d_h_dec = d_recon @ self.W_dec2.T\n",
    "        d_h_dec = d_h_dec * self.relu_derivative(self.h_dec)\n",
    "        \n",
    "        d_W_dec1 = z.T @ d_h_dec\n",
    "        d_b_dec1 = np.sum(d_h_dec, axis=0)\n",
    "        \n",
    "        # Gradient w.r.t. z\n",
    "        d_z = d_h_dec @ self.W_dec1.T\n",
    "        \n",
    "        # Gradient w.r.t. mu and log_var (through reparameterization)\n",
    "        d_mu = d_z + mu / batch_size  # KL gradient for mu\n",
    "        d_log_var = d_z * eps * 0.5 * std + 0.5 * (np.exp(log_var) - 1) / batch_size  # KL gradient for log_var\n",
    "        \n",
    "        # Encoder gradients for mu path\n",
    "        d_W_mu = self.h_enc.T @ d_mu\n",
    "        d_b_mu = np.sum(d_mu, axis=0)\n",
    "        \n",
    "        # Encoder gradients for log_var path\n",
    "        d_W_logvar = self.h_enc.T @ d_log_var\n",
    "        d_b_logvar = np.sum(d_log_var, axis=0)\n",
    "        \n",
    "        # Gradient w.r.t. h_enc\n",
    "        d_h_enc = d_mu @ self.W_mu.T + d_log_var @ self.W_logvar.T\n",
    "        d_h_enc = d_h_enc * self.relu_derivative(self.h_enc)\n",
    "        \n",
    "        d_W_enc1 = x.T @ d_h_enc\n",
    "        d_b_enc1 = np.sum(d_h_enc, axis=0)\n",
    "        \n",
    "        # Update weights\n",
    "        self.W_enc1 -= lr * d_W_enc1\n",
    "        self.b_enc1 -= lr * d_b_enc1\n",
    "        self.W_mu -= lr * d_W_mu\n",
    "        self.b_mu -= lr * d_b_mu\n",
    "        self.W_logvar -= lr * d_W_logvar\n",
    "        self.b_logvar -= lr * d_b_logvar\n",
    "        self.W_dec1 -= lr * d_W_dec1\n",
    "        self.b_dec1 -= lr * d_b_dec1\n",
    "        self.W_dec2 -= lr * d_W_dec2\n",
    "        self.b_dec2 -= lr * d_b_dec2\n",
    "    \n",
    "    def train(self, X, epochs=500, batch_size=64, lr=0.01):\n",
    "        \"\"\"Train the VAE.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        losses = {'total': [], 'recon': [], 'kl': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            epoch_recon = 0\n",
    "            epoch_kl = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch = X_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                x_recon, mu, log_var, z, eps = self.forward(batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, recon_loss, kl_loss = self.compute_loss(batch, x_recon, mu, log_var)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(batch, x_recon, mu, log_var, z, eps, lr)\n",
    "                \n",
    "                epoch_loss += loss\n",
    "                epoch_recon += recon_loss\n",
    "                epoch_kl += kl_loss\n",
    "                n_batches += 1\n",
    "            \n",
    "            losses['total'].append(epoch_loss / n_batches)\n",
    "            losses['recon'].append(epoch_recon / n_batches)\n",
    "            losses['kl'].append(epoch_kl / n_batches)\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {losses['total'][-1]:.4f}, \"\n",
    "                      f\"Recon: {losses['recon'][-1]:.4f}, KL: {losses['kl'][-1]:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Generate samples from the prior.\"\"\"\n",
    "        z = np.random.randn(n_samples, self.latent_dim)\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train VAE\n",
    "vae = VAE(input_dim=2, hidden_dim=32, latent_dim=2)\n",
    "losses = vae.train(X_train, epochs=500, batch_size=64, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training loss curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(losses['total'], label='Total Loss', linewidth=2)\n",
    "ax1.plot(losses['recon'], label='Reconstruction', linewidth=2)\n",
    "ax1.plot(losses['kl'], label='KL Divergence', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss Components')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Original data\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(X_train[:, 0], X_train[:, 1], alpha=0.5, s=10, c='blue')\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_title('Original Data')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "# Plot 3: Latent space encoding\n",
    "mu, log_var = vae.encode(X_train)\n",
    "ax3 = axes[0, 2]\n",
    "scatter = ax3.scatter(mu[:, 0], mu[:, 1], alpha=0.5, s=10, c='green')\n",
    "ax3.set_xlabel('$z_1$')\n",
    "ax3.set_ylabel('$z_2$')\n",
    "ax3.set_title('Latent Space Encoding ($\\\\mu$)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_aspect('equal')\n",
    "\n",
    "# Plot 4: Reconstructions\n",
    "x_recon, _, _, _, _ = vae.forward(X_train)\n",
    "ax4 = axes[1, 0]\n",
    "ax4.scatter(x_recon[:, 0], x_recon[:, 1], alpha=0.5, s=10, c='orange')\n",
    "ax4.set_xlabel('$\\\\hat{x}_1$')\n",
    "ax4.set_ylabel('$\\\\hat{x}_2$')\n",
    "ax4.set_title('Reconstructed Data')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_aspect('equal')\n",
    "\n",
    "# Plot 5: Generated samples from prior\n",
    "generated = vae.sample(2000)\n",
    "ax5 = axes[1, 1]\n",
    "ax5.scatter(generated[:, 0], generated[:, 1], alpha=0.5, s=10, c='red')\n",
    "ax5.set_xlabel('$x_1$')\n",
    "ax5.set_ylabel('$x_2$')\n",
    "ax5.set_title('Generated Samples from Prior')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "ax5.set_aspect('equal')\n",
    "\n",
    "# Plot 6: Latent space traversal (2D manifold)\n",
    "ax6 = axes[1, 2]\n",
    "n_grid = 20\n",
    "z1_range = np.linspace(-3, 3, n_grid)\n",
    "z2_range = np.linspace(-3, 3, n_grid)\n",
    "Z1, Z2 = np.meshgrid(z1_range, z2_range)\n",
    "z_grid = np.column_stack([Z1.ravel(), Z2.ravel()])\n",
    "\n",
    "# Decode grid points\n",
    "decoded_grid = vae.decode(z_grid)\n",
    "ax6.scatter(decoded_grid[:, 0], decoded_grid[:, 1], \n",
    "            c=np.linalg.norm(z_grid, axis=1), cmap='viridis', \n",
    "            alpha=0.7, s=15)\n",
    "ax6.set_xlabel('$x_1$')\n",
    "ax6.set_ylabel('$x_2$')\n",
    "ax6.set_title('Decoded Latent Grid (colored by ||z||)')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "ax6.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis\n",
    "\n",
    "### 4.1 Loss Decomposition\n",
    "\n",
    "The training dynamics show the interplay between reconstruction and regularization:\n",
    "- **Reconstruction loss** decreases as the model learns to encode/decode data\n",
    "- **KL divergence** ensures the latent distribution approximates $\\mathcal{N}(0, I)$\n",
    "\n",
    "### 4.2 Latent Space Structure\n",
    "\n",
    "The latent space visualization shows that:\n",
    "- Data is encoded into a continuous, smooth manifold\n",
    "- The KL regularization encourages a compact representation centered at origin\n",
    "- Different clusters in data space map to different regions in latent space\n",
    "\n",
    "### 4.3 Generative Capability\n",
    "\n",
    "Sampling from the prior $\\mathcal{N}(0, I)$ and decoding produces new data points that:\n",
    "- Follow the same distribution as training data\n",
    "- Demonstrate smooth interpolation between modes\n",
    "\n",
    "## 5. Key Takeaways\n",
    "\n",
    "1. **VAEs combine neural networks with variational inference** to learn probabilistic latent representations\n",
    "\n",
    "2. **The ELBO objective balances two goals**: accurate reconstruction and latent space regularization\n",
    "\n",
    "3. **The reparameterization trick** enables end-to-end training via backpropagation\n",
    "\n",
    "4. **VAEs are true generative models** that can sample new data by decoding from the prior\n",
    "\n",
    "5. **The latent space is continuous and structured**, enabling meaningful interpolation and manipulation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
