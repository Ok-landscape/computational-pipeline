{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Dimensionality Reduction\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Autoencoders are a class of neural networks designed for unsupervised representation learning. Their primary utility lies in learning a compressed, lower-dimensional representation (encoding) of input data, from which the original data can be approximately reconstructed.\n",
    "\n",
    "### 1.1 Mathematical Foundation\n",
    "\n",
    "An autoencoder consists of two components:\n",
    "\n",
    "1. **Encoder** $f_{\\phi}: \\mathcal{X} \\rightarrow \\mathcal{Z}$ that maps input $\\mathbf{x} \\in \\mathbb{R}^n$ to a latent representation $\\mathbf{z} \\in \\mathbb{R}^d$ where $d < n$\n",
    "\n",
    "2. **Decoder** $g_{\\theta}: \\mathcal{Z} \\rightarrow \\mathcal{X}$ that reconstructs $\\hat{\\mathbf{x}} \\in \\mathbb{R}^n$ from $\\mathbf{z}$\n",
    "\n",
    "The complete autoencoder function is:\n",
    "\n",
    "$$\\hat{\\mathbf{x}} = g_{\\theta}(f_{\\phi}(\\mathbf{x}))$$\n",
    "\n",
    "### 1.2 Loss Function\n",
    "\n",
    "The network is trained to minimize the reconstruction error:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{x}, \\hat{\\mathbf{x}}) = \\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|_2^2 = \\sum_{i=1}^{n}(x_i - \\hat{x}_i)^2$$\n",
    "\n",
    "For a dataset $\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots, \\mathbf{x}^{(m)}\\}$, the empirical risk is:\n",
    "\n",
    "$$J(\\phi, \\theta) = \\frac{1}{m}\\sum_{j=1}^{m}\\mathcal{L}(\\mathbf{x}^{(j)}, g_{\\theta}(f_{\\phi}(\\mathbf{x}^{(j)})))$$\n",
    "\n",
    "### 1.3 Architecture\n",
    "\n",
    "For a simple fully-connected autoencoder with one hidden layer, the encoder applies:\n",
    "\n",
    "$$\\mathbf{z} = \\sigma(\\mathbf{W}_e \\mathbf{x} + \\mathbf{b}_e)$$\n",
    "\n",
    "where $\\mathbf{W}_e \\in \\mathbb{R}^{d \\times n}$, $\\mathbf{b}_e \\in \\mathbb{R}^d$, and $\\sigma$ is a nonlinear activation function (e.g., ReLU: $\\sigma(z) = \\max(0, z)$).\n",
    "\n",
    "The decoder reconstructs:\n",
    "\n",
    "$$\\hat{\\mathbf{x}} = \\sigma(\\mathbf{W}_d \\mathbf{z} + \\mathbf{b}_d)$$\n",
    "\n",
    "where $\\mathbf{W}_d \\in \\mathbb{R}^{n \\times d}$ and $\\mathbf{b}_d \\in \\mathbb{R}^n$.\n",
    "\n",
    "### 1.4 Gradient Descent Update\n",
    "\n",
    "Parameters are updated via backpropagation:\n",
    "\n",
    "$$\\phi \\leftarrow \\phi - \\eta \\nabla_{\\phi} J(\\phi, \\theta)$$\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\phi, \\theta)$$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation\n",
    "\n",
    "We will implement a simple autoencoder from scratch using only NumPy to demonstrate dimensionality reduction on synthetic data. This implementation uses mini-batch gradient descent with the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generate Synthetic High-Dimensional Data\n",
    "\n",
    "We generate data that lies on a 2D manifold embedded in 10D space. This simulates the common scenario where high-dimensional data has intrinsic low-dimensional structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_manifold_data(n_samples=1000, n_features=10, intrinsic_dim=2, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate data on a low-dimensional manifold in high-dimensional space.\n",
    "    \n",
    "    The data is generated by:\n",
    "    1. Sampling points in intrinsic_dim dimensions\n",
    "    2. Applying a random linear transformation to n_features dimensions\n",
    "    3. Adding nonlinearity and noise\n",
    "    \"\"\"\n",
    "    # Generate intrinsic coordinates (3 clusters)\n",
    "    t = np.zeros((n_samples, intrinsic_dim))\n",
    "    labels = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    n_per_cluster = n_samples // 3\n",
    "    centers = [[-2, -2], [2, -2], [0, 2]]\n",
    "    \n",
    "    for i, center in enumerate(centers):\n",
    "        start_idx = i * n_per_cluster\n",
    "        end_idx = start_idx + n_per_cluster if i < 2 else n_samples\n",
    "        t[start_idx:end_idx] = np.random.randn(end_idx - start_idx, intrinsic_dim) * 0.5 + center\n",
    "        labels[start_idx:end_idx] = i\n",
    "    \n",
    "    # Random projection matrix\n",
    "    W = np.random.randn(intrinsic_dim, n_features) / np.sqrt(intrinsic_dim)\n",
    "    \n",
    "    # Nonlinear transformation with noise\n",
    "    X = np.tanh(t @ W) + noise * np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    return X, t, labels\n",
    "\n",
    "# Generate data\n",
    "X, t_true, labels = generate_manifold_data(n_samples=1500, n_features=10, intrinsic_dim=2)\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"True intrinsic coordinates shape: {t_true.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Autoencoder Implementation\n",
    "\n",
    "We implement a 3-layer autoencoder: Input → Hidden (encoding) → Output (reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    \"\"\"\n",
    "    Simple fully-connected autoencoder with one hidden (bottleneck) layer.\n",
    "    Uses ReLU activation and Adam optimizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, encoding_dim, hidden_dim=32):\n",
    "        \"\"\"\n",
    "        Initialize autoencoder.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input data\n",
    "        encoding_dim : int\n",
    "            Dimension of the bottleneck (latent) layer\n",
    "        hidden_dim : int\n",
    "            Dimension of intermediate hidden layers\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_dim, encoding_dim) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.b2 = np.zeros((1, encoding_dim))\n",
    "        \n",
    "        self.W3 = np.random.randn(encoding_dim, hidden_dim) * np.sqrt(2.0 / encoding_dim)\n",
    "        self.b3 = np.zeros((1, hidden_dim))\n",
    "        \n",
    "        self.W4 = np.random.randn(hidden_dim, input_dim) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.b4 = np.zeros((1, input_dim))\n",
    "        \n",
    "        # Adam optimizer parameters\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "        for name in ['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4']:\n",
    "            self.m[name] = np.zeros_like(getattr(self, name))\n",
    "            self.v[name] = np.zeros_like(getattr(self, name))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation: max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"Derivative of ReLU\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def encode(self, X):\n",
    "        \"\"\"Encode input to latent representation.\"\"\"\n",
    "        h1 = self.relu(X @ self.W1 + self.b1)\n",
    "        z = X @ self.W1 + self.b1\n",
    "        h1 = self.relu(z)\n",
    "        z2 = h1 @ self.W2 + self.b2\n",
    "        return z2  # Linear encoding layer\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent representation to reconstruction.\"\"\"\n",
    "        h3 = self.relu(z @ self.W3 + self.b3)\n",
    "        x_hat = h3 @ self.W4 + self.b4  # Linear output\n",
    "        return x_hat\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the autoencoder.\"\"\"\n",
    "        # Encoder\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.h1 = self.relu(self.z1)\n",
    "        self.z2 = self.h1 @ self.W2 + self.b2  # Encoding (linear)\n",
    "        \n",
    "        # Decoder\n",
    "        self.z3 = self.z2 @ self.W3 + self.b3\n",
    "        self.h3 = self.relu(self.z3)\n",
    "        self.z4 = self.h3 @ self.W4 + self.b4  # Reconstruction (linear)\n",
    "        \n",
    "        return self.z4\n",
    "    \n",
    "    def backward(self, X, X_hat):\n",
    "        \"\"\"Compute gradients via backpropagation.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradient (MSE loss derivative)\n",
    "        dz4 = (X_hat - X) / m\n",
    "        \n",
    "        self.dW4 = self.h3.T @ dz4\n",
    "        self.db4 = np.sum(dz4, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 3\n",
    "        dh3 = dz4 @ self.W4.T\n",
    "        dz3 = dh3 * self.relu_derivative(self.z3)\n",
    "        \n",
    "        self.dW3 = self.z2.T @ dz3\n",
    "        self.db3 = np.sum(dz3, axis=0, keepdims=True)\n",
    "        \n",
    "        # Encoding layer (linear, so no activation derivative)\n",
    "        dz2 = dz3 @ self.W3.T\n",
    "        \n",
    "        self.dW2 = self.h1.T @ dz2\n",
    "        self.db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        dh1 = dz2 @ self.W2.T\n",
    "        dz1 = dh1 * self.relu_derivative(self.z1)\n",
    "        \n",
    "        self.dW1 = X.T @ dz1\n",
    "        self.db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "    \n",
    "    def adam_update(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        \"\"\"Update weights using Adam optimizer.\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        for name in ['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4']:\n",
    "            grad = getattr(self, 'd' + name)\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            self.m[name] = beta1 * self.m[name] + (1 - beta1) * grad\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[name] = beta2 * self.v[name] + (1 - beta2) * (grad ** 2)\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = self.m[name] / (1 - beta1 ** self.t)\n",
    "            v_hat = self.v[name] / (1 - beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            param = getattr(self, name)\n",
    "            param -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            setattr(self, name, param)\n",
    "    \n",
    "    def fit(self, X, epochs=200, batch_size=64, lr=0.001, verbose=True):\n",
    "        \"\"\"Train the autoencoder.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                X_hat = self.forward(X_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = np.mean((X_batch - X_hat) ** 2)\n",
    "                epoch_loss += loss\n",
    "                n_batches += 1\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(X_batch, X_hat)\n",
    "                \n",
    "                # Update weights\n",
    "                self.adam_update(lr=lr)\n",
    "            \n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Preprocessing\n",
    "\n",
    "Standardize the data to have zero mean and unit variance for stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "X_normalized = (X - X_mean) / (X_std + 1e-8)\n",
    "\n",
    "# Split into train and test\n",
    "n_train = int(0.8 * len(X_normalized))\n",
    "indices = np.random.permutation(len(X_normalized))\n",
    "train_idx, test_idx = indices[:n_train], indices[n_train:]\n",
    "\n",
    "X_train = X_normalized[train_idx]\n",
    "X_test = X_normalized[test_idx]\n",
    "labels_train = labels[train_idx]\n",
    "labels_test = labels[test_idx]\n",
    "t_true_test = t_true[test_idx]\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train autoencoder\n",
    "ae = Autoencoder(input_dim=10, encoding_dim=2, hidden_dim=32)\n",
    "losses = ae.fit(X_train, epochs=200, batch_size=64, lr=0.005, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Evaluate Reconstruction Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test reconstruction error\n",
    "X_test_reconstructed = ae.forward(X_test)\n",
    "test_mse = np.mean((X_test - X_test_reconstructed) ** 2)\n",
    "print(f\"Test MSE: {test_mse:.6f}\")\n",
    "\n",
    "# Compute variance explained\n",
    "total_variance = np.var(X_test)\n",
    "residual_variance = np.var(X_test - X_test_reconstructed)\n",
    "variance_explained = 1 - (residual_variance / total_variance)\n",
    "print(f\"Variance explained: {variance_explained:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encodings for test data\n",
    "encodings = ae.encode(X_test)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Training loss curve\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(losses, 'b-', linewidth=1.5)\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('MSE Loss', fontsize=11)\n",
    "ax1.set_title('Training Loss Convergence', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: Learned latent space (2D encodings)\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a']\n",
    "for i in range(3):\n",
    "    mask = labels_test == i\n",
    "    ax2.scatter(encodings[mask, 0], encodings[mask, 1], \n",
    "                c=colors[i], label=f'Cluster {i+1}', alpha=0.6, s=30)\n",
    "ax2.set_xlabel('Latent Dimension 1', fontsize=11)\n",
    "ax2.set_ylabel('Latent Dimension 2', fontsize=11)\n",
    "ax2.set_title('Autoencoder Latent Space', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: True intrinsic coordinates for comparison\n",
    "ax3 = axes[1, 0]\n",
    "for i in range(3):\n",
    "    mask = labels_test == i\n",
    "    ax3.scatter(t_true_test[mask, 0], t_true_test[mask, 1], \n",
    "                c=colors[i], label=f'Cluster {i+1}', alpha=0.6, s=30)\n",
    "ax3.set_xlabel('True Dimension 1', fontsize=11)\n",
    "ax3.set_ylabel('True Dimension 2', fontsize=11)\n",
    "ax3.set_title('True Intrinsic Coordinates', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Reconstruction error distribution\n",
    "ax4 = axes[1, 1]\n",
    "reconstruction_errors = np.mean((X_test - X_test_reconstructed) ** 2, axis=1)\n",
    "for i in range(3):\n",
    "    mask = labels_test == i\n",
    "    ax4.hist(reconstruction_errors[mask], bins=20, alpha=0.5, \n",
    "             color=colors[i], label=f'Cluster {i+1}')\n",
    "ax4.set_xlabel('Per-sample MSE', fontsize=11)\n",
    "ax4.set_ylabel('Frequency', fontsize=11)\n",
    "ax4.set_title('Reconstruction Error Distribution', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison with PCA\n",
    "\n",
    "We compare the autoencoder's learned representation with Principal Component Analysis (PCA), a classical linear dimensionality reduction technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, n_components):\n",
    "    \"\"\"\n",
    "    Principal Component Analysis implementation.\n",
    "    \n",
    "    PCA finds the directions of maximum variance:\n",
    "    1. Center the data\n",
    "    2. Compute covariance matrix: C = (1/n) X^T X\n",
    "    3. Eigendecomposition: C = V Λ V^T\n",
    "    4. Project onto top k eigenvectors\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    X_centered = X - X.mean(axis=0)\n",
    "    \n",
    "    # Compute covariance matrix\n",
    "    cov_matrix = np.cov(X_centered.T)\n",
    "    \n",
    "    # Eigendecomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    \n",
    "    # Sort by eigenvalue (descending)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # Select top components\n",
    "    components = eigenvectors[:, :n_components]\n",
    "    \n",
    "    # Project data\n",
    "    X_pca = X_centered @ components\n",
    "    \n",
    "    # Variance explained\n",
    "    var_explained = eigenvalues[:n_components] / eigenvalues.sum()\n",
    "    \n",
    "    return X_pca, var_explained, components\n",
    "\n",
    "# Apply PCA\n",
    "X_test_pca, pca_var_explained, pca_components = pca(X_test, n_components=2)\n",
    "\n",
    "print(f\"PCA variance explained by first 2 components: {pca_var_explained.sum():.2%}\")\n",
    "print(f\"  - PC1: {pca_var_explained[0]:.2%}\")\n",
    "print(f\"  - PC2: {pca_var_explained[1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PCA vs Autoencoder\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# PCA projection\n",
    "ax1 = axes[0]\n",
    "for i in range(3):\n",
    "    mask = labels_test == i\n",
    "    ax1.scatter(X_test_pca[mask, 0], X_test_pca[mask, 1], \n",
    "                c=colors[i], label=f'Cluster {i+1}', alpha=0.6, s=30)\n",
    "ax1.set_xlabel('PC1', fontsize=11)\n",
    "ax1.set_ylabel('PC2', fontsize=11)\n",
    "ax1.set_title(f'PCA (Variance Explained: {pca_var_explained.sum():.1%})', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Autoencoder projection\n",
    "ax2 = axes[1]\n",
    "for i in range(3):\n",
    "    mask = labels_test == i\n",
    "    ax2.scatter(encodings[mask, 0], encodings[mask, 1], \n",
    "                c=colors[i], label=f'Cluster {i+1}', alpha=0.6, s=30)\n",
    "ax2.set_xlabel('Latent Dim 1', fontsize=11)\n",
    "ax2.set_ylabel('Latent Dim 2', fontsize=11)\n",
    "ax2.set_title(f'Autoencoder (Variance Explained: {variance_explained:.1%})', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Autoencoder Architecture**: A neural network with encoder $f_{\\phi}$ and decoder $g_{\\theta}$ that learns compressed representations by minimizing reconstruction error $\\|\\mathbf{x} - g_{\\theta}(f_{\\phi}(\\mathbf{x}))\\|_2^2$.\n",
    "\n",
    "2. **Nonlinear Dimensionality Reduction**: Unlike PCA which finds linear projections, autoencoders can capture nonlinear manifold structure through their nonlinear activation functions.\n",
    "\n",
    "3. **Cluster Preservation**: The 2D latent space preserves the cluster structure present in the original 10D data, demonstrating effective dimensionality reduction.\n",
    "\n",
    "4. **Comparison with PCA**: Both methods reduce dimensionality, but autoencoders can potentially capture more complex relationships due to their nonlinearity.\n",
    "\n",
    "### Key Equations Summary\n",
    "\n",
    "- **Encoder**: $\\mathbf{z} = f_{\\phi}(\\mathbf{x})$\n",
    "- **Decoder**: $\\hat{\\mathbf{x}} = g_{\\theta}(\\mathbf{z})$  \n",
    "- **Loss**: $\\mathcal{L} = \\frac{1}{m}\\sum_{i=1}^{m}\\|\\mathbf{x}^{(i)} - \\hat{\\mathbf{x}}^{(i)}\\|_2^2$\n",
    "- **Adam Update**: $\\theta \\leftarrow \\theta - \\eta \\cdot \\hat{m}_t / (\\sqrt{\\hat{v}_t} + \\epsilon)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
