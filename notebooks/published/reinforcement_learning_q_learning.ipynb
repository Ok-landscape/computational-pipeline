{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Q-Learning Algorithm\n",
    "\n",
    "## 1. Introduction to Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning (RL) is a paradigm of machine learning where an **agent** learns to make decisions by interacting with an **environment**. Unlike supervised learning, the agent is not given explicit correct answers but instead receives **rewards** or **penalties** based on its actions.\n",
    "\n",
    "### 1.1 The Markov Decision Process (MDP)\n",
    "\n",
    "RL problems are typically formalized as a **Markov Decision Process** (MDP), defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$:\n",
    "\n",
    "- $\\mathcal{S}$: State space (set of all possible states)\n",
    "- $\\mathcal{A}$: Action space (set of all possible actions)\n",
    "- $P(s'|s,a)$: State transition probability\n",
    "- $R(s,a,s')$: Reward function\n",
    "- $\\gamma \\in [0,1]$: Discount factor\n",
    "\n",
    "### 1.2 The Value Function\n",
    "\n",
    "The **state-value function** $V^\\pi(s)$ represents the expected cumulative reward starting from state $s$ and following policy $\\pi$:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\,\\bigg|\\, S_0 = s\\right]$$\n",
    "\n",
    "### 1.3 The Action-Value Function (Q-Function)\n",
    "\n",
    "The **action-value function** $Q^\\pi(s,a)$ represents the expected cumulative reward starting from state $s$, taking action $a$, and then following policy $\\pi$:\n",
    "\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\,\\bigg|\\, S_0 = s, A_0 = a\\right]$$\n",
    "\n",
    "## 2. Q-Learning Algorithm\n",
    "\n",
    "### 2.1 The Bellman Optimality Equation\n",
    "\n",
    "The optimal Q-function satisfies the **Bellman optimality equation**:\n",
    "\n",
    "$$Q^*(s,a) = \\mathbb{E}\\left[R_{t+1} + \\gamma \\max_{a'} Q^*(S_{t+1}, a') \\,\\bigg|\\, S_t = s, A_t = a\\right]$$\n",
    "\n",
    "### 2.2 Temporal Difference Learning\n",
    "\n",
    "Q-Learning is an **off-policy temporal difference (TD)** algorithm that directly approximates $Q^*$ without requiring a model of the environment. The update rule is:\n",
    "\n",
    "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)\\right]$$\n",
    "\n",
    "where:\n",
    "- $\\alpha \\in (0,1]$ is the **learning rate**\n",
    "- $r_{t+1}$ is the immediate reward\n",
    "- $\\gamma$ is the discount factor\n",
    "- $\\max_{a} Q(s_{t+1}, a)$ is the maximum Q-value at the next state\n",
    "\n",
    "The term in brackets is called the **TD error**:\n",
    "\n",
    "$$\\delta_t = r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)$$\n",
    "\n",
    "### 2.3 Exploration vs. Exploitation: $\\epsilon$-Greedy Policy\n",
    "\n",
    "To balance exploration and exploitation, Q-Learning uses an **$\\epsilon$-greedy policy**:\n",
    "\n",
    "$$a = \\begin{cases} \\text{random action} & \\text{with probability } \\epsilon \\\\ \\arg\\max_a Q(s, a) & \\text{with probability } 1-\\epsilon \\end{cases}$$\n",
    "\n",
    "## 3. Implementation: GridWorld Environment\n",
    "\n",
    "We will implement Q-Learning on a simple **GridWorld** environment where an agent must navigate from a starting position to a goal while avoiding obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    A simple GridWorld environment for reinforcement learning.\n",
    "    \n",
    "    The agent starts at position (0, 0) and must reach the goal at (grid_size-1, grid_size-1).\n",
    "    Obstacles provide negative rewards, and reaching the goal provides a positive reward.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=5):\n",
    "        self.grid_size = grid_size\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (grid_size - 1, grid_size - 1)\n",
    "        \n",
    "        # Actions: 0=up, 1=right, 2=down, 3=left\n",
    "        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = grid_size * grid_size\n",
    "        \n",
    "        # Define obstacles (negative reward areas)\n",
    "        self.obstacles = [(1, 1), (1, 3), (2, 1), (3, 3), (2, 3)]\n",
    "        \n",
    "        self.state = self.start\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the starting state.\"\"\"\n",
    "        self.state = self.start\n",
    "        return self._state_to_index(self.state)\n",
    "    \n",
    "    def _state_to_index(self, state):\n",
    "        \"\"\"Convert (row, col) to a single state index.\"\"\"\n",
    "        return state[0] * self.grid_size + state[1]\n",
    "    \n",
    "    def _index_to_state(self, index):\n",
    "        \"\"\"Convert state index to (row, col).\"\"\"\n",
    "        return (index // self.grid_size, index % self.grid_size)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action and return (next_state, reward, done).\n",
    "        \"\"\"\n",
    "        # Calculate next position\n",
    "        dr, dc = self.actions[action]\n",
    "        new_row = max(0, min(self.grid_size - 1, self.state[0] + dr))\n",
    "        new_col = max(0, min(self.grid_size - 1, self.state[1] + dc))\n",
    "        self.state = (new_row, new_col)\n",
    "        \n",
    "        # Determine reward\n",
    "        if self.state == self.goal:\n",
    "            reward = 10.0  # Goal reward\n",
    "            done = True\n",
    "        elif self.state in self.obstacles:\n",
    "            reward = -5.0  # Obstacle penalty\n",
    "            done = False\n",
    "        else:\n",
    "            reward = -0.1  # Small step penalty to encourage efficiency\n",
    "            done = False\n",
    "        \n",
    "        return self._state_to_index(self.state), reward, done\n",
    "    \n",
    "    def render(self, q_table=None):\n",
    "        \"\"\"Visualize the grid and optionally the policy.\"\"\"\n",
    "        grid = np.zeros((self.grid_size, self.grid_size))\n",
    "        \n",
    "        # Mark obstacles\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs] = 1\n",
    "        \n",
    "        # Mark goal\n",
    "        grid[self.goal] = 2\n",
    "        \n",
    "        return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent that learns an optimal policy through interaction with the environment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_states : int\n",
    "        Number of states in the environment\n",
    "    n_actions : int\n",
    "        Number of possible actions\n",
    "    alpha : float\n",
    "        Learning rate (0 < alpha <= 1)\n",
    "    gamma : float\n",
    "        Discount factor (0 <= gamma <= 1)\n",
    "    epsilon : float\n",
    "        Exploration rate for epsilon-greedy policy\n",
    "    epsilon_decay : float\n",
    "        Rate at which epsilon decays over episodes\n",
    "    epsilon_min : float\n",
    "        Minimum value of epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, \n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action using epsilon-greedy policy.\n",
    "        \n",
    "        With probability epsilon: choose random action (exploration)\n",
    "        With probability 1-epsilon: choose best action (exploitation)\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Q-value using the Q-learning update rule:\n",
    "        Q(s,a) <- Q(s,a) + alpha * [r + gamma * max_a' Q(s',a') - Q(s,a)]\n",
    "        \"\"\"\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        \n",
    "        # TD error\n",
    "        td_error = td_target - self.q_table[state, action]\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.q_table[state, action] += self.alpha * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon after each episode.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Q-Learning Agent\n",
    "\n",
    "We will now train the agent over multiple episodes. During training, we track:\n",
    "- **Episode rewards**: Total reward accumulated per episode\n",
    "- **Episode lengths**: Number of steps taken to reach the goal\n",
    "- **Epsilon values**: The exploration rate over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, n_episodes=500, max_steps=100):\n",
    "    \"\"\"\n",
    "    Train the Q-learning agent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    env : GridWorld\n",
    "        The environment\n",
    "    agent : QLearningAgent\n",
    "        The learning agent\n",
    "    n_episodes : int\n",
    "        Number of training episodes\n",
    "    max_steps : int\n",
    "        Maximum steps per episode\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Training history containing rewards, lengths, and epsilon values\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'rewards': [],\n",
    "        'lengths': [],\n",
    "        'epsilons': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Choose action\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Update Q-table\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Record history\n",
    "        history['rewards'].append(total_reward)\n",
    "        history['lengths'].append(step + 1)\n",
    "        history['epsilons'].append(agent.epsilon)\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Print progress every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(history['rewards'][-100:])\n",
    "            avg_length = np.mean(history['lengths'][-100:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Length: {avg_length:.1f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and agent\n",
    "env = GridWorld(grid_size=5)\n",
    "agent = QLearningAgent(\n",
    "    n_states=env.n_states,\n",
    "    n_actions=env.n_actions,\n",
    "    alpha=0.1,      # Learning rate\n",
    "    gamma=0.99,     # Discount factor\n",
    "    epsilon=1.0,    # Initial exploration rate\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "print(\"Training Q-Learning Agent...\\n\")\n",
    "history = train_agent(env, agent, n_episodes=500, max_steps=100)\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Visualization\n",
    "\n",
    "We now analyze the training results by visualizing:\n",
    "1. **Learning curves**: Episode rewards and lengths over time\n",
    "2. **Learned Q-values**: The final Q-table as a heatmap\n",
    "3. **Optimal policy**: The best action at each state\n",
    "4. **Agent trajectory**: Path taken by the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(history, agent, env):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of Q-learning results.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # 1. Episode Rewards (with moving average)\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    rewards = history['rewards']\n",
    "    window = 20\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "    ax1.plot(range(window-1, len(rewards)), moving_avg, color='red', \n",
    "             linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "    ax1.set_xlabel('Episode', fontsize=11)\n",
    "    ax1.set_ylabel('Total Reward', fontsize=11)\n",
    "    ax1.set_title('Learning Curve: Episode Rewards', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Episode Lengths\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    lengths = history['lengths']\n",
    "    moving_avg_len = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(lengths, alpha=0.3, color='green', label='Episode Length')\n",
    "    ax2.plot(range(window-1, len(lengths)), moving_avg_len, color='darkgreen', \n",
    "             linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "    ax2.set_xlabel('Episode', fontsize=11)\n",
    "    ax2.set_ylabel('Steps to Goal', fontsize=11)\n",
    "    ax2.set_title('Learning Curve: Episode Lengths', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Q-Values Heatmap (max Q-value at each state)\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    q_max = np.max(agent.q_table, axis=1).reshape(env.grid_size, env.grid_size)\n",
    "    im = ax3.imshow(q_max, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(im, ax=ax3, label='Max Q-Value')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(env.grid_size):\n",
    "        for j in range(env.grid_size):\n",
    "            text_color = 'white' if q_max[i, j] < np.mean(q_max) else 'black'\n",
    "            ax3.text(j, i, f'{q_max[i, j]:.1f}', ha='center', va='center', \n",
    "                    color=text_color, fontsize=9)\n",
    "    \n",
    "    ax3.set_xlabel('Column', fontsize=11)\n",
    "    ax3.set_ylabel('Row', fontsize=11)\n",
    "    ax3.set_title('Learned Value Function (Max Q-Values)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(range(env.grid_size))\n",
    "    ax3.set_yticks(range(env.grid_size))\n",
    "    \n",
    "    # 4. Optimal Policy Visualization\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    \n",
    "    # Create grid\n",
    "    grid = np.zeros((env.grid_size, env.grid_size))\n",
    "    for obs in env.obstacles:\n",
    "        grid[obs] = 1\n",
    "    grid[env.goal] = 2\n",
    "    \n",
    "    # Custom colormap\n",
    "    colors = ['#E8F5E9', '#EF9A9A', '#81C784']  # Empty, Obstacle, Goal\n",
    "    cmap = ListedColormap(colors)\n",
    "    ax4.imshow(grid, cmap=cmap, interpolation='nearest')\n",
    "    \n",
    "    # Draw policy arrows\n",
    "    arrow_symbols = ['↑', '→', '↓', '←']\n",
    "    for i in range(env.grid_size):\n",
    "        for j in range(env.grid_size):\n",
    "            if (i, j) == env.goal:\n",
    "                ax4.text(j, i, '★', ha='center', va='center', fontsize=16, color='gold')\n",
    "            elif (i, j) in env.obstacles:\n",
    "                ax4.text(j, i, '✕', ha='center', va='center', fontsize=14, color='darkred')\n",
    "            else:\n",
    "                state_idx = i * env.grid_size + j\n",
    "                best_action = np.argmax(agent.q_table[state_idx])\n",
    "                ax4.text(j, i, arrow_symbols[best_action], ha='center', va='center', \n",
    "                        fontsize=16, color='navy', fontweight='bold')\n",
    "    \n",
    "    ax4.set_xlabel('Column', fontsize=11)\n",
    "    ax4.set_ylabel('Row', fontsize=11)\n",
    "    ax4.set_title('Learned Optimal Policy', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xticks(range(env.grid_size))\n",
    "    ax4.set_yticks(range(env.grid_size))\n",
    "    ax4.grid(True, alpha=0.3, color='gray', linewidth=0.5)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor='#E8F5E9', edgecolor='black', label='Empty'),\n",
    "        mpatches.Patch(facecolor='#EF9A9A', edgecolor='black', label='Obstacle'),\n",
    "        mpatches.Patch(facecolor='#81C784', edgecolor='black', label='Goal')\n",
    "    ]\n",
    "    ax4.legend(handles=legend_elements, loc='upper left', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('plot.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    print(\"Figure saved to 'plot.png'\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Generate visualization\n",
    "fig = plot_training_results(history, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Trained Agent\n",
    "\n",
    "Let's test the trained agent by running it through the environment using the learned policy (without exploration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, n_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluate the trained agent without exploration.\n",
    "    \"\"\"\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0  # Pure exploitation\n",
    "    \n",
    "    total_rewards = []\n",
    "    total_steps = []\n",
    "    \n",
    "    print(\"Evaluation Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        path = [env._index_to_state(state)]\n",
    "        \n",
    "        for step in range(100):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            path.append(env._index_to_state(next_state))\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        total_steps.append(step + 1)\n",
    "        \n",
    "        status = \"SUCCESS\" if done else \"FAILED\"\n",
    "        print(f\"Episode {episode + 1}: {status} | Reward: {episode_reward:.2f} | Steps: {step + 1}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Average Reward: {np.mean(total_rewards):.2f} ± {np.std(total_rewards):.2f}\")\n",
    "    print(f\"Average Steps: {np.mean(total_steps):.1f} ± {np.std(total_steps):.1f}\")\n",
    "    print(f\"Success Rate: {sum([1 for r in total_rewards if r > 0]) / n_episodes * 100:.1f}%\")\n",
    "    \n",
    "    agent.epsilon = original_epsilon\n",
    "    return total_rewards, total_steps\n",
    "\n",
    "# Evaluate the trained agent\n",
    "eval_rewards, eval_steps = evaluate_agent(env, agent, n_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Displaying the Final Q-Table\n",
    "\n",
    "The Q-table contains the learned action-values for each state-action pair. Each row represents a state, and each column represents an action (Up, Right, Down, Left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a formatted Q-table\n",
    "q_table_df = pd.DataFrame(\n",
    "    agent.q_table,\n",
    "    columns=['Up (↑)', 'Right (→)', 'Down (↓)', 'Left (←)'],\n",
    "    index=[f'State {i} ({i//5}, {i%5})' for i in range(env.n_states)]\n",
    ")\n",
    "\n",
    "print(\"Final Q-Table (rounded to 2 decimal places):\")\n",
    "print(\"=\" * 60)\n",
    "print(q_table_df.round(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this notebook, we implemented the **Q-Learning algorithm**, a foundational model-free reinforcement learning technique. Key takeaways:\n",
    "\n",
    "1. **Q-Learning learns the optimal action-value function** $Q^*(s,a)$ through temporal difference updates, without requiring knowledge of the environment dynamics.\n",
    "\n",
    "2. **The update rule** $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma\\max_{a'}Q(s',a') - Q(s,a)]$ iteratively improves estimates using bootstrapped targets.\n",
    "\n",
    "3. **Exploration-exploitation tradeoff** is managed through $\\epsilon$-greedy policy with decaying $\\epsilon$, allowing initial exploration while converging to exploitation.\n",
    "\n",
    "4. **Convergence**: Given sufficient exploration and appropriate hyperparameters ($\\alpha$, $\\gamma$), Q-learning is guaranteed to converge to the optimal Q-function.\n",
    "\n",
    "### Extensions\n",
    "\n",
    "- **Deep Q-Networks (DQN)**: Replace tabular Q-function with neural network for large/continuous state spaces\n",
    "- **Double Q-Learning**: Address overestimation bias\n",
    "- **Prioritized Experience Replay**: Sample important transitions more frequently\n",
    "- **Dueling DQN**: Separate value and advantage estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
