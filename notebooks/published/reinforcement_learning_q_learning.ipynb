{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Q-Learning Algorithm\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "**Q-Learning** is a model-free, off-policy reinforcement learning algorithm that learns the optimal action-value function $Q^*(s, a)$ directly from experience, without requiring a model of the environment dynamics.\n",
    "\n",
    "### 1.1 The Reinforcement Learning Framework\n",
    "\n",
    "In reinforcement learning, an agent interacts with an environment over discrete time steps. At each time step $t$:\n",
    "- The agent observes state $s_t \\in \\mathcal{S}$\n",
    "- Selects an action $a_t \\in \\mathcal{A}$\n",
    "- Receives a reward $r_{t+1} \\in \\mathbb{R}$\n",
    "- Transitions to a new state $s_{t+1}$\n",
    "\n",
    "The goal is to find a policy $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$ that maximizes the expected cumulative discounted reward:\n",
    "\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$$\n",
    "\n",
    "where $\\gamma \\in [0, 1)$ is the discount factor.\n",
    "\n",
    "### 1.2 The Action-Value Function\n",
    "\n",
    "The **action-value function** (or Q-function) under policy $\\pi$ is defined as:\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[G_t \\mid s_t = s, a_t = a\\right] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} \\mid s_t = s, a_t = a\\right]$$\n",
    "\n",
    "The **optimal action-value function** is:\n",
    "\n",
    "$$Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)$$\n",
    "\n",
    "### 1.3 The Bellman Optimality Equation\n",
    "\n",
    "The optimal Q-function satisfies the **Bellman optimality equation**:\n",
    "\n",
    "$$Q^*(s, a) = \\mathbb{E}\\left[r + \\gamma \\max_{a'} Q^*(s', a') \\mid s, a\\right]$$\n",
    "\n",
    "This recursive relationship is the foundation of Q-learning.\n",
    "\n",
    "### 1.4 The Q-Learning Update Rule\n",
    "\n",
    "Q-learning uses **temporal difference (TD) learning** to iteratively update the Q-values:\n",
    "\n",
    "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\\right]$$\n",
    "\n",
    "where:\n",
    "- $\\alpha \\in (0, 1]$ is the learning rate\n",
    "- $r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a')$ is the **TD target**\n",
    "- $r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)$ is the **TD error** $\\delta_t$\n",
    "\n",
    "### 1.5 Exploration vs. Exploitation\n",
    "\n",
    "Q-learning requires balancing exploration (trying new actions) and exploitation (using known good actions). The **$\\epsilon$-greedy** policy addresses this:\n",
    "\n",
    "$$\\pi(a|s) = \\begin{cases} 1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|} & \\text{if } a = \\arg\\max_{a'} Q(s, a') \\\\ \\frac{\\epsilon}{|\\mathcal{A}|} & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "### 1.6 Convergence Properties\n",
    "\n",
    "Under certain conditions (all state-action pairs visited infinitely often, learning rate satisfying Robbins-Monro conditions), Q-learning converges to $Q^*$ with probability 1:\n",
    "\n",
    "$$\\sum_{t=1}^{\\infty} \\alpha_t = \\infty \\quad \\text{and} \\quad \\sum_{t=1}^{\\infty} \\alpha_t^2 < \\infty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation: Gridworld Environment\n",
    "\n",
    "We will implement Q-learning on a simple gridworld environment where an agent must navigate from a start position to a goal while avoiding obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyArrowPatch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    A simple gridworld environment for Q-learning.\n",
    "    \n",
    "    The agent starts at position (0, 0) and must reach the goal.\n",
    "    Walls are impassable, and stepping on traps gives negative reward.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.n_states = size * size\n",
    "        self.n_actions = 4  # up, down, left, right\n",
    "        \n",
    "        # Action mappings\n",
    "        self.actions = {\n",
    "            0: (-1, 0),  # up\n",
    "            1: (1, 0),   # down\n",
    "            2: (0, -1),  # left\n",
    "            3: (0, 1)    # right\n",
    "        }\n",
    "        self.action_names = ['↑', '↓', '←', '→']\n",
    "        \n",
    "        # Define environment layout\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (size - 1, size - 1)\n",
    "        \n",
    "        # Define walls (impassable cells)\n",
    "        self.walls = {(1, 1), (2, 1), (3, 1), (1, 3), (2, 3)}\n",
    "        \n",
    "        # Define traps (negative reward cells)\n",
    "        self.traps = {(0, 4), (3, 3), (4, 1)}\n",
    "        \n",
    "        # Rewards\n",
    "        self.goal_reward = 100\n",
    "        self.trap_reward = -50\n",
    "        self.step_reward = -1\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent to the starting position.\"\"\"\n",
    "        self.agent_pos = self.start\n",
    "        return self._pos_to_state(self.agent_pos)\n",
    "    \n",
    "    def _pos_to_state(self, pos):\n",
    "        \"\"\"Convert (row, col) position to state index.\"\"\"\n",
    "        return pos[0] * self.size + pos[1]\n",
    "    \n",
    "    def _state_to_pos(self, state):\n",
    "        \"\"\"Convert state index to (row, col) position.\"\"\"\n",
    "        return (state // self.size, state % self.size)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return (next_state, reward, done).\"\"\"\n",
    "        # Calculate new position\n",
    "        delta = self.actions[action]\n",
    "        new_pos = (self.agent_pos[0] + delta[0], self.agent_pos[1] + delta[1])\n",
    "        \n",
    "        # Check boundaries and walls\n",
    "        if (0 <= new_pos[0] < self.size and \n",
    "            0 <= new_pos[1] < self.size and \n",
    "            new_pos not in self.walls):\n",
    "            self.agent_pos = new_pos\n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.agent_pos == self.goal:\n",
    "            reward = self.goal_reward\n",
    "            done = True\n",
    "        elif self.agent_pos in self.traps:\n",
    "            reward = self.trap_reward\n",
    "            done = False\n",
    "        else:\n",
    "            reward = self.step_reward\n",
    "            done = False\n",
    "        \n",
    "        return self._pos_to_state(self.agent_pos), reward, done\n",
    "    \n",
    "    def get_valid_actions(self, state):\n",
    "        \"\"\"Get list of valid actions from a state.\"\"\"\n",
    "        pos = self._state_to_pos(state)\n",
    "        valid = []\n",
    "        for action, delta in self.actions.items():\n",
    "            new_pos = (pos[0] + delta[0], pos[1] + delta[1])\n",
    "            if (0 <= new_pos[0] < self.size and \n",
    "                0 <= new_pos[1] < self.size and \n",
    "                new_pos not in self.walls):\n",
    "                valid.append(action)\n",
    "        return valid if valid else list(range(self.n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-Learning Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent with epsilon-greedy exploration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, \n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Q-learning agent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_states : int\n",
    "            Number of states in the environment\n",
    "        n_actions : int\n",
    "            Number of possible actions\n",
    "        alpha : float\n",
    "            Learning rate\n",
    "        gamma : float\n",
    "            Discount factor\n",
    "        epsilon : float\n",
    "            Initial exploration rate\n",
    "        epsilon_decay : float\n",
    "            Decay rate for epsilon after each episode\n",
    "        epsilon_min : float\n",
    "            Minimum epsilon value\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        \n",
    "        # For tracking learning progress\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Q-value using the Q-learning update rule.\n",
    "        \n",
    "        Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]\n",
    "        \"\"\"\n",
    "        # Calculate TD target\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * np.max(self.Q[next_state])\n",
    "        \n",
    "        # Calculate TD error\n",
    "        td_error = td_target - self.Q[state, action]\n",
    "        self.td_errors.append(abs(td_error))\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.Q[state, action] += self.alpha * td_error\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon after each episode.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, n_episodes=1000, max_steps=100):\n",
    "    \"\"\"\n",
    "    Train the Q-learning agent.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    rewards_history : list\n",
    "        Total reward per episode\n",
    "    steps_history : list\n",
    "        Number of steps per episode\n",
    "    epsilon_history : list\n",
    "        Epsilon value per episode\n",
    "    \"\"\"\n",
    "    rewards_history = []\n",
    "    steps_history = []\n",
    "    epsilon_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Choose and take action\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Learn from experience\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        steps_history.append(step + 1)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-100:])\n",
    "            avg_steps = np.mean(steps_history[-100:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Steps: {avg_steps:.2f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return rewards_history, steps_history, epsilon_history\n",
    "\n",
    "# Create environment and agent\n",
    "env = GridWorld(size=5)\n",
    "agent = QLearningAgent(\n",
    "    n_states=env.n_states,\n",
    "    n_actions=env.n_actions,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "print(\"Training Q-Learning Agent...\\n\")\n",
    "rewards, steps, epsilons = train_agent(env, agent, n_episodes=1000, max_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(rewards, steps, epsilons, agent, env):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of Q-learning results.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Learning Curve (Rewards)\n",
    "    ax1 = fig.add_subplot(2, 3, 1)\n",
    "    window = 50\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "    ax1.plot(range(window-1, len(rewards)), smoothed_rewards, \n",
    "             color='red', linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.set_title('Learning Curve: Rewards per Episode')\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Steps per Episode\n",
    "    ax2 = fig.add_subplot(2, 3, 2)\n",
    "    smoothed_steps = np.convolve(steps, np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(steps, alpha=0.3, color='green', label='Episode Steps')\n",
    "    ax2.plot(range(window-1, len(steps)), smoothed_steps, \n",
    "             color='darkgreen', linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Steps')\n",
    "    ax2.set_title('Steps per Episode')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Epsilon Decay\n",
    "    ax3 = fig.add_subplot(2, 3, 3)\n",
    "    ax3.plot(epsilons, color='purple', linewidth=2)\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Epsilon (ε)')\n",
    "    ax3.set_title('Exploration Rate Decay')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Plot 4: Q-Value Heatmap (max Q per state)\n",
    "    ax4 = fig.add_subplot(2, 3, 4)\n",
    "    max_Q = np.max(agent.Q, axis=1).reshape(env.size, env.size)\n",
    "    im = ax4.imshow(max_Q, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(im, ax=ax4, label='Max Q-Value')\n",
    "    \n",
    "    # Mark special cells\n",
    "    for wall in env.walls:\n",
    "        ax4.add_patch(Rectangle((wall[1]-0.5, wall[0]-0.5), 1, 1, \n",
    "                                fill=True, color='black', alpha=0.7))\n",
    "    for trap in env.traps:\n",
    "        ax4.add_patch(Rectangle((trap[1]-0.5, trap[0]-0.5), 1, 1, \n",
    "                                fill=True, color='red', alpha=0.3))\n",
    "    \n",
    "    ax4.plot(env.start[1], env.start[0], 'go', markersize=15, label='Start')\n",
    "    ax4.plot(env.goal[1], env.goal[0], 'y*', markersize=20, label='Goal')\n",
    "    ax4.set_title('Learned State Values (max Q)')\n",
    "    ax4.set_xlabel('Column')\n",
    "    ax4.set_ylabel('Row')\n",
    "    ax4.legend(loc='upper left', fontsize=8)\n",
    "    \n",
    "    # Plot 5: Optimal Policy Visualization\n",
    "    ax5 = fig.add_subplot(2, 3, 5)\n",
    "    \n",
    "    # Create grid\n",
    "    for i in range(env.size + 1):\n",
    "        ax5.axhline(y=i, color='black', linewidth=0.5)\n",
    "        ax5.axvline(x=i, color='black', linewidth=0.5)\n",
    "    \n",
    "    # Draw cells and policy arrows\n",
    "    arrow_params = {\n",
    "        0: (0, 0.3),   # up\n",
    "        1: (0, -0.3),  # down\n",
    "        2: (-0.3, 0),  # left\n",
    "        3: (0.3, 0)    # right\n",
    "    }\n",
    "    \n",
    "    for state in range(env.n_states):\n",
    "        row, col = env._state_to_pos(state)\n",
    "        pos = (col + 0.5, env.size - row - 0.5)\n",
    "        \n",
    "        # Color cells\n",
    "        if (row, col) in env.walls:\n",
    "            ax5.add_patch(Rectangle((col, env.size - row - 1), 1, 1, \n",
    "                                   fill=True, color='black'))\n",
    "        elif (row, col) in env.traps:\n",
    "            ax5.add_patch(Rectangle((col, env.size - row - 1), 1, 1, \n",
    "                                   fill=True, color='red', alpha=0.3))\n",
    "        elif (row, col) == env.goal:\n",
    "            ax5.add_patch(Rectangle((col, env.size - row - 1), 1, 1, \n",
    "                                   fill=True, color='gold', alpha=0.5))\n",
    "        elif (row, col) == env.start:\n",
    "            ax5.add_patch(Rectangle((col, env.size - row - 1), 1, 1, \n",
    "                                   fill=True, color='green', alpha=0.3))\n",
    "        \n",
    "        # Draw policy arrow (skip walls and goal)\n",
    "        if (row, col) not in env.walls and (row, col) != env.goal:\n",
    "            best_action = np.argmax(agent.Q[state])\n",
    "            dx, dy = arrow_params[best_action]\n",
    "            ax5.arrow(pos[0], pos[1], dx, dy, head_width=0.15, \n",
    "                     head_length=0.1, fc='blue', ec='blue')\n",
    "    \n",
    "    ax5.set_xlim(0, env.size)\n",
    "    ax5.set_ylim(0, env.size)\n",
    "    ax5.set_aspect('equal')\n",
    "    ax5.set_title('Learned Optimal Policy')\n",
    "    ax5.set_xlabel('Column')\n",
    "    ax5.set_ylabel('Row')\n",
    "    \n",
    "    # Plot 6: TD Error Convergence\n",
    "    ax6 = fig.add_subplot(2, 3, 6)\n",
    "    td_window = 100\n",
    "    td_errors = agent.td_errors\n",
    "    if len(td_errors) > td_window:\n",
    "        smoothed_td = np.convolve(td_errors, np.ones(td_window)/td_window, mode='valid')\n",
    "        ax6.plot(smoothed_td, color='orange', linewidth=1)\n",
    "    else:\n",
    "        ax6.plot(td_errors, color='orange', linewidth=1)\n",
    "    ax6.set_xlabel('Update Step')\n",
    "    ax6.set_ylabel('|TD Error|')\n",
    "    ax6.set_title('TD Error Convergence')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    ax6.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nFigure saved to 'plot.png'\")\n",
    "\n",
    "# Generate visualization\n",
    "plot_results(rewards, steps, epsilons, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, agent, n_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate the learned policy without exploration (epsilon=0).\n",
    "    \"\"\"\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0  # Pure exploitation\n",
    "    \n",
    "    total_rewards = []\n",
    "    total_steps = []\n",
    "    successes = 0\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(100):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                successes += 1\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        total_steps.append(step + 1)\n",
    "    \n",
    "    agent.epsilon = original_epsilon  # Restore epsilon\n",
    "    \n",
    "    print(\"Policy Evaluation Results (100 episodes):\")\n",
    "    print(f\"  Success Rate: {successes}%\")\n",
    "    print(f\"  Average Reward: {np.mean(total_rewards):.2f} ± {np.std(total_rewards):.2f}\")\n",
    "    print(f\"  Average Steps: {np.mean(total_steps):.2f} ± {np.std(total_steps):.2f}\")\n",
    "    print(f\"  Best Reward: {max(total_rewards):.2f}\")\n",
    "    print(f\"  Worst Reward: {min(total_rewards):.2f}\")\n",
    "\n",
    "# Evaluate the trained policy\n",
    "evaluate_policy(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display the Learned Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_q_table(agent, env):\n",
    "    \"\"\"\n",
    "    Display the Q-table in a readable format.\n",
    "    \"\"\"\n",
    "    print(\"Learned Q-Table (Q-values for each state-action pair):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'State':^10} {'Position':^10} {'↑':^10} {'↓':^10} {'←':^10} {'→':^10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for state in range(env.n_states):\n",
    "        pos = env._state_to_pos(state)\n",
    "        if pos in env.walls:\n",
    "            continue\n",
    "        \n",
    "        q_values = agent.Q[state]\n",
    "        best_action = np.argmax(q_values)\n",
    "        \n",
    "        row = f\"{state:^10} {str(pos):^10}\"\n",
    "        for i, q in enumerate(q_values):\n",
    "            if i == best_action:\n",
    "                row += f\" {q:>8.2f}*\"\n",
    "            else:\n",
    "                row += f\" {q:>9.2f}\"\n",
    "        print(row)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"* indicates the best action for each state\")\n",
    "\n",
    "display_q_table(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trace Optimal Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_optimal_path(env, agent):\n",
    "    \"\"\"\n",
    "    Trace the optimal path from start to goal using the learned policy.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    path = [env._state_to_pos(state)]\n",
    "    actions_taken = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(\"\\nOptimal Path from Start to Goal:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for step in range(20):  # Max steps to prevent infinite loops\n",
    "        action = np.argmax(agent.Q[state])\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        pos = env._state_to_pos(state)\n",
    "        next_pos = env._state_to_pos(next_state)\n",
    "        \n",
    "        print(f\"Step {step + 1}: {pos} → {env.action_names[action]} → {next_pos} (reward: {reward})\")\n",
    "        \n",
    "        actions_taken.append(env.action_names[action])\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            path.append(next_pos)\n",
    "            break\n",
    "        \n",
    "        path.append(next_pos)\n",
    "        state = next_state\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Total steps: {len(actions_taken)}\")\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "    print(f\"Path: {' → '.join([str(p) for p in path])}\")\n",
    "    \n",
    "    return path, actions_taken\n",
    "\n",
    "path, actions = trace_optimal_path(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### Algorithm Properties\n",
    "\n",
    "Q-learning is:\n",
    "- **Model-free**: Does not require knowledge of transition probabilities $P(s'|s,a)$\n",
    "- **Off-policy**: Learns about the greedy policy while following an exploratory policy\n",
    "- **Guaranteed to converge**: Under appropriate conditions (sufficient exploration, decaying learning rate)\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Exploration-Exploitation Trade-off**: The $\\epsilon$-greedy policy balances trying new actions vs. exploiting known good actions\n",
    "\n",
    "2. **Temporal Credit Assignment**: The discount factor $\\gamma$ determines how far into the future rewards influence current decisions\n",
    "\n",
    "3. **Convergence**: TD error decreases over time as the Q-values approach their optimal values\n",
    "\n",
    "4. **Policy Extraction**: The optimal policy is obtained by taking $\\arg\\max_a Q^*(s, a)$ for each state\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Tabular representation**: Does not scale to large or continuous state spaces\n",
    "- **Sample inefficiency**: Requires many episodes to converge\n",
    "- **Function approximation**: Deep Q-Networks (DQN) extend Q-learning to handle larger state spaces\n",
    "\n",
    "### Extensions\n",
    "\n",
    "Modern variants include:\n",
    "- **Double Q-Learning**: Reduces overestimation bias\n",
    "- **Prioritized Experience Replay**: Focuses on important transitions\n",
    "- **Dueling DQN**: Separates state-value and advantage estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
