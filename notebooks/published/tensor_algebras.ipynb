{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Algebras: A Computational Introduction\n",
    "\n",
    "## 1. Mathematical Foundations\n",
    "\n",
    "### 1.1 Definition of Tensors\n",
    "\n",
    "A **tensor** is a multilinear map that generalizes scalars, vectors, and matrices to higher dimensions. Given a vector space $V$ over a field $\\mathbb{F}$ (typically $\\mathbb{R}$ or $\\mathbb{C}$), a tensor of type $(r, s)$ is an element of the tensor product space:\n",
    "\n",
    "$$T \\in \\underbrace{V \\otimes V \\otimes \\cdots \\otimes V}_{r \\text{ copies}} \\otimes \\underbrace{V^* \\otimes V^* \\otimes \\cdots \\otimes V^*}_{s \\text{ copies}}$$\n",
    "\n",
    "where $V^*$ denotes the dual space of $V$.\n",
    "\n",
    "### 1.2 Tensor Product\n",
    "\n",
    "The **tensor product** $\\otimes$ of two vectors $u \\in V$ and $v \\in W$ creates a bilinear object $u \\otimes v \\in V \\otimes W$ satisfying:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(u_1 + u_2) \\otimes v &= u_1 \\otimes v + u_2 \\otimes v \\\\\n",
    "u \\otimes (v_1 + v_2) &= u \\otimes v_1 + u \\otimes v_2 \\\\\n",
    "(\\alpha u) \\otimes v &= u \\otimes (\\alpha v) = \\alpha (u \\otimes v)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In component form, if $u = u^i e_i$ and $v = v^j f_j$, then:\n",
    "\n",
    "$$u \\otimes v = u^i v^j (e_i \\otimes f_j)$$\n",
    "\n",
    "### 1.3 The Tensor Algebra\n",
    "\n",
    "The **tensor algebra** $T(V)$ over a vector space $V$ is the direct sum of all tensor powers:\n",
    "\n",
    "$$T(V) = \\bigoplus_{k=0}^{\\infty} V^{\\otimes k} = \\mathbb{F} \\oplus V \\oplus (V \\otimes V) \\oplus (V \\otimes V \\otimes V) \\oplus \\cdots$$\n",
    "\n",
    "where $V^{\\otimes 0} = \\mathbb{F}$ (the base field) and $V^{\\otimes 1} = V$.\n",
    "\n",
    "The tensor algebra forms an **associative algebra** with multiplication given by the tensor product:\n",
    "\n",
    "$$\\cdot : T(V) \\times T(V) \\to T(V), \\quad (a, b) \\mapsto a \\otimes b$$\n",
    "\n",
    "### 1.4 Index Notation and Einstein Summation\n",
    "\n",
    "A tensor $T$ of type $(r, s)$ has components $T^{i_1 \\cdots i_r}_{j_1 \\cdots j_s}$. Under a change of basis $e'_i = A^j_i e_j$, the components transform as:\n",
    "\n",
    "$$T'^{i_1 \\cdots i_r}_{j_1 \\cdots j_s} = A^{i_1}_{k_1} \\cdots A^{i_r}_{k_r} (A^{-1})^{l_1}_{j_1} \\cdots (A^{-1})^{l_s}_{j_s} T^{k_1 \\cdots k_r}_{l_1 \\cdots l_s}$$\n",
    "\n",
    "**Einstein summation convention**: Repeated indices (one upper, one lower) imply summation.\n",
    "\n",
    "### 1.5 Tensor Contraction\n",
    "\n",
    "**Contraction** reduces the rank of a tensor by summing over a pair of indices:\n",
    "\n",
    "$$C^i_j(T^{abc}_{def}) = T^{abi}_{dif} = \\sum_{i} T^{abi}_{dif}$$\n",
    "\n",
    "For a matrix (type (1,1) tensor), contraction yields the trace:\n",
    "\n",
    "$$\\text{Tr}(A) = A^i_i = \\sum_i A^i_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computational Implementation\n",
    "\n",
    "We now implement tensor algebra operations using NumPy, demonstrating the mathematical concepts computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Tensor Algebras: Computational Demonstrations\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tensor Product Implementation\n",
    "\n",
    "The outer product of vectors creates a rank-2 tensor. More generally, `np.tensordot` and `np.einsum` handle arbitrary tensor products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_product(a, b):\n",
    "    \"\"\"\n",
    "    Compute the tensor product of two arrays.\n",
    "    The result has shape a.shape + b.shape.\n",
    "    \"\"\"\n",
    "    return np.tensordot(a, b, axes=0)\n",
    "\n",
    "# Example: Tensor product of two vectors\n",
    "u = np.array([1, 2, 3])\n",
    "v = np.array([4, 5])\n",
    "\n",
    "T_uv = tensor_product(u, v)\n",
    "print(\"Vector u:\", u)\n",
    "print(\"Vector v:\", v)\n",
    "print(\"\\nTensor product u ⊗ v:\")\n",
    "print(T_uv)\n",
    "print(f\"Shape: {T_uv.shape} (rank-2 tensor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify bilinearity property: (αu) ⊗ v = α(u ⊗ v)\n",
    "alpha = 2.5\n",
    "lhs = tensor_product(alpha * u, v)\n",
    "rhs = alpha * tensor_product(u, v)\n",
    "\n",
    "print(\"Bilinearity verification:\")\n",
    "print(f\"(αu) ⊗ v = α(u ⊗ v): {np.allclose(lhs, rhs)}\")\n",
    "\n",
    "# Verify distributivity: (u₁ + u₂) ⊗ v = u₁ ⊗ v + u₂ ⊗ v\n",
    "u1 = np.array([1, 0, 1])\n",
    "u2 = np.array([0, 2, 2])\n",
    "lhs = tensor_product(u1 + u2, v)\n",
    "rhs = tensor_product(u1, v) + tensor_product(u2, v)\n",
    "print(f\"(u₁ + u₂) ⊗ v = u₁ ⊗ v + u₂ ⊗ v: {np.allclose(lhs, rhs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Higher-Order Tensors\n",
    "\n",
    "We can construct tensors of arbitrary rank through iterated tensor products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rank-3 tensor from three vectors\n",
    "a = np.array([1, 2])\n",
    "b = np.array([3, 4])\n",
    "c = np.array([5, 6])\n",
    "\n",
    "# T = a ⊗ b ⊗ c\n",
    "T_abc = tensor_product(tensor_product(a, b), c)\n",
    "print(f\"Rank-3 tensor a ⊗ b ⊗ c has shape: {T_abc.shape}\")\n",
    "print(f\"Total number of components: {T_abc.size}\")\n",
    "print(\"\\nComponents T_ijk:\")\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            print(f\"  T[{i},{j},{k}] = {T_abc[i,j,k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tensor Contraction\n",
    "\n",
    "Contraction sums over paired indices, reducing the tensor rank. Using `np.einsum` provides elegant notation matching index notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rank-2 tensor (matrix)\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Contraction (trace): sum over diagonal = A^i_i\n",
    "trace_einsum = np.einsum('ii->', A)\n",
    "trace_numpy = np.trace(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"\\nTrace via einsum (A^i_i): {trace_einsum}\")\n",
    "print(f\"Trace via np.trace: {trace_numpy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contraction of a rank-4 tensor\n",
    "# Create T^{ij}_{kl} and contract over j and k to get T^i_l\n",
    "n = 3\n",
    "T4 = np.random.randn(n, n, n, n)  # T^{ij}_{kl}\n",
    "\n",
    "# Contract: sum over j=k -> T^i_l = T^{ij}_{jl}\n",
    "T2_contracted = np.einsum('ijjl->il', T4)\n",
    "\n",
    "print(f\"Original rank-4 tensor shape: {T4.shape}\")\n",
    "print(f\"After contraction (j=k), rank-2 tensor shape: {T2_contracted.shape}\")\n",
    "print(\"\\nContracted tensor T^i_l:\")\n",
    "print(T2_contracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Tensor Transformations Under Change of Basis\n",
    "\n",
    "A fundamental property of tensors is their transformation law under change of basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_vector(v, A):\n",
    "    \"\"\"Transform contravariant vector: v'^i = A^i_j v^j\"\"\"\n",
    "    return np.einsum('ij,j->i', A, v)\n",
    "\n",
    "def transform_covector(w, A):\n",
    "    \"\"\"Transform covariant vector: w'_i = (A^{-1})^j_i w_j\"\"\"\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    return np.einsum('ji,j->i', A_inv, w)\n",
    "\n",
    "def transform_tensor_11(T, A):\n",
    "    \"\"\"Transform type (1,1) tensor: T'^i_j = A^i_k (A^{-1})^l_j T^k_l\"\"\"\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    return np.einsum('ik,lj,kl->ij', A, A_inv, T)\n",
    "\n",
    "# Define a rotation matrix (orthogonal transformation)\n",
    "theta = np.pi / 4  # 45 degrees\n",
    "R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "              [np.sin(theta),  np.cos(theta)]])\n",
    "\n",
    "# Original vector and tensor\n",
    "v_orig = np.array([1, 0])\n",
    "T_orig = np.array([[2, 1],\n",
    "                   [0, 3]])\n",
    "\n",
    "# Transform\n",
    "v_new = transform_vector(v_orig, R)\n",
    "T_new = transform_tensor_11(T_orig, R)\n",
    "\n",
    "print(\"Rotation by 45°:\")\n",
    "print(f\"Original vector: {v_orig}\")\n",
    "print(f\"Transformed vector: {v_new}\")\n",
    "print(f\"\\nOriginal tensor:\\n{T_orig}\")\n",
    "print(f\"Transformed tensor:\\n{T_new}\")\n",
    "\n",
    "# Verify trace is invariant\n",
    "print(f\"\\nTrace invariance: {np.trace(T_orig):.6f} = {np.trace(T_new):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Metric Tensor and Index Raising/Lowering\n",
    "\n",
    "The **metric tensor** $g_{ij}$ allows conversion between contravariant and covariant indices:\n",
    "\n",
    "$$v_i = g_{ij} v^j \\quad \\text{(lowering)}$$\n",
    "$$v^i = g^{ij} v_j \\quad \\text{(raising)}$$\n",
    "\n",
    "where $g^{ij}$ is the inverse metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a non-trivial metric (e.g., for oblique coordinates)\n",
    "# g_ij corresponds to basis vectors at 60° angle\n",
    "angle = np.pi / 3  # 60 degrees\n",
    "g = np.array([[1, np.cos(angle)],\n",
    "              [np.cos(angle), 1]])\n",
    "\n",
    "g_inv = np.linalg.inv(g)  # Inverse metric g^{ij}\n",
    "\n",
    "print(\"Metric tensor g_ij:\")\n",
    "print(g)\n",
    "print(\"\\nInverse metric g^ij:\")\n",
    "print(g_inv)\n",
    "\n",
    "# Verify g g^{-1} = δ\n",
    "print(f\"\\ng g^{{-1}} = identity: {np.allclose(g @ g_inv, np.eye(2))}\")\n",
    "\n",
    "# Index lowering: v_i = g_{ij} v^j\n",
    "v_up = np.array([1, 2])  # Contravariant components v^i\n",
    "v_down = np.einsum('ij,j->i', g, v_up)  # Covariant components v_i\n",
    "\n",
    "print(f\"\\nContravariant v^i: {v_up}\")\n",
    "print(f\"Covariant v_i (lowered): {v_down}\")\n",
    "\n",
    "# Verify raising brings it back\n",
    "v_raised = np.einsum('ij,j->i', g_inv, v_down)\n",
    "print(f\"Raised back v^i: {v_raised}\")\n",
    "print(f\"Roundtrip successful: {np.allclose(v_up, v_raised)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Symmetric and Antisymmetric Tensors\n",
    "\n",
    "Tensors can be decomposed into symmetric and antisymmetric parts:\n",
    "\n",
    "$$T_{(ij)} = \\frac{1}{2}(T_{ij} + T_{ji}) \\quad \\text{(symmetric)}$$\n",
    "$$T_{[ij]} = \\frac{1}{2}(T_{ij} - T_{ji}) \\quad \\text{(antisymmetric)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrize(T, axes=(0, 1)):\n",
    "    \"\"\"Symmetrize tensor over specified axes.\"\"\"\n",
    "    return 0.5 * (T + np.swapaxes(T, axes[0], axes[1]))\n",
    "\n",
    "def antisymmetrize(T, axes=(0, 1)):\n",
    "    \"\"\"Antisymmetrize tensor over specified axes.\"\"\"\n",
    "    return 0.5 * (T - np.swapaxes(T, axes[0], axes[1]))\n",
    "\n",
    "# Create a general rank-2 tensor\n",
    "T = np.array([[1, 4, 7],\n",
    "              [2, 5, 8],\n",
    "              [3, 6, 9]])\n",
    "\n",
    "T_sym = symmetrize(T)\n",
    "T_antisym = antisymmetrize(T)\n",
    "\n",
    "print(\"Original tensor T:\")\n",
    "print(T)\n",
    "print(\"\\nSymmetric part T_(ij):\")\n",
    "print(T_sym)\n",
    "print(\"\\nAntisymmetric part T_[ij]:\")\n",
    "print(T_antisym)\n",
    "print(f\"\\nVerify T = T_(ij) + T_[ij]: {np.allclose(T, T_sym + T_antisym)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 The Levi-Civita Symbol\n",
    "\n",
    "The **Levi-Civita symbol** $\\epsilon_{ijk...}$ is a totally antisymmetric tensor:\n",
    "\n",
    "$$\\epsilon_{ijk} = \\begin{cases} +1 & \\text{even permutation of } (1,2,3) \\\\ -1 & \\text{odd permutation} \\\\ 0 & \\text{repeated indices} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levi_civita(n):\n",
    "    \"\"\"Construct the n-dimensional Levi-Civita symbol.\"\"\"\n",
    "    shape = (n,) * n\n",
    "    eps = np.zeros(shape)\n",
    "    \n",
    "    # Iterate over all permutations\n",
    "    for perm in product(range(n), repeat=n):\n",
    "        if len(set(perm)) == n:  # No repeated indices\n",
    "            # Count inversions to determine sign\n",
    "            inversions = sum(1 for i in range(n) for j in range(i+1, n) if perm[i] > perm[j])\n",
    "            eps[perm] = (-1) ** inversions\n",
    "    \n",
    "    return eps\n",
    "\n",
    "# 3D Levi-Civita symbol\n",
    "eps3 = levi_civita(3)\n",
    "print(\"Levi-Civita symbol ε_ijk (3D):\")\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        for k in range(3):\n",
    "            if eps3[i,j,k] != 0:\n",
    "                print(f\"  ε[{i},{j},{k}] = {int(eps3[i,j,k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application: Cross product via Levi-Civita\n",
    "# (a × b)^i = ε^{ijk} a_j b_k\n",
    "\n",
    "a = np.array([1, 0, 0])\n",
    "b = np.array([0, 1, 0])\n",
    "\n",
    "cross_einsum = np.einsum('ijk,j,k->i', eps3, a, b)\n",
    "cross_numpy = np.cross(a, b)\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"a × b (via Levi-Civita): {cross_einsum}\")\n",
    "print(f\"a × b (via np.cross): {cross_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization: Tensor Algebra Structure\n",
    "\n",
    "We visualize the structure of tensor spaces and transformation properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Tensor product structure (dimension growth)\n",
    "ax1 = axes[0, 0]\n",
    "dims = [2, 3, 4, 5]\n",
    "ranks = range(0, 5)\n",
    "for d in dims:\n",
    "    tensor_dims = [d**r for r in ranks]\n",
    "    ax1.semilogy(ranks, tensor_dims, 'o-', label=f'dim(V) = {d}', markersize=8)\n",
    "ax1.set_xlabel('Tensor Rank k', fontsize=12)\n",
    "ax1.set_ylabel('dim(V⊗k)', fontsize=12)\n",
    "ax1.set_title('Dimension Growth in Tensor Algebra T(V)', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(ranks)\n",
    "\n",
    "# Plot 2: Tensor contraction network\n",
    "ax2 = axes[0, 1]\n",
    "# Visualize contraction as a heatmap showing correlation structure\n",
    "n = 10\n",
    "T = np.random.randn(n, n, n)\n",
    "# Contract over first and last index: T^i_j_i -> V_j\n",
    "contracted = np.einsum('iji->j', T)\n",
    "# Show the \"projection\" effect\n",
    "original_slices = T[:, :, 0]  # Slice of original\n",
    "im = ax2.imshow(original_slices, cmap='coolwarm', aspect='auto')\n",
    "ax2.set_title('Rank-3 Tensor Slice T[:,:,0]', fontsize=14)\n",
    "ax2.set_xlabel('Index j', fontsize=12)\n",
    "ax2.set_ylabel('Index i', fontsize=12)\n",
    "plt.colorbar(im, ax=ax2, label='Value')\n",
    "\n",
    "# Plot 3: Metric tensor geometry\n",
    "ax3 = axes[1, 0]\n",
    "# Visualize how metric transforms unit circle\n",
    "theta_vals = np.linspace(0, 2*np.pi, 100)\n",
    "circle = np.array([np.cos(theta_vals), np.sin(theta_vals)])\n",
    "\n",
    "# Different metrics\n",
    "metrics = {\n",
    "    'Euclidean': np.eye(2),\n",
    "    'Oblique (60°)': np.array([[1, 0.5], [0.5, 1]]),\n",
    "    'Scaled': np.array([[2, 0], [0, 0.5]]),\n",
    "    'Sheared': np.array([[1, 0.7], [0.7, 1]])\n",
    "}\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(metrics)))\n",
    "for (name, g), color in zip(metrics.items(), colors):\n",
    "    # Unit vectors under metric: |v|_g = 1 means g_ij v^i v^j = 1\n",
    "    # Transform circle by sqrt(g^{-1}) to get unit ellipse\n",
    "    g_inv = np.linalg.inv(g)\n",
    "    L = np.linalg.cholesky(g_inv)\n",
    "    ellipse = L @ circle\n",
    "    ax3.plot(ellipse[0], ellipse[1], '-', color=color, label=name, linewidth=2)\n",
    "\n",
    "ax3.set_xlim(-2.5, 2.5)\n",
    "ax3.set_ylim(-2.5, 2.5)\n",
    "ax3.set_aspect('equal')\n",
    "ax3.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax3.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax3.set_title('Unit \"Circles\" Under Different Metrics', fontsize=14)\n",
    "ax3.set_xlabel('x', fontsize=12)\n",
    "ax3.set_ylabel('y', fontsize=12)\n",
    "ax3.legend(loc='upper right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Invariants under transformation\n",
    "ax4 = axes[1, 1]\n",
    "# Show that trace and determinant are invariants under similarity transform\n",
    "n_trials = 50\n",
    "traces_orig = []\n",
    "traces_transformed = []\n",
    "dets_orig = []\n",
    "dets_transformed = []\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    # Random matrix\n",
    "    M = np.random.randn(3, 3)\n",
    "    # Random invertible transformation\n",
    "    P = np.random.randn(3, 3)\n",
    "    while np.abs(np.linalg.det(P)) < 0.1:\n",
    "        P = np.random.randn(3, 3)\n",
    "    \n",
    "    # Similarity transform: M' = P M P^{-1}\n",
    "    M_transformed = P @ M @ np.linalg.inv(P)\n",
    "    \n",
    "    traces_orig.append(np.trace(M))\n",
    "    traces_transformed.append(np.trace(M_transformed))\n",
    "    dets_orig.append(np.linalg.det(M))\n",
    "    dets_transformed.append(np.linalg.det(M_transformed))\n",
    "\n",
    "ax4.scatter(traces_orig, traces_transformed, alpha=0.6, label='Trace', s=50)\n",
    "ax4.scatter(dets_orig, dets_transformed, alpha=0.6, label='Determinant', s=50, marker='s')\n",
    "# Perfect correlation line\n",
    "lims = [-10, 10]\n",
    "ax4.plot(lims, lims, 'k--', alpha=0.5, label='y = x')\n",
    "ax4.set_xlim(lims)\n",
    "ax4.set_ylim(lims)\n",
    "ax4.set_xlabel('Original Value', fontsize=12)\n",
    "ax4.set_ylabel('Value After Similarity Transform', fontsize=12)\n",
    "ax4.set_title('Tensor Invariants: Trace & Determinant', fontsize=14)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "This notebook has demonstrated the fundamental concepts of **tensor algebras**:\n",
    "\n",
    "1. **Tensor Products**: Bilinear operations creating higher-rank tensors, with dimension growing as $d^k$ for rank-$k$ tensors over a $d$-dimensional space.\n",
    "\n",
    "2. **Contraction**: Index summation reducing tensor rank, generalizing the trace operation.\n",
    "\n",
    "3. **Transformation Laws**: Tensors transform covariantly/contravariantly under basis changes, preserving geometric meaning.\n",
    "\n",
    "4. **Metric Tensors**: Enable raising/lowering indices, defining inner products and geometry.\n",
    "\n",
    "5. **Symmetry Properties**: Tensors decompose into symmetric and antisymmetric parts; the Levi-Civita symbol is totally antisymmetric.\n",
    "\n",
    "6. **Invariants**: Quantities like trace and determinant remain unchanged under similarity transformations.\n",
    "\n",
    "### Applications\n",
    "\n",
    "Tensor algebras are foundational in:\n",
    "- **General Relativity**: Spacetime curvature described by the Riemann tensor $R^\\mu_{\\nu\\rho\\sigma}$\n",
    "- **Continuum Mechanics**: Stress and strain tensors $\\sigma_{ij}$, $\\epsilon_{ij}$\n",
    "- **Machine Learning**: Deep learning operations, tensor decompositions (CP, Tucker)\n",
    "- **Quantum Mechanics**: Multipartite quantum states live in tensor product spaces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
