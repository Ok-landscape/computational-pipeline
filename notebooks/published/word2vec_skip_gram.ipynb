{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Skip-Gram Model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Word2Vec is a family of neural network models designed to learn distributed representations of words from large text corpora. The **Skip-Gram** architecture, introduced by Mikolov et al. (2013), learns word embeddings by predicting context words given a target word.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "Given a sequence of training words $w_1, w_2, \\ldots, w_T$, the Skip-Gram model maximizes the average log probability:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} | w_t)$$\n",
    "\n",
    "where $c$ is the context window size and $\\theta$ represents all model parameters.\n",
    "\n",
    "### Softmax Probability\n",
    "\n",
    "The probability of observing context word $w_O$ given center word $w_I$ is defined using the softmax function:\n",
    "\n",
    "$$P(w_O | w_I) = \\frac{\\exp(\\mathbf{u}_{w_O}^\\top \\mathbf{v}_{w_I})}{\\sum_{w=1}^{V} \\exp(\\mathbf{u}_w^\\top \\mathbf{v}_{w_I})}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{v}_{w_I} \\in \\mathbb{R}^d$ is the input (center) embedding of word $w_I$\n",
    "- $\\mathbf{u}_{w_O} \\in \\mathbb{R}^d$ is the output (context) embedding of word $w_O$\n",
    "- $V$ is the vocabulary size\n",
    "- $d$ is the embedding dimension\n",
    "\n",
    "### Negative Sampling\n",
    "\n",
    "Computing the full softmax is expensive for large vocabularies. **Negative Sampling** approximates the objective by sampling $k$ negative examples:\n",
    "\n",
    "$$\\log \\sigma(\\mathbf{u}_{w_O}^\\top \\mathbf{v}_{w_I}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} \\left[ \\log \\sigma(-\\mathbf{u}_{w_i}^\\top \\mathbf{v}_{w_I}) \\right]$$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function and $P_n(w) \\propto U(w)^{3/4}$ is the noise distribution based on unigram frequencies.\n",
    "\n",
    "### Gradient Updates\n",
    "\n",
    "For the center word embedding $\\mathbf{v}_{w_I}$:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_{w_I}} = (\\sigma(\\mathbf{u}_{w_O}^\\top \\mathbf{v}_{w_I}) - 1) \\mathbf{u}_{w_O} + \\sum_{i=1}^{k} \\sigma(\\mathbf{u}_{w_i}^\\top \\mathbf{v}_{w_I}) \\mathbf{u}_{w_i}$$\n",
    "\n",
    "For context embeddings, similar gradients are computed with respect to $\\mathbf{u}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We create a synthetic corpus with semantic structure to demonstrate the Skip-Gram model's ability to learn meaningful word relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic corpus with semantic clusters\n",
    "corpus = [\n",
    "    \"the king rules the kingdom with power and wisdom\",\n",
    "    \"the queen rules the kingdom with grace and wisdom\",\n",
    "    \"the prince will become king of the kingdom\",\n",
    "    \"the princess will become queen of the kingdom\",\n",
    "    \"man and woman live in the village\",\n",
    "    \"boy and girl play in the village\",\n",
    "    \"the king and queen have a son and daughter\",\n",
    "    \"the man works while the woman cares for family\",\n",
    "    \"the boy grows to become a man\",\n",
    "    \"the girl grows to become a woman\",\n",
    "    \"power and wisdom define a good king\",\n",
    "    \"grace and wisdom define a good queen\",\n",
    "    \"the kingdom prospers under wise rule\",\n",
    "    \"the village is part of the kingdom\",\n",
    "    \"son of king is prince of kingdom\",\n",
    "    \"daughter of queen is princess of kingdom\"\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
    "\n",
    "# Build vocabulary\n",
    "word_counts = Counter(word for sentence in tokenized_corpus for word in sentence)\n",
    "vocab = sorted(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample words: {vocab[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Training Pair Generation\n",
    "\n",
    "We generate (center, context) pairs using a sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_pairs(tokenized_corpus, word_to_idx, window_size=2):\n",
    "    \"\"\"Generate Skip-Gram training pairs (center_word, context_word).\"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for sentence in tokenized_corpus:\n",
    "        indices = [word_to_idx[word] for word in sentence]\n",
    "        \n",
    "        for center_pos, center_idx in enumerate(indices):\n",
    "            # Context window\n",
    "            for offset in range(-window_size, window_size + 1):\n",
    "                if offset == 0:\n",
    "                    continue\n",
    "                context_pos = center_pos + offset\n",
    "                \n",
    "                if 0 <= context_pos < len(indices):\n",
    "                    context_idx = indices[context_pos]\n",
    "                    pairs.append((center_idx, context_idx))\n",
    "    \n",
    "    return np.array(pairs)\n",
    "\n",
    "window_size = 2\n",
    "training_pairs = generate_training_pairs(tokenized_corpus, word_to_idx, window_size)\n",
    "print(f\"Number of training pairs: {len(training_pairs)}\")\n",
    "print(f\"Sample pairs: {[(idx_to_word[p[0]], idx_to_word[p[1]]) for p in training_pairs[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Model Implementation\n",
    "\n",
    "We implement the Skip-Gram model with negative sampling from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNegSampling:\n",
    "    \"\"\"Skip-Gram model with Negative Sampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Initialize embeddings with small random values\n",
    "        # W_in: center word embeddings (V x d)\n",
    "        # W_out: context word embeddings (V x d)\n",
    "        self.W_in = np.random.uniform(-0.5/embedding_dim, 0.5/embedding_dim, \n",
    "                                       (vocab_size, embedding_dim))\n",
    "        self.W_out = np.random.uniform(-0.5/embedding_dim, 0.5/embedding_dim, \n",
    "                                        (vocab_size, embedding_dim))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Numerically stable sigmoid function.\"\"\"\n",
    "        return np.where(x >= 0, \n",
    "                        1 / (1 + np.exp(-x)), \n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "    \n",
    "    def train_pair(self, center_idx, context_idx, negative_indices):\n",
    "        \"\"\"Train on a single (center, context) pair with negative samples.\"\"\"\n",
    "        \n",
    "        # Get embeddings\n",
    "        v_center = self.W_in[center_idx]  # (d,)\n",
    "        u_context = self.W_out[context_idx]  # (d,)\n",
    "        u_negatives = self.W_out[negative_indices]  # (k, d)\n",
    "        \n",
    "        # Forward pass - positive sample\n",
    "        pos_score = np.dot(u_context, v_center)\n",
    "        pos_sigmoid = self.sigmoid(pos_score)\n",
    "        \n",
    "        # Forward pass - negative samples\n",
    "        neg_scores = np.dot(u_negatives, v_center)\n",
    "        neg_sigmoids = self.sigmoid(neg_scores)\n",
    "        \n",
    "        # Compute loss (negative log likelihood)\n",
    "        loss = -np.log(pos_sigmoid + 1e-10) - np.sum(np.log(1 - neg_sigmoids + 1e-10))\n",
    "        \n",
    "        # Gradients\n",
    "        # For positive context word\n",
    "        grad_u_pos = (pos_sigmoid - 1) * v_center\n",
    "        \n",
    "        # For negative words\n",
    "        grad_u_neg = np.outer(neg_sigmoids, v_center)\n",
    "        \n",
    "        # For center word\n",
    "        grad_v = (pos_sigmoid - 1) * u_context + np.dot(neg_sigmoids, u_negatives)\n",
    "        \n",
    "        # Update embeddings (SGD)\n",
    "        self.W_in[center_idx] -= self.lr * grad_v\n",
    "        self.W_out[context_idx] -= self.lr * grad_u_pos\n",
    "        self.W_out[negative_indices] -= self.lr * grad_u_neg\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"Return the learned word embeddings (input embeddings).\"\"\"\n",
    "        return self.W_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "We train the model using negative sampling, where negative samples are drawn from the unigram distribution raised to the 3/4 power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_sampling_distribution(word_counts, vocab, power=0.75):\n",
    "    \"\"\"Create sampling distribution P(w) âˆ U(w)^(3/4).\"\"\"\n",
    "    freqs = np.array([word_counts[word] for word in vocab], dtype=np.float64)\n",
    "    powered_freqs = freqs ** power\n",
    "    return powered_freqs / powered_freqs.sum()\n",
    "\n",
    "# Training parameters\n",
    "embedding_dim = 20\n",
    "num_epochs = 100\n",
    "num_negatives = 5\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Initialize model\n",
    "model = SkipGramNegSampling(vocab_size, embedding_dim, learning_rate)\n",
    "\n",
    "# Create negative sampling distribution\n",
    "neg_sample_dist = create_negative_sampling_distribution(word_counts, vocab)\n",
    "\n",
    "# Training\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Shuffle training pairs\n",
    "    np.random.shuffle(training_pairs)\n",
    "    \n",
    "    for center_idx, context_idx in training_pairs:\n",
    "        # Sample negative words (excluding center and context)\n",
    "        negative_indices = np.random.choice(\n",
    "            vocab_size, \n",
    "            size=num_negatives, \n",
    "            replace=False, \n",
    "            p=neg_sample_dist\n",
    "        )\n",
    "        \n",
    "        # Remove center and context from negatives if present\n",
    "        negative_indices = negative_indices[\n",
    "            (negative_indices != center_idx) & (negative_indices != context_idx)\n",
    "        ]\n",
    "        \n",
    "        if len(negative_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        loss = model.train_pair(center_idx, context_idx, negative_indices)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    avg_loss = epoch_loss / len(training_pairs)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Visualization\n",
    "\n",
    "We examine the learned embeddings by:\n",
    "1. Plotting the training loss curve\n",
    "2. Finding similar words using cosine similarity\n",
    "3. Visualizing embeddings in 2D using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-10)\n",
    "\n",
    "def find_similar_words(word, embeddings, word_to_idx, idx_to_word, top_k=5):\n",
    "    \"\"\"Find most similar words using cosine similarity.\"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        return []\n",
    "    \n",
    "    word_idx = word_to_idx[word]\n",
    "    word_vec = embeddings[word_idx]\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        if idx != word_idx:\n",
    "            sim = cosine_similarity(word_vec, embeddings[idx])\n",
    "            similarities.append((idx_to_word[idx], sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Get learned embeddings\n",
    "embeddings = model.get_embeddings()\n",
    "\n",
    "# Find similar words for key terms\n",
    "test_words = ['king', 'queen', 'man', 'woman', 'boy', 'girl']\n",
    "print(\"Word Similarities:\")\n",
    "print(\"=\" * 50)\n",
    "for word in test_words:\n",
    "    similar = find_similar_words(word, embeddings, word_to_idx, idx_to_word, top_k=3)\n",
    "    print(f\"{word}: {[(w, f'{s:.3f}') for w, s in similar]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "axes[0].plot(losses, 'b-', linewidth=1.5)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Average Loss', fontsize=12)\n",
    "axes[0].set_title('Skip-Gram Training Loss', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: t-SNE visualization of word embeddings\n",
    "# Select interesting words to visualize\n",
    "words_to_plot = ['king', 'queen', 'prince', 'princess', \n",
    "                 'man', 'woman', 'boy', 'girl',\n",
    "                 'kingdom', 'village', 'power', 'wisdom', 'grace']\n",
    "\n",
    "# Get indices and embeddings for selected words\n",
    "plot_indices = [word_to_idx[w] for w in words_to_plot if w in word_to_idx]\n",
    "plot_embeddings = embeddings[plot_indices]\n",
    "plot_words = [idx_to_word[idx] for idx in plot_indices]\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(plot_words)-1))\n",
    "embeddings_2d = tsne.fit_transform(plot_embeddings)\n",
    "\n",
    "# Color mapping by semantic category\n",
    "colors = []\n",
    "for word in plot_words:\n",
    "    if word in ['king', 'queen', 'prince', 'princess']:\n",
    "        colors.append('royalblue')\n",
    "    elif word in ['man', 'woman', 'boy', 'girl']:\n",
    "        colors.append('forestgreen')\n",
    "    else:\n",
    "        colors.append('coral')\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors, s=100, alpha=0.7)\n",
    "\n",
    "# Add word labels\n",
    "for i, word in enumerate(plot_words):\n",
    "    axes[1].annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                     xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "axes[1].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[1].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "axes[1].set_title('Word Embeddings Visualization (t-SNE)', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='royalblue', label='Royalty'),\n",
    "    Patch(facecolor='forestgreen', label='People'),\n",
    "    Patch(facecolor='coral', label='Concepts')\n",
    "]\n",
    "axes[1].legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy Testing\n",
    "\n",
    "A classic test for word embeddings is the analogy task: $\\mathbf{v}_{king} - \\mathbf{v}_{man} + \\mathbf{v}_{woman} \\approx \\mathbf{v}_{queen}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word_a, word_b, word_c, embeddings, word_to_idx, idx_to_word, top_k=3):\n",
    "    \"\"\"\n",
    "    Solve analogy: a is to b as c is to ?\n",
    "    Computes: embedding(b) - embedding(a) + embedding(c)\n",
    "    \"\"\"\n",
    "    if any(w not in word_to_idx for w in [word_a, word_b, word_c]):\n",
    "        return []\n",
    "    \n",
    "    vec_a = embeddings[word_to_idx[word_a]]\n",
    "    vec_b = embeddings[word_to_idx[word_b]]\n",
    "    vec_c = embeddings[word_to_idx[word_c]]\n",
    "    \n",
    "    # Analogy vector\n",
    "    result_vec = vec_b - vec_a + vec_c\n",
    "    \n",
    "    # Find closest words\n",
    "    exclude = {word_a, word_b, word_c}\n",
    "    similarities = []\n",
    "    \n",
    "    for idx in range(len(embeddings)):\n",
    "        word = idx_to_word[idx]\n",
    "        if word not in exclude:\n",
    "            sim = cosine_similarity(result_vec, embeddings[idx])\n",
    "            similarities.append((word, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test analogies\n",
    "print(\"Word Analogies:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "analogies = [\n",
    "    ('man', 'king', 'woman'),      # man:king :: woman:?\n",
    "    ('boy', 'man', 'girl'),        # boy:man :: girl:?\n",
    "    ('king', 'prince', 'queen'),   # king:prince :: queen:?\n",
    "]\n",
    "\n",
    "for a, b, c in analogies:\n",
    "    result = word_analogy(a, b, c, embeddings, word_to_idx, idx_to_word)\n",
    "    print(f\"{a}:{b} :: {c}:? -> {[(w, f'{s:.3f}') for w, s in result]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the Word2Vec Skip-Gram model with negative sampling:\n",
    "\n",
    "1. **Mathematical Foundation**: We derived the objective function, softmax probability, and negative sampling approximation\n",
    "\n",
    "2. **Implementation**: Built a complete Skip-Gram model from scratch using NumPy with proper gradient computation\n",
    "\n",
    "3. **Training**: Applied the model to a synthetic corpus with semantic structure\n",
    "\n",
    "4. **Evaluation**: Visualized the learned embeddings and tested word analogies\n",
    "\n",
    "The model successfully learns semantic relationships, clustering similar concepts (royalty, common people) and capturing analogical reasoning patterns like $\\text{man} : \\text{king} :: \\text{woman} : \\text{queen}$.\n",
    "\n",
    "### References\n",
    "\n",
    "- Mikolov, T., et al. (2013). \"Efficient Estimation of Word Representations in Vector Space.\" arXiv:1301.3781\n",
    "- Mikolov, T., et al. (2013). \"Distributed Representations of Words and Phrases and their Compositionality.\" NIPS 2013"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
