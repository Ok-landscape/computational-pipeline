{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Gradient Method\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **Conjugate Gradient (CG) method** is an iterative algorithm for solving systems of linear equations of the form:\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$$\n",
    "\n",
    "where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is a **symmetric positive definite (SPD)** matrix, $\\mathbf{b} \\in \\mathbb{R}^n$ is the known right-hand side vector, and $\\mathbf{x} \\in \\mathbb{R}^n$ is the unknown solution vector.\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### Quadratic Form Minimization\n",
    "\n",
    "Solving $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ is equivalent to minimizing the quadratic form:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T\\mathbf{A}\\mathbf{x} - \\mathbf{b}^T\\mathbf{x}$$\n",
    "\n",
    "The gradient of this function is:\n",
    "\n",
    "$$\\nabla f(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} - \\mathbf{b}$$\n",
    "\n",
    "At the minimum, $\\nabla f(\\mathbf{x}) = \\mathbf{0}$, which gives us $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$.\n",
    "\n",
    "### A-Conjugacy\n",
    "\n",
    "Two vectors $\\mathbf{p}_i$ and $\\mathbf{p}_j$ are said to be **A-conjugate** (or $\\mathbf{A}$-orthogonal) if:\n",
    "\n",
    "$$\\mathbf{p}_i^T \\mathbf{A} \\mathbf{p}_j = 0 \\quad \\text{for } i \\neq j$$\n",
    "\n",
    "This property ensures that each search direction is independent in the $\\mathbf{A}$-inner product space.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "Given an initial guess $\\mathbf{x}_0$, the CG algorithm proceeds as:\n",
    "\n",
    "1. **Initialize:**\n",
    "   $$\\mathbf{r}_0 = \\mathbf{b} - \\mathbf{A}\\mathbf{x}_0$$\n",
    "   $$\\mathbf{p}_0 = \\mathbf{r}_0$$\n",
    "\n",
    "2. **For** $k = 0, 1, 2, \\ldots$ **until convergence:**\n",
    "   \n",
    "   - Compute step length:\n",
    "     $$\\alpha_k = \\frac{\\mathbf{r}_k^T \\mathbf{r}_k}{\\mathbf{p}_k^T \\mathbf{A} \\mathbf{p}_k}$$\n",
    "   \n",
    "   - Update solution:\n",
    "     $$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$$\n",
    "   \n",
    "   - Update residual:\n",
    "     $$\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k \\mathbf{A} \\mathbf{p}_k$$\n",
    "   \n",
    "   - Compute conjugate direction coefficient:\n",
    "     $$\\beta_k = \\frac{\\mathbf{r}_{k+1}^T \\mathbf{r}_{k+1}}{\\mathbf{r}_k^T \\mathbf{r}_k}$$\n",
    "   \n",
    "   - Update search direction:\n",
    "     $$\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k$$\n",
    "\n",
    "### Convergence Properties\n",
    "\n",
    "For an SPD matrix $\\mathbf{A}$, the CG method converges in at most $n$ iterations (in exact arithmetic). The convergence rate depends on the **condition number** $\\kappa(\\mathbf{A}) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$:\n",
    "\n",
    "$$\\|\\mathbf{x}_k - \\mathbf{x}^*\\|_\\mathbf{A} \\leq 2\\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^k \\|\\mathbf{x}_0 - \\mathbf{x}^*\\|_\\mathbf{A}$$\n",
    "\n",
    "where $\\|\\mathbf{v}\\|_\\mathbf{A} = \\sqrt{\\mathbf{v}^T \\mathbf{A} \\mathbf{v}}$ is the $\\mathbf{A}$-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Conjugate Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b, x0=None, tol=1e-10, max_iter=None):\n",
    "    \"\"\"\n",
    "    Solve Ax = b using the Conjugate Gradient method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A : ndarray\n",
    "        Symmetric positive definite matrix (n x n)\n",
    "    b : ndarray\n",
    "        Right-hand side vector (n,)\n",
    "    x0 : ndarray, optional\n",
    "        Initial guess (n,). Defaults to zero vector.\n",
    "    tol : float, optional\n",
    "        Convergence tolerance for residual norm\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations. Defaults to n.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x : ndarray\n",
    "        Solution vector\n",
    "    residuals : list\n",
    "        Residual norms at each iteration\n",
    "    \"\"\"\n",
    "    n = len(b)\n",
    "    if x0 is None:\n",
    "        x = np.zeros(n)\n",
    "    else:\n",
    "        x = x0.copy()\n",
    "    \n",
    "    if max_iter is None:\n",
    "        max_iter = n\n",
    "    \n",
    "    # Initialize residual and search direction\n",
    "    r = b - A @ x\n",
    "    p = r.copy()\n",
    "    rs_old = r @ r\n",
    "    \n",
    "    residuals = [np.sqrt(rs_old)]\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        Ap = A @ p\n",
    "        \n",
    "        # Step length\n",
    "        alpha = rs_old / (p @ Ap)\n",
    "        \n",
    "        # Update solution\n",
    "        x = x + alpha * p\n",
    "        \n",
    "        # Update residual\n",
    "        r = r - alpha * Ap\n",
    "        \n",
    "        rs_new = r @ r\n",
    "        residuals.append(np.sqrt(rs_new))\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.sqrt(rs_new) < tol:\n",
    "            break\n",
    "        \n",
    "        # Conjugate direction coefficient\n",
    "        beta = rs_new / rs_old\n",
    "        \n",
    "        # Update search direction\n",
    "        p = r + beta * p\n",
    "        \n",
    "        rs_old = rs_new\n",
    "    \n",
    "    return x, residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Test Problem\n",
    "\n",
    "We create a symmetric positive definite matrix with controlled condition number to demonstrate convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spd_matrix(n, condition_number):\n",
    "    \"\"\"\n",
    "    Create a symmetric positive definite matrix with specified condition number.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n : int\n",
    "        Matrix dimension\n",
    "    condition_number : float\n",
    "        Desired condition number (ratio of largest to smallest eigenvalue)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    A : ndarray\n",
    "        SPD matrix with specified condition number\n",
    "    \"\"\"\n",
    "    # Generate random orthogonal matrix via QR decomposition\n",
    "    Q, _ = np.linalg.qr(np.random.randn(n, n))\n",
    "    \n",
    "    # Create eigenvalues distributed between 1 and condition_number\n",
    "    eigenvalues = np.linspace(1, condition_number, n)\n",
    "    \n",
    "    # Construct A = Q * D * Q^T\n",
    "    A = Q @ np.diag(eigenvalues) @ Q.T\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Problem parameters\n",
    "n = 100  # Matrix size\n",
    "condition_numbers = [10, 100, 1000]  # Different condition numbers to test\n",
    "\n",
    "# Create true solution and right-hand side\n",
    "x_true = np.random.randn(n)\n",
    "\n",
    "print(f\"Problem size: {n} x {n}\")\n",
    "print(f\"Testing condition numbers: {condition_numbers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Experiments\n",
    "\n",
    "### Effect of Condition Number on Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for different condition numbers\n",
    "results = {}\n",
    "\n",
    "for kappa in condition_numbers:\n",
    "    # Create SPD matrix with given condition number\n",
    "    A = create_spd_matrix(n, kappa)\n",
    "    b = A @ x_true\n",
    "    \n",
    "    # Solve using CG\n",
    "    x_cg, residuals = conjugate_gradient(A, b, tol=1e-12, max_iter=200)\n",
    "    \n",
    "    # Compute actual condition number\n",
    "    eigenvalues = eigh(A, eigvals_only=True)\n",
    "    actual_kappa = eigenvalues[-1] / eigenvalues[0]\n",
    "    \n",
    "    # Compute solution error\n",
    "    error = np.linalg.norm(x_cg - x_true) / np.linalg.norm(x_true)\n",
    "    \n",
    "    results[kappa] = {\n",
    "        'residuals': residuals,\n",
    "        'actual_kappa': actual_kappa,\n",
    "        'error': error,\n",
    "        'iterations': len(residuals) - 1\n",
    "    }\n",
    "    \n",
    "    print(f\"κ = {kappa}:\")\n",
    "    print(f\"  Iterations: {results[kappa]['iterations']}\")\n",
    "    print(f\"  Final residual: {residuals[-1]:.2e}\")\n",
    "    print(f\"  Relative error: {error:.2e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Residual convergence for different condition numbers\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "for i, kappa in enumerate(condition_numbers):\n",
    "    residuals = results[kappa]['residuals']\n",
    "    ax1.semilogy(range(len(residuals)), residuals, \n",
    "                 color=colors[i], linewidth=2, \n",
    "                 label=f'κ = {kappa}')\n",
    "\n",
    "ax1.set_xlabel('Iteration', fontsize=12)\n",
    "ax1.set_ylabel('Residual Norm $\\\\|\\\\mathbf{r}_k\\\\|$', fontsize=12)\n",
    "ax1.set_title('Convergence of Conjugate Gradient Method', fontsize=14)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Theoretical vs observed convergence bound\n",
    "ax2 = axes[0, 1]\n",
    "kappa = 100\n",
    "A = create_spd_matrix(n, kappa)\n",
    "b = A @ x_true\n",
    "x_cg, residuals = conjugate_gradient(A, b, tol=1e-14, max_iter=150)\n",
    "\n",
    "# Theoretical bound\n",
    "sqrt_kappa = np.sqrt(kappa)\n",
    "bound_factor = (sqrt_kappa - 1) / (sqrt_kappa + 1)\n",
    "iterations = np.arange(len(residuals))\n",
    "theoretical_bound = 2 * (bound_factor ** iterations) * residuals[0]\n",
    "\n",
    "ax2.semilogy(iterations, residuals, 'b-', linewidth=2, label='Observed')\n",
    "ax2.semilogy(iterations, theoretical_bound, 'r--', linewidth=2, label='Theoretical bound')\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Residual Norm', fontsize=12)\n",
    "ax2.set_title(f'Theoretical Bound vs Observed (κ = {kappa})', fontsize=14)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: 2D visualization of search directions (small problem)\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "# Simple 2D problem for visualization\n",
    "A_2d = np.array([[4, 1], [1, 3]])\n",
    "b_2d = np.array([1, 2])\n",
    "x_true_2d = np.linalg.solve(A_2d, b_2d)\n",
    "\n",
    "# Store trajectory\n",
    "x = np.array([0., 0.])\n",
    "trajectory = [x.copy()]\n",
    "\n",
    "r = b_2d - A_2d @ x\n",
    "p = r.copy()\n",
    "rs_old = r @ r\n",
    "\n",
    "for _ in range(2):\n",
    "    Ap = A_2d @ p\n",
    "    alpha = rs_old / (p @ Ap)\n",
    "    x = x + alpha * p\n",
    "    trajectory.append(x.copy())\n",
    "    r = r - alpha * Ap\n",
    "    rs_new = r @ r\n",
    "    if rs_new < 1e-14:\n",
    "        break\n",
    "    beta = rs_new / rs_old\n",
    "    p = r + beta * p\n",
    "    rs_old = rs_new\n",
    "\n",
    "trajectory = np.array(trajectory)\n",
    "\n",
    "# Plot contours of quadratic form\n",
    "x_range = np.linspace(-0.5, 0.5, 100)\n",
    "y_range = np.linspace(-0.2, 0.8, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(len(x_range)):\n",
    "    for j in range(len(y_range)):\n",
    "        vec = np.array([X[j, i], Y[j, i]])\n",
    "        Z[j, i] = 0.5 * vec @ A_2d @ vec - b_2d @ vec\n",
    "\n",
    "ax3.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "ax3.plot(trajectory[:, 0], trajectory[:, 1], 'ro-', markersize=8, linewidth=2, label='CG path')\n",
    "ax3.plot(x_true_2d[0], x_true_2d[1], 'g*', markersize=15, label='Solution')\n",
    "ax3.set_xlabel('$x_1$', fontsize=12)\n",
    "ax3.set_ylabel('$x_2$', fontsize=12)\n",
    "ax3.set_title('CG Trajectory in 2D', fontsize=14)\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Iterations vs condition number\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "test_kappas = [5, 10, 25, 50, 100, 250, 500, 1000]\n",
    "iterations_needed = []\n",
    "\n",
    "for kappa in test_kappas:\n",
    "    A = create_spd_matrix(n, kappa)\n",
    "    b = A @ x_true\n",
    "    _, residuals = conjugate_gradient(A, b, tol=1e-8, max_iter=500)\n",
    "    iterations_needed.append(len(residuals) - 1)\n",
    "\n",
    "# Theoretical scaling: O(sqrt(kappa))\n",
    "theoretical_scaling = np.sqrt(test_kappas)\n",
    "theoretical_scaling = theoretical_scaling * (iterations_needed[3] / theoretical_scaling[3])  # Normalize\n",
    "\n",
    "ax4.loglog(test_kappas, iterations_needed, 'bo-', markersize=8, linewidth=2, label='Observed iterations')\n",
    "ax4.loglog(test_kappas, theoretical_scaling, 'r--', linewidth=2, label=r'$O(\\sqrt{\\kappa})$ scaling')\n",
    "ax4.set_xlabel('Condition Number κ', fontsize=12)\n",
    "ax4.set_ylabel('Iterations to Convergence', fontsize=12)\n",
    "ax4.set_title('Iteration Scaling with Condition Number', fontsize=14)\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Direct Solver\n",
    "\n",
    "For large sparse systems, CG is significantly more efficient than direct methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create a larger test problem\n",
    "n_large = 500\n",
    "kappa_test = 100\n",
    "\n",
    "A_large = create_spd_matrix(n_large, kappa_test)\n",
    "x_true_large = np.random.randn(n_large)\n",
    "b_large = A_large @ x_true_large\n",
    "\n",
    "# Time direct solver\n",
    "start = time.time()\n",
    "x_direct = np.linalg.solve(A_large, b_large)\n",
    "time_direct = time.time() - start\n",
    "\n",
    "# Time CG solver\n",
    "start = time.time()\n",
    "x_cg, _ = conjugate_gradient(A_large, b_large, tol=1e-10)\n",
    "time_cg = time.time() - start\n",
    "\n",
    "print(f\"Problem size: {n_large} x {n_large}\")\n",
    "print(f\"Condition number: {kappa_test}\")\n",
    "print(f\"\\nDirect solver (LU):\")\n",
    "print(f\"  Time: {time_direct*1000:.2f} ms\")\n",
    "print(f\"  Error: {np.linalg.norm(x_direct - x_true_large) / np.linalg.norm(x_true_large):.2e}\")\n",
    "print(f\"\\nConjugate Gradient:\")\n",
    "print(f\"  Time: {time_cg*1000:.2f} ms\")\n",
    "print(f\"  Error: {np.linalg.norm(x_cg - x_true_large) / np.linalg.norm(x_true_large):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The Conjugate Gradient method is a powerful iterative algorithm for solving linear systems with symmetric positive definite matrices. Key observations:\n",
    "\n",
    "1. **Convergence Speed**: The number of iterations scales as $O(\\sqrt{\\kappa})$ where $\\kappa$ is the condition number.\n",
    "\n",
    "2. **Exact Solution**: In exact arithmetic, CG converges in at most $n$ iterations for an $n \\times n$ system.\n",
    "\n",
    "3. **A-Conjugate Directions**: The search directions are mutually $\\mathbf{A}$-orthogonal, ensuring optimal progress at each step.\n",
    "\n",
    "4. **Efficiency**: For large sparse systems, CG is significantly more efficient than direct methods like LU decomposition.\n",
    "\n",
    "5. **Preconditioning**: The method can be accelerated by preconditioning, which effectively reduces the condition number of the system.\n",
    "\n",
    "The CG method remains one of the most important algorithms in numerical linear algebra, particularly for large-scale problems arising in scientific computing, optimization, and machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
