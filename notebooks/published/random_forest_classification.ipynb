{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. This approach leverages the wisdom of crowds to achieve superior predictive performance and robustness compared to individual decision trees.\n",
    "\n",
    "### Decision Trees: The Building Blocks\n",
    "\n",
    "A decision tree partitions the feature space into rectangular regions by recursively splitting on feature values. At each node, the algorithm selects the feature and threshold that maximizes information gain or minimizes impurity.\n",
    "\n",
    "**Gini Impurity** measures the probability of incorrectly classifying a randomly chosen element:\n",
    "\n",
    "$$G(t) = 1 - \\sum_{k=1}^{K} p_k^2$$\n",
    "\n",
    "where $p_k$ is the proportion of class $k$ samples at node $t$, and $K$ is the total number of classes.\n",
    "\n",
    "**Entropy** (Information Gain criterion) quantifies the uncertainty in the class distribution:\n",
    "\n",
    "$$H(t) = -\\sum_{k=1}^{K} p_k \\log_2(p_k)$$\n",
    "\n",
    "The **information gain** from a split is:\n",
    "\n",
    "$$\\text{IG}(t, \\text{split}) = H(t) - \\sum_{i \\in \\{\\text{left}, \\text{right}\\}} \\frac{n_i}{n_t} H(i)$$\n",
    "\n",
    "### Ensemble Methods and Bagging\n",
    "\n",
    "Random Forest employs **Bootstrap Aggregating (Bagging)**:\n",
    "\n",
    "1. For each tree $b = 1, 2, \\ldots, B$, draw a bootstrap sample $\\mathcal{D}_b$ of size $n$ from the training data with replacement\n",
    "2. Grow a decision tree $T_b$ on $\\mathcal{D}_b$\n",
    "3. At each split, randomly select $m$ features from the $p$ available features (typically $m = \\sqrt{p}$ for classification)\n",
    "\n",
    "The final prediction for classification is determined by **majority voting**:\n",
    "\n",
    "$$\\hat{y} = \\text{mode}\\{T_1(\\mathbf{x}), T_2(\\mathbf{x}), \\ldots, T_B(\\mathbf{x})\\}$$\n",
    "\n",
    "### Variance Reduction\n",
    "\n",
    "The variance of the ensemble prediction is:\n",
    "\n",
    "$$\\text{Var}(\\bar{T}) = \\rho \\sigma^2 + \\frac{1-\\rho}{B}\\sigma^2$$\n",
    "\n",
    "where $\\rho$ is the average correlation between trees and $\\sigma^2$ is the variance of individual trees. By decorrelating trees through random feature selection, Random Forest reduces $\\rho$, thereby reducing overall variance.\n",
    "\n",
    "### Out-of-Bag (OOB) Error Estimation\n",
    "\n",
    "Since each bootstrap sample contains approximately $1 - e^{-1} \\approx 63.2\\%$ of unique observations, the remaining samples (out-of-bag) provide a natural validation set. The OOB error is an unbiased estimate of the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We generate a synthetic binary classification dataset with two features, consisting of two interleaving moon-shaped clusters. This non-linear decision boundary demonstrates the power of Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_moons(n_samples=500, noise=0.2):\n",
    "    \"\"\"Generate two interleaving half circles (moons).\"\"\"\n",
    "    n_samples_per_class = n_samples // 2\n",
    "    \n",
    "    # First moon (class 0)\n",
    "    theta1 = np.linspace(0, np.pi, n_samples_per_class)\n",
    "    x1 = np.cos(theta1)\n",
    "    y1 = np.sin(theta1)\n",
    "    \n",
    "    # Second moon (class 1) - shifted and flipped\n",
    "    theta2 = np.linspace(0, np.pi, n_samples_per_class)\n",
    "    x2 = 1 - np.cos(theta2)\n",
    "    y2 = 0.5 - np.sin(theta2)\n",
    "    \n",
    "    X = np.vstack([np.column_stack([x1, y1]), np.column_stack([x2, y2])])\n",
    "    y = np.hstack([np.zeros(n_samples_per_class), np.ones(n_samples_per_class)])\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    X += np.random.randn(*X.shape) * noise\n",
    "    \n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(len(y))\n",
    "    return X[idx], y[idx].astype(int)\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_moons(n_samples=500, noise=0.25)\n",
    "\n",
    "# Train-test split\n",
    "n_train = int(0.8 * len(y))\n",
    "indices = np.random.permutation(len(y))\n",
    "X_train, X_test = X[indices[:n_train]], X[indices[n_train:]]\n",
    "y_train, y_test = y[indices[:n_train]], y[indices[n_train:]]\n",
    "\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")\n",
    "print(f\"Class distribution (train): {dict(zip(*np.unique(y_train, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Implementation\n",
    "\n",
    "We implement a decision tree classifier from scratch using the Gini impurity criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    \"\"\"A node in the decision tree.\"\"\"\n",
    "    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_idx = feature_idx  # Index of feature to split on\n",
    "        self.threshold = threshold       # Threshold value for split\n",
    "        self.left = left                 # Left child (feature <= threshold)\n",
    "        self.right = right               # Right child (feature > threshold)\n",
    "        self.value = value               # Predicted class (for leaf nodes)\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"Decision Tree Classifier using Gini impurity.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=10, min_samples_split=2, max_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.root = None\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        \"\"\"Calculate Gini impurity.\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "    \n",
    "    def _best_split(self, X, y, feature_indices):\n",
    "        \"\"\"Find the best split for a node.\"\"\"\n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        parent_gini = self._gini(y)\n",
    "        n = len(y)\n",
    "        \n",
    "        for feature_idx in feature_indices:\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate weighted Gini impurity after split\n",
    "                n_left, n_right = np.sum(left_mask), np.sum(right_mask)\n",
    "                gini_left = self._gini(y[left_mask])\n",
    "                gini_right = self._gini(y[right_mask])\n",
    "                weighted_gini = (n_left * gini_left + n_right * gini_right) / n\n",
    "                \n",
    "                # Information gain\n",
    "                gain = parent_gini - weighted_gini\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold, best_gain\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively build the decision tree.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            n_classes == 1):\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return DecisionNode(value=leaf_value)\n",
    "        \n",
    "        # Random feature selection (for Random Forest)\n",
    "        if self.max_features is not None:\n",
    "            feature_indices = np.random.choice(n_features, \n",
    "                                               min(self.max_features, n_features), \n",
    "                                               replace=False)\n",
    "        else:\n",
    "            feature_indices = np.arange(n_features)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold, best_gain = self._best_split(X, y, feature_indices)\n",
    "        \n",
    "        if best_feature is None or best_gain <= 0:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return DecisionNode(value=leaf_value)\n",
    "        \n",
    "        # Split data and recurse\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        left_child = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_child = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return DecisionNode(feature_idx=best_feature, threshold=best_threshold,\n",
    "                           left=left_child, right=right_child)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the decision tree.\"\"\"\n",
    "        self.root = self._build_tree(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _predict_sample(self, x, node):\n",
    "        \"\"\"Predict class for a single sample.\"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self._predict_sample(x, node.left)\n",
    "        else:\n",
    "            return self._predict_sample(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for multiple samples.\"\"\"\n",
    "        return np.array([self._predict_sample(x, self.root) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Implementation\n",
    "\n",
    "The Random Forest aggregates predictions from multiple decision trees, each trained on a bootstrap sample with random feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    \"\"\"Random Forest Classifier using bootstrap aggregating.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, max_depth=10, min_samples_split=2, \n",
    "                 max_features='sqrt', oob_score=True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.oob_score = oob_score\n",
    "        self.trees = []\n",
    "        self.oob_indices = []\n",
    "        self.oob_score_ = None\n",
    "    \n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        \"\"\"Generate a bootstrap sample.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        oob_mask = np.ones(n_samples, dtype=bool)\n",
    "        oob_mask[np.unique(indices)] = False\n",
    "        return X[indices], y[indices], np.where(oob_mask)[0]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the Random Forest.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Determine max_features\n",
    "        if self.max_features == 'sqrt':\n",
    "            max_feat = int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            max_feat = int(np.log2(n_features))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            max_feat = self.max_features\n",
    "        else:\n",
    "            max_feat = n_features\n",
    "        \n",
    "        self.trees = []\n",
    "        self.oob_indices = []\n",
    "        \n",
    "        # Train each tree\n",
    "        for i in range(self.n_estimators):\n",
    "            X_boot, y_boot, oob_idx = self._bootstrap_sample(X, y)\n",
    "            \n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_features=max_feat\n",
    "            )\n",
    "            tree.fit(X_boot, y_boot)\n",
    "            \n",
    "            self.trees.append(tree)\n",
    "            self.oob_indices.append(oob_idx)\n",
    "        \n",
    "        # Calculate OOB score\n",
    "        if self.oob_score:\n",
    "            self._compute_oob_score(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compute_oob_score(self, X, y):\n",
    "        \"\"\"Compute Out-of-Bag score.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        oob_predictions = np.zeros((n_samples, 2))  # Count votes for each class\n",
    "        \n",
    "        for tree, oob_idx in zip(self.trees, self.oob_indices):\n",
    "            if len(oob_idx) > 0:\n",
    "                preds = tree.predict(X[oob_idx])\n",
    "                for i, idx in enumerate(oob_idx):\n",
    "                    oob_predictions[idx, preds[i]] += 1\n",
    "        \n",
    "        # Get samples with at least one OOB prediction\n",
    "        valid_samples = np.sum(oob_predictions, axis=1) > 0\n",
    "        final_predictions = np.argmax(oob_predictions[valid_samples], axis=1)\n",
    "        \n",
    "        self.oob_score_ = np.mean(final_predictions == y[valid_samples])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes using majority voting.\"\"\"\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Majority vote\n",
    "        return np.array([Counter(predictions[:, i]).most_common(1)[0][0] \n",
    "                        for i in range(X.shape[0])])\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        proba = np.zeros((X.shape[0], 2))\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            counts = np.bincount(predictions[:, i], minlength=2)\n",
    "            proba[i] = counts / self.n_estimators\n",
    "        \n",
    "        return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, max_features='sqrt', oob_score=True)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = rf.predict(X_train)\n",
    "y_pred_test = rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "train_acc = accuracy(y_train, y_pred_train)\n",
    "test_acc = accuracy(y_test, y_pred_test)\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"OOB Score: {rf.oob_score_:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN={cm[0,0]:3d}  FP={cm[0,1]:3d}\")\n",
    "print(f\"  FN={cm[1,0]:3d}  TP={cm[1,1]:3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We visualize the decision boundary learned by the Random Forest, along with the effect of ensemble size on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Decision Boundary\n",
    "ax1 = axes[0, 0]\n",
    "h = 0.02  # Step size in mesh\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = rf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "ax1.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "ax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='RdYlBu', \n",
    "            edgecolors='black', s=50, alpha=0.8)\n",
    "ax1.set_xlabel('Feature $x_1$', fontsize=12)\n",
    "ax1.set_ylabel('Feature $x_2$', fontsize=12)\n",
    "ax1.set_title(f'Random Forest Decision Boundary\\n(Test Accuracy: {test_acc:.3f})', fontsize=14)\n",
    "\n",
    "# 2. Prediction Probabilities\n",
    "ax2 = axes[0, 1]\n",
    "proba = rf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "proba = proba.reshape(xx.shape)\n",
    "\n",
    "contour = ax2.contourf(xx, yy, proba, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='RdYlBu', \n",
    "            edgecolors='black', s=50, alpha=0.8)\n",
    "plt.colorbar(contour, ax=ax2, label='P(Class 1)')\n",
    "ax2.set_xlabel('Feature $x_1$', fontsize=12)\n",
    "ax2.set_ylabel('Feature $x_2$', fontsize=12)\n",
    "ax2.set_title('Prediction Probability Landscape', fontsize=14)\n",
    "\n",
    "# 3. Effect of Number of Trees\n",
    "ax3 = axes[1, 0]\n",
    "n_trees_range = [1, 5, 10, 20, 50, 100, 150, 200]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "oob_scores = []\n",
    "\n",
    "for n_trees in n_trees_range:\n",
    "    rf_temp = RandomForestClassifier(n_estimators=n_trees, max_depth=10, oob_score=True)\n",
    "    rf_temp.fit(X_train, y_train)\n",
    "    train_scores.append(accuracy(y_train, rf_temp.predict(X_train)))\n",
    "    test_scores.append(accuracy(y_test, rf_temp.predict(X_test)))\n",
    "    oob_scores.append(rf_temp.oob_score_)\n",
    "\n",
    "ax3.plot(n_trees_range, train_scores, 'b-o', label='Training', linewidth=2, markersize=8)\n",
    "ax3.plot(n_trees_range, test_scores, 'r-s', label='Test', linewidth=2, markersize=8)\n",
    "ax3.plot(n_trees_range, oob_scores, 'g-^', label='OOB', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Number of Trees ($B$)', fontsize=12)\n",
    "ax3.set_ylabel('Accuracy', fontsize=12)\n",
    "ax3.set_title('Effect of Ensemble Size on Performance', fontsize=14)\n",
    "ax3.legend(loc='lower right', fontsize=11)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([0.75, 1.02])\n",
    "\n",
    "# 4. Single Tree vs Random Forest Comparison\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Train single decision tree\n",
    "single_tree = DecisionTreeClassifier(max_depth=10)\n",
    "single_tree.fit(X_train, y_train)\n",
    "Z_tree = single_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "# Plot side by side comparison using contours\n",
    "ax4.contour(xx, yy, Z_tree, levels=[0.5], colors='red', linewidths=2, linestyles='--')\n",
    "ax4.contour(xx, yy, Z, levels=[0.5], colors='blue', linewidths=2, linestyles='-')\n",
    "ax4.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='RdYlBu', \n",
    "            edgecolors='black', s=50, alpha=0.8)\n",
    "\n",
    "tree_acc = accuracy(y_test, single_tree.predict(X_test))\n",
    "ax4.set_xlabel('Feature $x_1$', fontsize=12)\n",
    "ax4.set_ylabel('Feature $x_2$', fontsize=12)\n",
    "ax4.set_title('Decision Boundary Comparison', fontsize=14)\n",
    "ax4.legend([f'Single Tree (Acc: {tree_acc:.3f})', \n",
    "            f'Random Forest (Acc: {test_acc:.3f})'], \n",
    "           loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Ensemble Advantage**: The Random Forest produces smoother decision boundaries compared to a single decision tree, reducing overfitting.\n",
    "\n",
    "2. **Convergence Behavior**: As the number of trees increases, both training and test accuracy stabilize, demonstrating that adding more trees beyond a certain point yields diminishing returns.\n",
    "\n",
    "3. **OOB Estimation**: The Out-of-Bag score provides a reliable estimate of generalization performance without requiring a separate validation set.\n",
    "\n",
    "4. **Probabilistic Predictions**: The probability landscape shows how confidence varies across the feature space, with higher uncertainty near the decision boundary.\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "- **Training**: $O(B \\cdot n \\cdot m \\cdot \\log n)$ where $B$ is the number of trees, $n$ is the number of samples, and $m$ is the number of features considered at each split\n",
    "- **Prediction**: $O(B \\cdot \\log n)$ per sample\n",
    "\n",
    "Random Forests are embarrassingly parallelâ€”each tree can be trained independently, making them highly scalable for large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
