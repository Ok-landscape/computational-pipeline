{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation from Scratch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Backpropagation is the cornerstone algorithm for training neural networks. It provides an efficient method to compute gradients of the loss function with respect to each weight in the network by applying the chain rule of calculus systematically from the output layer back to the input layer.\n",
    "\n",
    "This notebook implements a complete neural network from scratch, demonstrating the mathematical foundations and computational mechanics of backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "Consider a feedforward neural network with $L$ layers. For each layer $l$, we define:\n",
    "\n",
    "- $\\mathbf{W}^{(l)}$: Weight matrix connecting layer $l-1$ to layer $l$\n",
    "- $\\mathbf{b}^{(l)}$: Bias vector for layer $l$\n",
    "- $\\mathbf{z}^{(l)}$: Pre-activation (weighted input) at layer $l$\n",
    "- $\\mathbf{a}^{(l)}$: Activation (output) at layer $l$\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "The forward pass computes the network output through successive transformations:\n",
    "\n",
    "$$\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}$$\n",
    "\n",
    "$$\\mathbf{a}^{(l)} = \\sigma(\\mathbf{z}^{(l)})$$\n",
    "\n",
    "where $\\sigma$ is the activation function. For the input layer, $\\mathbf{a}^{(0)} = \\mathbf{x}$.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "**Sigmoid:**\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$$\n",
    "\n",
    "**ReLU:**\n",
    "$$\\text{ReLU}(z) = \\max(0, z), \\quad \\text{ReLU}'(z) = \\begin{cases} 1 & z > 0 \\\\ 0 & z \\leq 0 \\end{cases}$$\n",
    "\n",
    "**Tanh:**\n",
    "$$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}, \\quad \\tanh'(z) = 1 - \\tanh^2(z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "For binary classification, we use the binary cross-entropy loss:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]$$\n",
    "\n",
    "where $m$ is the number of training examples, $y^{(i)}$ is the true label, and $\\hat{y}^{(i)}$ is the predicted probability.\n",
    "\n",
    "### Backpropagation Algorithm\n",
    "\n",
    "The key insight of backpropagation is the recursive computation of error terms $\\boldsymbol{\\delta}^{(l)}$ for each layer.\n",
    "\n",
    "**Output Layer Error ($l = L$):**\n",
    "$$\\boldsymbol{\\delta}^{(L)} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(L)}} = \\mathbf{a}^{(L)} - \\mathbf{y}$$\n",
    "\n",
    "**Hidden Layer Error ($l < L$):**\n",
    "$$\\boldsymbol{\\delta}^{(l)} = \\left( (\\mathbf{W}^{(l+1)})^T \\boldsymbol{\\delta}^{(l+1)} \\right) \\odot \\sigma'(\\mathbf{z}^{(l)})$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "**Gradient Computation:**\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}} = \\frac{1}{m} \\boldsymbol{\\delta}^{(l)} (\\mathbf{a}^{(l-1)})^T$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(l)}} = \\frac{1}{m} \\sum_{i=1}^{m} \\boldsymbol{\\delta}^{(l,i)}$$\n",
    "\n",
    "### Parameter Update (Gradient Descent)\n",
    "\n",
    "$$\\mathbf{W}^{(l)} \\leftarrow \\mathbf{W}^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}}$$\n",
    "\n",
    "$$\\mathbf{b}^{(l)} \\leftarrow \\mathbf{b}^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(l)}}$$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We now implement a fully-connected neural network with configurable architecture to solve a nonlinear classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions and Their Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU function.\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"Tanh activation function.\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"Derivative of tanh function.\"\"\"\n",
    "    return 1 - np.tanh(z)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A fully-connected feedforward neural network implemented from scratch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    layer_sizes : list\n",
    "        Number of neurons in each layer, including input and output.\n",
    "    activation : str\n",
    "        Activation function for hidden layers ('sigmoid', 'relu', 'tanh').\n",
    "    learning_rate : float\n",
    "        Step size for gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='relu', learning_rate=0.01):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Set activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_derivative = tanh_derivative\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(1, self.num_layers):\n",
    "            # He initialization for ReLU, Xavier for others\n",
    "            if activation == 'relu':\n",
    "                scale = np.sqrt(2 / layer_sizes[i-1])\n",
    "            else:\n",
    "                scale = np.sqrt(1 / layer_sizes[i-1])\n",
    "            \n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i-1]) * scale\n",
    "            b = np.zeros((layer_sizes[i], 1))\n",
    "            \n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # Storage for intermediate values\n",
    "        self.z_values = []  # Pre-activations\n",
    "        self.a_values = []  # Activations\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_features, m)\n",
    "            Input data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Network output.\n",
    "        \"\"\"\n",
    "        self.z_values = []\n",
    "        self.a_values = [X]  # a^(0) = X\n",
    "        \n",
    "        a = X\n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = self.weights[i] @ a + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            \n",
    "            # Use sigmoid for output layer (binary classification)\n",
    "            if i == self.num_layers - 2:\n",
    "                a = sigmoid(z)\n",
    "            else:\n",
    "                a = self.activation(z)\n",
    "            \n",
    "            self.a_values.append(a)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : ndarray\n",
    "            True labels.\n",
    "        y_pred : ndarray\n",
    "            Predicted probabilities.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Loss value.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[1]\n",
    "        epsilon = 1e-15  # Prevent log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to compute gradients.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Input data.\n",
    "        y : ndarray\n",
    "            True labels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list, list\n",
    "            Gradients for weights and biases.\n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        dW = [None] * (self.num_layers - 1)\n",
    "        db = [None] * (self.num_layers - 1)\n",
    "        \n",
    "        # Output layer error\n",
    "        delta = self.a_values[-1] - y  # dL/dz for cross-entropy + sigmoid\n",
    "        \n",
    "        # Backpropagate through layers\n",
    "        for l in range(self.num_layers - 2, -1, -1):\n",
    "            # Compute gradients\n",
    "            dW[l] = (1/m) * (delta @ self.a_values[l].T)\n",
    "            db[l] = (1/m) * np.sum(delta, axis=1, keepdims=True)\n",
    "            \n",
    "            # Compute delta for previous layer (if not at input)\n",
    "            if l > 0:\n",
    "                delta = (self.weights[l].T @ delta) * self.activation_derivative(self.z_values[l-1])\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def update_parameters(self, dW, db):\n",
    "        \"\"\"\n",
    "        Update weights and biases using gradient descent.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dW : list\n",
    "            Weight gradients.\n",
    "        db : list\n",
    "            Bias gradients.\n",
    "        \"\"\"\n",
    "        for l in range(len(self.weights)):\n",
    "            self.weights[l] -= self.learning_rate * dW[l]\n",
    "            self.biases[l] -= self.learning_rate * db[l]\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Training data.\n",
    "        y : ndarray\n",
    "            Training labels.\n",
    "        epochs : int\n",
    "            Number of training iterations.\n",
    "        verbose : bool\n",
    "            Whether to print progress.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            predictions = (y_pred > 0.5).astype(int)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            self.accuracy_history.append(accuracy)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW, db = self.backward(X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(dW, db)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Input data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Nonlinear Dataset\n",
    "\n",
    "We create a spiral dataset that requires nonlinear decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral_data(n_samples=200, noise=0.2):\n",
    "    \"\"\"\n",
    "    Generate a 2D spiral dataset for binary classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples per class.\n",
    "    noise : float\n",
    "        Standard deviation of Gaussian noise.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray, shape (2, 2*n_samples)\n",
    "        Features.\n",
    "    y : ndarray, shape (1, 2*n_samples)\n",
    "        Labels.\n",
    "    \"\"\"\n",
    "    theta = np.sqrt(np.random.rand(n_samples)) * 2 * np.pi\n",
    "    \n",
    "    # Class 0: First spiral\n",
    "    r0 = 2 * theta + np.pi\n",
    "    x0 = r0 * np.cos(theta) + np.random.randn(n_samples) * noise\n",
    "    y0 = r0 * np.sin(theta) + np.random.randn(n_samples) * noise\n",
    "    \n",
    "    # Class 1: Second spiral (rotated by pi)\n",
    "    r1 = 2 * theta + np.pi\n",
    "    x1 = -r1 * np.cos(theta) + np.random.randn(n_samples) * noise\n",
    "    y1 = -r1 * np.sin(theta) + np.random.randn(n_samples) * noise\n",
    "    \n",
    "    # Combine\n",
    "    X = np.vstack([np.hstack([x0, x1]), np.hstack([y0, y1])])\n",
    "    y = np.hstack([np.zeros(n_samples), np.ones(n_samples)]).reshape(1, -1)\n",
    "    \n",
    "    # Normalize\n",
    "    X = (X - X.mean(axis=1, keepdims=True)) / X.std(axis=1, keepdims=True)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_spiral_data(n_samples=200, noise=0.3)\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Class distribution: {np.sum(y==0)} class 0, {np.sum(y==1)} class 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture: 2 input -> 16 hidden -> 8 hidden -> 1 output\n",
    "layer_sizes = [2, 16, 8, 1]\n",
    "\n",
    "# Create and train network\n",
    "nn = NeuralNetwork(\n",
    "    layer_sizes=layer_sizes,\n",
    "    activation='tanh',\n",
    "    learning_rate=0.5\n",
    ")\n",
    "\n",
    "print(\"Training Neural Network...\")\n",
    "print(f\"Architecture: {' -> '.join(map(str, layer_sizes))}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "nn.train(X, y, epochs=1000, verbose=True)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final Loss: {nn.loss_history[-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {nn.accuracy_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(nn, X, y, ax):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary learned by the network.\n",
    "    \"\"\"\n",
    "    # Create mesh grid\n",
    "    margin = 0.5\n",
    "    x_min, x_max = X[0, :].min() - margin, X[0, :].max() + margin\n",
    "    y_min, y_max = X[1, :].min() - margin, X[1, :].max() + margin\n",
    "    \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 200),\n",
    "        np.linspace(y_min, y_max, 200)\n",
    "    )\n",
    "    \n",
    "    # Predict on mesh\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()].T\n",
    "    Z = nn.forward(grid_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "    ax.contourf(xx, yy, Z, levels=[0, 0.5, 1], cmap=cmap_light, alpha=0.6)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='k', linewidths=2)\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = ax.scatter(\n",
    "        X[0, :], X[1, :],\n",
    "        c=y.ravel(),\n",
    "        cmap=ListedColormap(['#FF0000', '#0000FF']),\n",
    "        edgecolors='k',\n",
    "        s=40\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.set_title('Decision Boundary', fontsize=14)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Training data\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['#FF0000' if yi == 0 else '#0000FF' for yi in y.ravel()]\n",
    "ax1.scatter(X[0, :], X[1, :], c=colors, edgecolors='k', s=40)\n",
    "ax1.set_xlabel('$x_1$', fontsize=12)\n",
    "ax1.set_ylabel('$x_2$', fontsize=12)\n",
    "ax1.set_title('Spiral Dataset', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "ax2 = axes[0, 1]\n",
    "plot_decision_boundary(nn, X, y, ax2)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Loss curve\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(nn.loss_history, 'b-', linewidth=2)\n",
    "ax3.set_xlabel('Epoch', fontsize=12)\n",
    "ax3.set_ylabel('Loss', fontsize=12)\n",
    "ax3.set_title('Training Loss (Binary Cross-Entropy)', fontsize=14)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# Plot 4: Accuracy curve\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(nn.accuracy_history, 'g-', linewidth=2)\n",
    "ax4.set_xlabel('Epoch', fontsize=12)\n",
    "ax4.set_ylabel('Accuracy', fontsize=12)\n",
    "ax4.set_title('Training Accuracy', fontsize=14)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Gradient Flow\n",
    "\n",
    "Let's examine the gradient magnitudes during training to understand the dynamics of backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute current gradients for analysis\n",
    "_ = nn.forward(X)\n",
    "dW, db = nn.backward(X, y)\n",
    "\n",
    "print(\"Gradient Analysis\")\n",
    "print(\"=\" * 50)\n",
    "for i, (w, dw) in enumerate(zip(nn.weights, dW)):\n",
    "    print(f\"\\nLayer {i+1}:\")\n",
    "    print(f\"  Weight shape: {w.shape}\")\n",
    "    print(f\"  Weight magnitude: {np.linalg.norm(w):.4f}\")\n",
    "    print(f\"  Gradient magnitude: {np.linalg.norm(dw):.6f}\")\n",
    "    print(f\"  Gradient/Weight ratio: {np.linalg.norm(dw)/np.linalg.norm(w):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Mathematical foundations** of neural networks and backpropagation using the chain rule\n",
    "\n",
    "2. **Forward propagation**: Computing $\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}$ and $\\mathbf{a}^{(l)} = \\sigma(\\mathbf{z}^{(l)})$\n",
    "\n",
    "3. **Backpropagation**: Computing gradients via $\\boldsymbol{\\delta}^{(l)} = \\left( (\\mathbf{W}^{(l+1)})^T \\boldsymbol{\\delta}^{(l+1)} \\right) \\odot \\sigma'(\\mathbf{z}^{(l)})$\n",
    "\n",
    "4. **Gradient descent**: Updating parameters with $\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\nabla_W \\mathcal{L}$\n",
    "\n",
    "The network successfully learned to classify the nonlinear spiral dataset, demonstrating the power of multi-layer networks trained with backpropagation to learn complex decision boundaries.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Vanishing gradients**: Deeper layers may receive smaller gradient signals, especially with sigmoid/tanh activations\n",
    "- **Weight initialization**: Proper scaling (He/Xavier) prevents saturation and ensures healthy gradient flow\n",
    "- **Learning rate**: Too high causes divergence; too low causes slow convergence\n",
    "- **Architecture**: More neurons/layers can model more complex boundaries but risk overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
