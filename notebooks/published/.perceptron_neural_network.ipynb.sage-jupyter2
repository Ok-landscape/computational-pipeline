{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-f10230ce-d9f5-4950-9891-a62c85b3a8e5.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_backend_state":1763845657057,"last_ipynb_save":1763845680326,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1763845657083,"exec_count":3,"id":"1cfb80","input":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# Set random seed for reproducibility\nnp.random.seed(42)","kernel":"python3","pos":2,"start":1763845657071,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845657094,"exec_count":4,"id":"aea0fc","input":"class Perceptron:\n    \"\"\"\n    Perceptron classifier.\n    \n    Parameters\n    ----------\n    eta : float\n        Learning rate (between 0.0 and 1.0)\n    n_iter : int\n        Number of passes over the training dataset\n    random_state : int\n        Random number generator seed for weight initialization\n    \n    Attributes\n    ----------\n    w_ : 1d-array\n        Weights after fitting\n    b_ : float\n        Bias unit after fitting\n    errors_ : list\n        Number of misclassifications in each epoch\n    \"\"\"\n    \n    def __init__(self, eta=0.1, n_iter=50, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n    \n    def fit(self, X, y):\n        \"\"\"\n        Fit training data.\n        \n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vectors\n        y : array-like, shape = [n_samples]\n            Target values (0 or 1)\n        \n        Returns\n        -------\n        self : object\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n        self.b_ = 0.0\n        self.errors_ = []\n        \n        for _ in range(self.n_iter):\n            errors = 0\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_ += update * xi\n                self.b_ += update\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        \n        return self\n    \n    def net_input(self, X):\n        \"\"\"Calculate net input: z = w^T x + b\"\"\"\n        return np.dot(X, self.w_) + self.b_\n    \n    def predict(self, X):\n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.net_input(X) >= 0.0, 1, 0)","kernel":"python3","pos":3,"start":1763845657090,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845657104,"exec_count":5,"id":"14f69b","input":"# Generate linearly separable data\nn_samples = 100\n\n# Class 0: centered at (-2, -2)\nX_class0 = np.random.randn(n_samples // 2, 2) * 0.8 + np.array([-2, -2])\ny_class0 = np.zeros(n_samples // 2)\n\n# Class 1: centered at (2, 2)\nX_class1 = np.random.randn(n_samples // 2, 2) * 0.8 + np.array([2, 2])\ny_class1 = np.ones(n_samples // 2)\n\n# Combine the data\nX = np.vstack([X_class0, X_class1])\ny = np.hstack([y_class0, y_class1])\n\n# Shuffle the dataset\nshuffle_idx = np.random.permutation(n_samples)\nX = X[shuffle_idx]\ny = y[shuffle_idx]\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Class distribution: {np.bincount(y.astype(int))}\")","kernel":"python3","output":{"0":{"name":"stdout","text":"Dataset shape: (100, 2)\nClass distribution: [50 50]\n"}},"pos":5,"start":1763845657101,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845657169,"exec_count":6,"id":"0b7041","input":"# Initialize and train the perceptron\nppn = Perceptron(eta=0.1, n_iter=20)\nppn.fit(X, y)\n\nprint(f\"Learned weights: w1 = {ppn.w_[0]:.4f}, w2 = {ppn.w_[1]:.4f}\")\nprint(f\"Learned bias: b = {ppn.b_:.4f}\")\nprint(f\"Final accuracy: {np.mean(ppn.predict(X) == y) * 100:.1f}%\")","kernel":"python3","output":{"0":{"name":"stdout","text":"Learned weights: w1 = 0.0627, w2 = 0.1918\nLearned bias: b = 0.1000\nFinal accuracy: 100.0%\n"}},"pos":7,"start":1763845657150,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845657182,"exec_count":7,"id":"371496","input":"def plot_decision_regions(X, y, classifier, resolution=0.02):\n    \"\"\"\n    Plot decision regions for a 2D dataset.\n    \"\"\"\n    # Setup marker generator and color map\n    markers = ('o', 's')\n    colors = ('lightblue', 'lightcoral')\n    cmap = ListedColormap(colors)\n    \n    # Plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    \n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                          np.arange(x2_min, x2_max, resolution))\n    \n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    \n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    \n    # Plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                   y=X[y == cl, 1],\n                   alpha=0.8, \n                   c=[colors[idx]],\n                   marker=markers[idx], \n                   label=f'Class {int(cl)}',\n                   edgecolor='black')","kernel":"python3","pos":9,"start":1763845657178,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845658942,"exec_count":8,"id":"3ec5d1","input":"# Create the comprehensive visualization\nfig = plt.figure(figsize=(14, 5))\n\n# Plot 1: Decision boundary and regions\nax1 = fig.add_subplot(1, 3, 1)\nplot_decision_regions(X, y, ppn)\n\n# Plot the decision boundary line\nx_boundary = np.array([X[:, 0].min() - 1, X[:, 0].max() + 1])\ny_boundary = -(ppn.w_[0] * x_boundary + ppn.b_) / ppn.w_[1]\nax1.plot(x_boundary, y_boundary, 'k--', linewidth=2, label='Decision boundary')\n\nax1.set_xlabel('$x_1$', fontsize=12)\nax1.set_ylabel('$x_2$', fontsize=12)\nax1.set_title('Perceptron Decision Regions', fontsize=14)\nax1.legend(loc='upper left')\n\n# Plot 2: Convergence (errors per epoch)\nax2 = fig.add_subplot(1, 3, 2)\nax2.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o', \n         color='steelblue', linewidth=2, markersize=6)\nax2.set_xlabel('Epoch', fontsize=12)\nax2.set_ylabel('Number of Misclassifications', fontsize=12)\nax2.set_title('Perceptron Convergence', fontsize=14)\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Weight vector visualization\nax3 = fig.add_subplot(1, 3, 3)\n\n# Plot data points\nax3.scatter(X[y == 0, 0], X[y == 0, 1], c='lightblue', marker='o', \n           edgecolor='black', alpha=0.6, label='Class 0')\nax3.scatter(X[y == 1, 0], X[y == 1, 1], c='lightcoral', marker='s', \n           edgecolor='black', alpha=0.6, label='Class 1')\n\n# Plot the weight vector (perpendicular to decision boundary)\norigin = np.array([0, 0])\nweight_scale = 1.5\nax3.quiver(origin[0], origin[1], ppn.w_[0] * weight_scale, ppn.w_[1] * weight_scale,\n          angles='xy', scale_units='xy', scale=1, color='red', \n          width=0.02, label=f'Weight vector $\\\\mathbf{{w}}$')\n\n# Decision boundary\nax3.plot(x_boundary, y_boundary, 'k--', linewidth=2)\n\nax3.set_xlabel('$x_1$', fontsize=12)\nax3.set_ylabel('$x_2$', fontsize=12)\nax3.set_title('Weight Vector Visualization', fontsize=14)\nax3.set_xlim(X[:, 0].min() - 1, X[:, 0].max() + 1)\nax3.set_ylim(X[:, 1].min() - 1, X[:, 1].max() + 1)\nax3.legend(loc='upper left')\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\n\n# Save the plot\nplt.savefig('plot.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nPlot saved to 'plot.png'\")","kernel":"python3","output":{"0":{"data":{"image/png":"16ec403480dd88384ddb671a55b5a0713e3020ab","text/plain":"<Figure size 1400x500 with 3 Axes>"},"metadata":{"image/png":{"height":488,"width":1388}}},"1":{"name":"stdout","text":"\nPlot saved to 'plot.png'\n"}},"pos":10,"start":1763845657197,"state":"done","type":"cell"}
{"cell_type":"code","end":1763845658965,"exec_count":9,"id":"1fb098","input":"# XOR dataset\nX_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_xor = np.array([0, 1, 1, 0])  # XOR truth table\n\n# Train perceptron on XOR\nppn_xor = Perceptron(eta=0.1, n_iter=100)\nppn_xor.fit(X_xor, y_xor)\n\n# Check accuracy\npredictions = ppn_xor.predict(X_xor)\naccuracy = np.mean(predictions == y_xor) * 100\n\nprint(\"XOR Problem:\")\nprint(\"-\" * 30)\nfor i in range(len(X_xor)):\n    print(f\"Input: {X_xor[i]} | Target: {y_xor[i]} | Prediction: {predictions[i]}\")\nprint(\"-\" * 30)\nprint(f\"Accuracy: {accuracy:.1f}%\")\nprint(f\"\\nFinal errors per epoch: {ppn_xor.errors_[-5:]}\")\nprint(\"\\nThe perceptron fails to converge on the XOR problem,\")\nprint(\"demonstrating that it cannot learn non-linearly separable patterns.\")","kernel":"python3","output":{"0":{"name":"stdout","text":"XOR Problem:\n------------------------------\nInput: [0 0] | Target: 0 | Prediction: 1\nInput: [0 1] | Target: 1 | Prediction: 0\nInput: [1 0] | Target: 1 | Prediction: 0\nInput: [1 1] | Target: 0 | Prediction: 0\n------------------------------\nAccuracy: 25.0%\n\nFinal errors per epoch: [4, 4, 4, 4, 4]\n\nThe perceptron fails to converge on the XOR problem,\ndemonstrating that it cannot learn non-linearly separable patterns.\n"}},"pos":12,"start":1763845658957,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"3c515b","input":"## Visualization\n\nWe create a comprehensive visualization showing:\n1. The decision boundary and data points\n2. The classification regions\n3. The convergence of the learning algorithm","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"5457ee","input":"# Perceptron Neural Network\n\n## Introduction\n\nThe **perceptron** is the simplest form of a neural network, introduced by Frank Rosenblatt in 1958. It serves as the fundamental building block for understanding more complex neural architectures and provides key insights into linear classification problems.\n\n## Mathematical Foundation\n\n### The Perceptron Model\n\nA perceptron takes multiple input signals $x_1, x_2, \\ldots, x_n$ and produces a single binary output. The mathematical model is defined as:\n\n$$y = \\phi\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = \\phi(\\mathbf{w}^T \\mathbf{x} + b)$$\n\nwhere:\n- $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T$ is the input vector\n- $\\mathbf{w} = [w_1, w_2, \\ldots, w_n]^T$ is the weight vector\n- $b$ is the bias term\n- $\\phi$ is the activation function\n\n### Activation Function\n\nThe classical perceptron uses the **Heaviside step function** as its activation:\n\n$$\\phi(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0 \\end{cases}$$\n\nThis produces a binary classification: the perceptron \"fires\" (outputs 1) when the weighted sum exceeds the threshold.\n\n### Decision Boundary\n\nThe perceptron defines a **hyperplane** in the input space that separates the two classes. For a 2D input space, the decision boundary is:\n\n$$w_1 x_1 + w_2 x_2 + b = 0$$\n\nThis can be rewritten as:\n\n$$x_2 = -\\frac{w_1}{w_2} x_1 - \\frac{b}{w_2}$$\n\n## Learning Algorithm\n\n### Perceptron Learning Rule\n\nThe perceptron learns by adjusting its weights based on classification errors. For each training sample $(\\mathbf{x}^{(i)}, y^{(i)})$:\n\n1. Compute the prediction: $\\hat{y}^{(i)} = \\phi(\\mathbf{w}^T \\mathbf{x}^{(i)} + b)$\n\n2. Update weights if misclassified:\n   $$\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta (y^{(i)} - \\hat{y}^{(i)}) \\mathbf{x}^{(i)}$$\n   $$b \\leftarrow b + \\eta (y^{(i)} - \\hat{y}^{(i)})$$\n\nwhere $\\eta$ is the **learning rate**.\n\n### Convergence Theorem\n\nThe **Perceptron Convergence Theorem** states that if the training data is **linearly separable**, the perceptron learning algorithm will converge to a solution in a finite number of steps. The number of updates is bounded by:\n\n$$k \\leq \\frac{R^2 \\|\\mathbf{w}^*\\|^2}{\\gamma^2}$$\n\nwhere:\n- $R = \\max_i \\|\\mathbf{x}^{(i)}\\|$ is the maximum norm of input vectors\n- $\\mathbf{w}^*$ is any separating weight vector\n- $\\gamma = \\min_i y^{(i)}(\\mathbf{w}^{*T} \\mathbf{x}^{(i)})$ is the margin\n\n## Limitations\n\nThe perceptron can only solve **linearly separable** problems. The famous example of XOR demonstrates this limitationâ€”no single hyperplane can separate the XOR truth table. This led to the development of **multi-layer perceptrons (MLPs)** which can learn non-linear decision boundaries.","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"72b76d","input":"## Implementation\n\nWe will implement a perceptron from scratch and demonstrate its learning capabilities on a linearly separable dataset.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"864934","input":"## Train the Perceptron","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"d53445","input":"## XOR Problem Demonstration\n\nTo illustrate the fundamental limitation of a single-layer perceptron, we demonstrate its inability to solve the XOR problem.","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"f64587","input":"## Generate Synthetic Dataset\n\nWe create a linearly separable dataset consisting of two classes with distinct cluster centers.","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"f878ad","input":"## Summary\n\nIn this notebook, we have:\n\n1. **Derived the mathematical model** of the perceptron, including its activation function and decision boundary\n\n2. **Implemented the perceptron learning algorithm** from scratch using NumPy\n\n3. **Trained the perceptron** on a linearly separable dataset and observed convergence\n\n4. **Visualized** the decision regions, convergence behavior, and weight vector interpretation\n\n5. **Demonstrated the XOR limitation**, showing why multi-layer networks are necessary for non-linear problems\n\n### Key Takeaways\n\n- The perceptron is a **linear classifier** that finds a hyperplane to separate classes\n- It **converges** on linearly separable data but **fails** on non-linearly separable problems\n- The **weight vector** is perpendicular to the decision boundary\n- Despite its limitations, the perceptron laid the foundation for modern deep learning","pos":13,"type":"cell"}
{"id":0,"time":1763845649827,"type":"user"}
{"last_load":1763845650115,"type":"file"}