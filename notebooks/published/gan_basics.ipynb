{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs): Fundamentals\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Generative Adversarial Networks (GANs), introduced by Goodfellow et al. in 2014, represent a paradigm shift in generative modeling. GANs employ a game-theoretic framework where two neural networks—a **Generator** $G$ and a **Discriminator** $D$—compete in a minimax game.\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### The Adversarial Game\n",
    "\n",
    "The Generator $G: \\mathcal{Z} \\rightarrow \\mathcal{X}$ maps samples from a latent distribution $p_z(z)$ (typically Gaussian) to the data space $\\mathcal{X}$. The Discriminator $D: \\mathcal{X} \\rightarrow [0, 1]$ outputs the probability that a sample originated from the true data distribution $p_{\\text{data}}(x)$ rather than from $G$.\n",
    "\n",
    "### Minimax Objective\n",
    "\n",
    "The training objective is formulated as a two-player minimax game:\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "where:\n",
    "- $\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)]$ measures how well $D$ identifies real samples\n",
    "- $\\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$ measures how well $D$ identifies fake samples\n",
    "\n",
    "### Optimal Discriminator\n",
    "\n",
    "For a fixed generator $G$, the optimal discriminator $D^*_G$ is:\n",
    "\n",
    "$$D^*_G(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}$$\n",
    "\n",
    "where $p_g(x)$ is the distribution induced by $G$.\n",
    "\n",
    "### Global Optimum\n",
    "\n",
    "The global minimum of the virtual training criterion $C(G) = \\max_D V(G, D)$ is achieved if and only if $p_g = p_{\\text{data}}$. At this point:\n",
    "\n",
    "$$C(G) = -\\log 4$$\n",
    "\n",
    "### Jensen-Shannon Divergence\n",
    "\n",
    "The GAN objective can be reformulated in terms of the Jensen-Shannon divergence:\n",
    "\n",
    "$$C(G) = -\\log 4 + 2 \\cdot D_{JS}(p_{\\text{data}} \\| p_g)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$D_{JS}(P \\| Q) = \\frac{1}{2} D_{KL}\\left(P \\| \\frac{P+Q}{2}\\right) + \\frac{1}{2} D_{KL}\\left(Q \\| \\frac{P+Q}{2}\\right)$$\n",
    "\n",
    "## Training Dynamics\n",
    "\n",
    "In practice, training alternates between:\n",
    "\n",
    "1. **Discriminator update**: Maximize $V(D, G)$ with respect to $D$\n",
    "2. **Generator update**: Minimize $V(D, G)$ with respect to $G$\n",
    "\n",
    "The generator loss is often modified to $-\\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]$ for stronger gradients early in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simple GAN implementation using NumPy\n",
    "# We will learn to generate samples from a 1D Gaussian mixture\n",
    "\n",
    "class SimpleGAN:\n",
    "    \"\"\"A minimal GAN implementation for 1D data using NumPy.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=1, hidden_dim=32, lr=0.01):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Generator weights: z -> hidden -> output\n",
    "        self.g_w1 = np.random.randn(latent_dim, hidden_dim) * 0.1\n",
    "        self.g_b1 = np.zeros(hidden_dim)\n",
    "        self.g_w2 = np.random.randn(hidden_dim, 1) * 0.1\n",
    "        self.g_b2 = np.zeros(1)\n",
    "        \n",
    "        # Discriminator weights: x -> hidden -> sigmoid\n",
    "        self.d_w1 = np.random.randn(1, hidden_dim) * 0.1\n",
    "        self.d_b1 = np.zeros(hidden_dim)\n",
    "        self.d_w2 = np.random.randn(hidden_dim, 1) * 0.1\n",
    "        self.d_b2 = np.zeros(1)\n",
    "    \n",
    "    def leaky_relu(self, x, alpha=0.01):\n",
    "        return np.where(x > 0, x, alpha * x)\n",
    "    \n",
    "    def leaky_relu_derivative(self, x, alpha=0.01):\n",
    "        return np.where(x > 0, 1, alpha)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def generator_forward(self, z):\n",
    "        \"\"\"Generate samples from latent vectors.\"\"\"\n",
    "        self.g_h1_pre = z @ self.g_w1 + self.g_b1\n",
    "        self.g_h1 = self.leaky_relu(self.g_h1_pre)\n",
    "        self.g_out = self.g_h1 @ self.g_w2 + self.g_b2\n",
    "        return self.g_out\n",
    "    \n",
    "    def discriminator_forward(self, x):\n",
    "        \"\"\"Classify samples as real or fake.\"\"\"\n",
    "        self.d_h1_pre = x @ self.d_w1 + self.d_b1\n",
    "        self.d_h1 = self.leaky_relu(self.d_h1_pre)\n",
    "        self.d_logit = self.d_h1 @ self.d_w2 + self.d_b2\n",
    "        self.d_out = self.sigmoid(self.d_logit)\n",
    "        return self.d_out\n",
    "    \n",
    "    def train_discriminator(self, real_samples, z):\n",
    "        \"\"\"Update discriminator to distinguish real from fake.\"\"\"\n",
    "        batch_size = real_samples.shape[0]\n",
    "        \n",
    "        # Forward pass on real samples\n",
    "        d_real = self.discriminator_forward(real_samples)\n",
    "        d_real_h1 = self.d_h1.copy()\n",
    "        d_real_h1_pre = self.d_h1_pre.copy()\n",
    "        \n",
    "        # Forward pass on fake samples\n",
    "        fake_samples = self.generator_forward(z)\n",
    "        d_fake = self.discriminator_forward(fake_samples)\n",
    "        d_fake_h1 = self.d_h1.copy()\n",
    "        d_fake_h1_pre = self.d_h1_pre.copy()\n",
    "        \n",
    "        # Loss: -[log(D(x)) + log(1 - D(G(z)))]\n",
    "        eps = 1e-8\n",
    "        d_loss = -np.mean(np.log(d_real + eps) + np.log(1 - d_fake + eps))\n",
    "        \n",
    "        # Gradients for real samples\n",
    "        d_real_grad = -1.0 / (d_real + eps) / batch_size\n",
    "        d_real_logit_grad = d_real_grad * d_real * (1 - d_real)\n",
    "        \n",
    "        d_w2_grad_real = d_real_h1.T @ d_real_logit_grad\n",
    "        d_b2_grad_real = np.sum(d_real_logit_grad, axis=0)\n",
    "        \n",
    "        d_h1_grad_real = d_real_logit_grad @ self.d_w2.T\n",
    "        d_h1_pre_grad_real = d_h1_grad_real * self.leaky_relu_derivative(d_real_h1_pre)\n",
    "        \n",
    "        d_w1_grad_real = real_samples.T @ d_h1_pre_grad_real\n",
    "        d_b1_grad_real = np.sum(d_h1_pre_grad_real, axis=0)\n",
    "        \n",
    "        # Gradients for fake samples\n",
    "        d_fake_grad = 1.0 / (1 - d_fake + eps) / batch_size\n",
    "        d_fake_logit_grad = d_fake_grad * d_fake * (1 - d_fake)\n",
    "        \n",
    "        d_w2_grad_fake = d_fake_h1.T @ d_fake_logit_grad\n",
    "        d_b2_grad_fake = np.sum(d_fake_logit_grad, axis=0)\n",
    "        \n",
    "        d_h1_grad_fake = d_fake_logit_grad @ self.d_w2.T\n",
    "        d_h1_pre_grad_fake = d_h1_grad_fake * self.leaky_relu_derivative(d_fake_h1_pre)\n",
    "        \n",
    "        d_w1_grad_fake = fake_samples.T @ d_h1_pre_grad_fake\n",
    "        d_b1_grad_fake = np.sum(d_h1_pre_grad_fake, axis=0)\n",
    "        \n",
    "        # Update discriminator\n",
    "        self.d_w2 -= self.lr * (d_w2_grad_real + d_w2_grad_fake)\n",
    "        self.d_b2 -= self.lr * (d_b2_grad_real + d_b2_grad_fake)\n",
    "        self.d_w1 -= self.lr * (d_w1_grad_real + d_w1_grad_fake)\n",
    "        self.d_b1 -= self.lr * (d_b1_grad_real + d_b1_grad_fake)\n",
    "        \n",
    "        return d_loss\n",
    "    \n",
    "    def train_generator(self, z):\n",
    "        \"\"\"Update generator to fool discriminator.\"\"\"\n",
    "        batch_size = z.shape[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        fake_samples = self.generator_forward(z)\n",
    "        d_fake = self.discriminator_forward(fake_samples)\n",
    "        \n",
    "        # Loss: -log(D(G(z))) (non-saturating loss)\n",
    "        eps = 1e-8\n",
    "        g_loss = -np.mean(np.log(d_fake + eps))\n",
    "        \n",
    "        # Backprop through discriminator (frozen)\n",
    "        d_fake_grad = -1.0 / (d_fake + eps) / batch_size\n",
    "        d_logit_grad = d_fake_grad * d_fake * (1 - d_fake)\n",
    "        \n",
    "        d_h1_grad = d_logit_grad @ self.d_w2.T\n",
    "        d_h1_pre_grad = d_h1_grad * self.leaky_relu_derivative(self.d_h1_pre)\n",
    "        \n",
    "        # Gradient w.r.t. fake samples (generator output)\n",
    "        g_out_grad = d_h1_pre_grad @ self.d_w1.T\n",
    "        \n",
    "        # Backprop through generator\n",
    "        g_w2_grad = self.g_h1.T @ g_out_grad\n",
    "        g_b2_grad = np.sum(g_out_grad, axis=0)\n",
    "        \n",
    "        g_h1_grad = g_out_grad @ self.g_w2.T\n",
    "        g_h1_pre_grad = g_h1_grad * self.leaky_relu_derivative(self.g_h1_pre)\n",
    "        \n",
    "        g_w1_grad = z.T @ g_h1_pre_grad\n",
    "        g_b1_grad = np.sum(g_h1_pre_grad, axis=0)\n",
    "        \n",
    "        # Update generator\n",
    "        self.g_w2 -= self.lr * g_w2_grad\n",
    "        self.g_b2 -= self.lr * g_b2_grad\n",
    "        self.g_w1 -= self.lr * g_w1_grad\n",
    "        self.g_b1 -= self.lr * g_b1_grad\n",
    "        \n",
    "        return g_loss\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate samples from the learned distribution.\"\"\"\n",
    "        z = np.random.randn(n_samples, self.latent_dim)\n",
    "        return self.generator_forward(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_data(n_samples):\n",
    "    \"\"\"Generate samples from a bimodal Gaussian mixture.\"\"\"\n",
    "    # Two modes centered at -2 and 2\n",
    "    mix = np.random.rand(n_samples) < 0.5\n",
    "    samples = np.where(mix, \n",
    "                       np.random.randn(n_samples) * 0.5 - 2,\n",
    "                       np.random.randn(n_samples) * 0.5 + 2)\n",
    "    return samples.reshape(-1, 1)\n",
    "\n",
    "# Training configuration\n",
    "n_epochs = 2000\n",
    "batch_size = 128\n",
    "d_steps = 1  # Discriminator updates per generator update\n",
    "\n",
    "# Initialize GAN\n",
    "gan = SimpleGAN(latent_dim=1, hidden_dim=64, lr=0.005)\n",
    "\n",
    "# Storage for metrics\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "samples_history = []\n",
    "\n",
    "print(\"Training GAN to learn a bimodal Gaussian mixture...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train discriminator\n",
    "    for _ in range(d_steps):\n",
    "        real_samples = generate_real_data(batch_size)\n",
    "        z = np.random.randn(batch_size, gan.latent_dim)\n",
    "        d_loss = gan.train_discriminator(real_samples, z)\n",
    "    \n",
    "    # Train generator\n",
    "    z = np.random.randn(batch_size, gan.latent_dim)\n",
    "    g_loss = gan.train_generator(z)\n",
    "    \n",
    "    d_losses.append(d_loss)\n",
    "    g_losses.append(g_loss)\n",
    "    \n",
    "    # Save samples for visualization\n",
    "    if epoch % 200 == 0 or epoch == n_epochs - 1:\n",
    "        samples = gan.generate(1000)\n",
    "        samples_history.append((epoch, samples.flatten()))\n",
    "        print(f\"Epoch {epoch:4d} | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive visualization\nfig = plt.figure(figsize=(14, 10))\ngs = GridSpec(3, 2, figure=fig, hspace=0.3, wspace=0.25)\n\n# Plot 1: Training losses\nax1 = fig.add_subplot(gs[0, :])\nax1.plot(d_losses, label='Discriminator Loss', alpha=0.7, linewidth=0.8)\nax1.plot(g_losses, label='Generator Loss', alpha=0.7, linewidth=0.8)\nax1.axhline(y=-np.log(4), color='k', linestyle='--', label='Optimal: $-\\\\log 4$', alpha=0.5)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('GAN Training Dynamics')\nax1.legend(loc='upper right')\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Distribution evolution\nax2 = fig.add_subplot(gs[1, 0])\ncolors = plt.cm.viridis(np.linspace(0, 1, len(samples_history)))\nfor i, (epoch, samples) in enumerate(samples_history):\n    ax2.hist(samples, bins=50, alpha=0.4, density=True, \n             color=colors[i], label=f'Epoch {epoch}')\n\n# Plot true distribution\nx_range = np.linspace(-5, 5, 1000)\ntrue_pdf = 0.5 * (np.exp(-(x_range + 2)**2 / (2 * 0.5**2)) / (0.5 * np.sqrt(2*np.pi)) +\n                  np.exp(-(x_range - 2)**2 / (2 * 0.5**2)) / (0.5 * np.sqrt(2*np.pi)))\nax2.plot(x_range, true_pdf, 'k-', linewidth=2, label='True Distribution')\nax2.set_xlabel('x')\nax2.set_ylabel('Density')\nax2.set_title('Generated Distribution Evolution')\nax2.legend(loc='upper right', fontsize=8)\nax2.set_xlim(-5, 5)\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Final comparison\nax3 = fig.add_subplot(gs[1, 1])\nfinal_samples = gan.generate(5000).flatten()\nreal_data = generate_real_data(5000).flatten()\n\nax3.hist(real_data, bins=60, alpha=0.5, density=True, label='Real Data', color='blue')\nax3.hist(final_samples, bins=60, alpha=0.5, density=True, label='Generated', color='red')\nax3.plot(x_range, true_pdf, 'k--', linewidth=1.5, label='True PDF')\nax3.set_xlabel('x')\nax3.set_ylabel('Density')\nax3.set_title('Final Distribution Comparison')\nax3.legend()\nax3.set_xlim(-5, 5)\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Discriminator decision boundary\nax4 = fig.add_subplot(gs[2, 0])\nx_test = np.linspace(-5, 5, 500).reshape(-1, 1)\nd_scores = gan.discriminator_forward(x_test).flatten()\n\nax4.plot(x_test, d_scores, 'g-', linewidth=2, label='$D(x)$')\nax4.axhline(y=0.5, color='k', linestyle='--', alpha=0.5, label='Decision boundary')\n# Compute true_pdf for x_test (500 points) to match dimensions\nx_test_flat = x_test.flatten()\ntrue_pdf_500 = 0.5 * (np.exp(-(x_test_flat + 2)**2 / (2 * 0.5**2)) / (0.5 * np.sqrt(2*np.pi)) +\n                      np.exp(-(x_test_flat - 2)**2 / (2 * 0.5**2)) / (0.5 * np.sqrt(2*np.pi)))\nax4.fill_between(x_test_flat, 0, true_pdf_500/true_pdf_500.max(),\n                  alpha=0.2, color='blue', label='True data density')\nax4.set_xlabel('x')\nax4.set_ylabel('$D(x)$')\nax4.set_title('Discriminator Response')\nax4.legend(loc='upper right')\nax4.set_xlim(-5, 5)\nax4.set_ylim(0, 1)\nax4.grid(True, alpha=0.3)\n\n# Plot 5: Generator transformation\nax5 = fig.add_subplot(gs[2, 1])\nz_samples = np.linspace(-3, 3, 500).reshape(-1, 1)\ng_outputs = gan.generator_forward(z_samples).flatten()\n\nax5.plot(z_samples, g_outputs, 'r-', linewidth=2)\nax5.set_xlabel('Latent $z$')\nax5.set_ylabel('Generated $G(z)$')\nax5.set_title('Generator Mapping: $z \\\\rightarrow G(z)$')\nax5.grid(True, alpha=0.3)\n\n# Highlight the bimodal output regions\nax5.axhline(y=-2, color='b', linestyle='--', alpha=0.5)\nax5.axhline(y=2, color='b', linestyle='--', alpha=0.5)\n\nplt.suptitle('Generative Adversarial Network: Learning a Bimodal Distribution', \n             fontsize=14, fontweight='bold', y=1.02)\n\n# Save the figure\nplt.savefig('plot.png', dpi=150, bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\\nFigure saved to plot.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the fundamental concepts of Generative Adversarial Networks:\n",
    "\n",
    "1. **Theoretical Framework**: GANs formulate generative modeling as a minimax game between generator and discriminator networks.\n",
    "\n",
    "2. **Training Dynamics**: The adversarial training process alternates between discriminator and generator updates, ideally converging to $p_g = p_{\\text{data}}$.\n",
    "\n",
    "3. **Practical Implementation**: Even a simple two-layer network can learn to generate samples from a multimodal distribution.\n",
    "\n",
    "4. **Key Observations**:\n",
    "   - The generator learns to map latent samples to the bimodal target distribution\n",
    "   - The discriminator converges toward outputting $D(x) \\approx 0.5$ when the generator succeeds\n",
    "   - Training stability requires careful balancing of learning rates and architectures\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Goodfellow et al. (2014). *Generative Adversarial Nets*. NeurIPS.\n",
    "- Arjovsky et al. (2017). *Wasserstein GAN*. ICML.\n",
    "- Gulrajani et al. (2017). *Improved Training of Wasserstein GANs*. NeurIPS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}