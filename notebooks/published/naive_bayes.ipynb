{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "Naive Bayes is a family of probabilistic classifiers based on **Bayes' theorem** with a strong (naive) independence assumption between features. Despite its simplicity, Naive Bayes often performs surprisingly well in practice, particularly for text classification and spam filtering.\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "The foundation of Naive Bayes is Bayes' theorem, which describes the probability of an event based on prior knowledge:\n",
    "\n",
    "$$P(C_k | \\mathbf{x}) = \\frac{P(\\mathbf{x} | C_k) P(C_k)}{P(\\mathbf{x})}$$\n",
    "\n",
    "Where:\n",
    "- $P(C_k | \\mathbf{x})$ is the **posterior probability** of class $C_k$ given features $\\mathbf{x}$\n",
    "- $P(\\mathbf{x} | C_k)$ is the **likelihood** of features $\\mathbf{x}$ given class $C_k$\n",
    "- $P(C_k)$ is the **prior probability** of class $C_k$\n",
    "- $P(\\mathbf{x})$ is the **evidence** (normalizing constant)\n",
    "\n",
    "### The Naive Independence Assumption\n",
    "\n",
    "The \"naive\" assumption states that all features are conditionally independent given the class:\n",
    "\n",
    "$$P(\\mathbf{x} | C_k) = P(x_1, x_2, \\ldots, x_n | C_k) = \\prod_{i=1}^{n} P(x_i | C_k)$$\n",
    "\n",
    "This simplification allows for efficient computation even with high-dimensional feature spaces.\n",
    "\n",
    "### Classification Rule\n",
    "\n",
    "The Naive Bayes classifier assigns the class with maximum posterior probability:\n",
    "\n",
    "$$\\hat{y} = \\underset{k \\in \\{1, \\ldots, K\\}}{\\arg\\max} \\ P(C_k) \\prod_{i=1}^{n} P(x_i | C_k)$$\n",
    "\n",
    "Since $P(\\mathbf{x})$ is constant for all classes, it can be ignored in the maximization.\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "For continuous features, we assume each feature follows a Gaussian distribution:\n",
    "\n",
    "$$P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{k,i})^2}{2\\sigma_{k,i}^2}\\right)$$\n",
    "\n",
    "Where $\\mu_{k,i}$ and $\\sigma_{k,i}^2$ are the mean and variance of feature $i$ for class $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "We will implement Gaussian Naive Bayes from scratch and apply it to a synthetic classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation.\n",
    "    \n",
    "    Assumes features are continuous and follow Gaussian distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "        self.means = None\n",
    "        self.variances = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Naive Bayes model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target labels\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.variances = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # Calculate statistics for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.priors[idx] = len(X_c) / n_samples\n",
    "            self.means[idx, :] = X_c.mean(axis=0)\n",
    "            self.variances[idx, :] = X_c.var(axis=0) + 1e-9  # Add small value for numerical stability\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _gaussian_likelihood(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Calculate Gaussian probability density.\n",
    "        \n",
    "        P(x|C_k) = (1/sqrt(2*pi*var)) * exp(-(x-mean)^2 / (2*var))\n",
    "        \"\"\"\n",
    "        coefficient = 1.0 / np.sqrt(2.0 * np.pi * var)\n",
    "        exponent = np.exp(-(x - mean) ** 2 / (2.0 * var))\n",
    "        return coefficient * exponent\n",
    "    \n",
    "    def _calculate_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Calculate posterior probability for each class.\n",
    "        \n",
    "        Uses log probabilities to avoid numerical underflow.\n",
    "        \"\"\"\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Log prior\n",
    "            prior = np.log(self.priors[idx])\n",
    "            \n",
    "            # Sum of log likelihoods (product becomes sum in log space)\n",
    "            likelihood = np.sum(np.log(self._gaussian_likelihood(\n",
    "                x, self.means[idx, :], self.variances[idx, :]\n",
    "            )))\n",
    "            \n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        return posteriors\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            posteriors = self._calculate_posterior(x)\n",
    "            predictions.append(self.classes[np.argmax(posteriors)])\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for samples in X.\n",
    "        \"\"\"\n",
    "        probas = []\n",
    "        for x in X:\n",
    "            posteriors = np.array(self._calculate_posterior(x))\n",
    "            # Convert log probabilities to probabilities using softmax\n",
    "            posteriors = posteriors - np.max(posteriors)  # Numerical stability\n",
    "            exp_posteriors = np.exp(posteriors)\n",
    "            probas.append(exp_posteriors / np.sum(exp_posteriors))\n",
    "        return np.array(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset\n",
    "\n",
    "We create a binary classification problem with two Gaussian-distributed classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples_per_class=200):\n",
    "    \"\"\"\n",
    "    Generate synthetic 2D data from two Gaussian distributions.\n",
    "    \"\"\"\n",
    "    # Class 0: centered at (2, 2)\n",
    "    mean_0 = [2, 2]\n",
    "    cov_0 = [[1.0, 0.3], [0.3, 1.0]]\n",
    "    X_0 = np.random.multivariate_normal(mean_0, cov_0, n_samples_per_class)\n",
    "    y_0 = np.zeros(n_samples_per_class)\n",
    "    \n",
    "    # Class 1: centered at (5, 5)\n",
    "    mean_1 = [5, 5]\n",
    "    cov_1 = [[1.5, -0.4], [-0.4, 1.5]]\n",
    "    X_1 = np.random.multivariate_normal(mean_1, cov_1, n_samples_per_class)\n",
    "    y_1 = np.ones(n_samples_per_class)\n",
    "    \n",
    "    # Combine data\n",
    "    X = np.vstack([X_0, X_1])\n",
    "    y = np.hstack([y_0, y_1])\n",
    "    \n",
    "    # Shuffle\n",
    "    shuffle_idx = np.random.permutation(len(y))\n",
    "    X, y = X[shuffle_idx], y[shuffle_idx]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_synthetic_data(200)\n",
    "\n",
    "# Split into train and test sets\n",
    "split_idx = int(0.8 * len(y))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Display learned parameters\n",
    "print(\"Learned Parameters:\")\n",
    "print(\"=\"*50)\n",
    "for idx, c in enumerate(gnb.classes):\n",
    "    print(f\"\\nClass {int(c)}:\")\n",
    "    print(f\"  Prior P(C_{int(c)}) = {gnb.priors[idx]:.4f}\")\n",
    "    print(f\"  Mean μ = {gnb.means[idx]}\")\n",
    "    print(f\"  Variance σ² = {gnb.variances[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    classes = np.unique(y_true)\n",
    "    n_classes = len(classes)\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for i, c_true in enumerate(classes):\n",
    "        for j, c_pred in enumerate(classes):\n",
    "            cm[i, j] = np.sum((y_true == c_true) & (y_pred == c_pred))\n",
    "    return cm\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We visualize the decision boundary and class distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Decision Boundary\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Create mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Predict on mesh\n",
    "Z = gnb.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "ax1.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "ax1.contour(xx, yy, Z, colors='k', linewidths=0.5)\n",
    "\n",
    "# Plot data points\n",
    "scatter = ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', \n",
    "                      edgecolors='k', alpha=0.7, s=50)\n",
    "\n",
    "# Mark class means\n",
    "for idx, c in enumerate(gnb.classes):\n",
    "    ax1.scatter(gnb.means[idx, 0], gnb.means[idx, 1], \n",
    "               marker='*', s=300, c='black', edgecolors='white', linewidths=2,\n",
    "               label=f'Class {int(c)} mean')\n",
    "\n",
    "ax1.set_xlabel('Feature $x_1$', fontsize=12)\n",
    "ax1.set_ylabel('Feature $x_2$', fontsize=12)\n",
    "ax1.set_title('Naive Bayes Decision Boundary', fontsize=14)\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot 2: Probability Contours\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Get probability predictions\n",
    "Z_proba = gnb.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z_proba = Z_proba.reshape(xx.shape)\n",
    "\n",
    "# Plot probability contours\n",
    "contour = ax2.contourf(xx, yy, Z_proba, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "plt.colorbar(contour, ax=ax2, label='P(Class 1 | x)')\n",
    "\n",
    "# Plot data points\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', \n",
    "           edgecolors='k', alpha=0.7, s=50)\n",
    "\n",
    "# Add decision boundary (P=0.5)\n",
    "ax2.contour(xx, yy, Z_proba, levels=[0.5], colors='k', linewidths=2)\n",
    "\n",
    "ax2.set_xlabel('Feature $x_1$', fontsize=12)\n",
    "ax2.set_ylabel('Feature $x_2$', fontsize=12)\n",
    "ax2.set_title('Posterior Probability $P(C_1 | \\\\mathbf{x})$', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Naive Independence Assumption\n",
    "\n",
    "The key insight of Naive Bayes is that it treats each feature independently. Let's examine how this affects the likelihood calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the naive independence assumption\n",
    "print(\"Demonstrating the Naive Independence Assumption\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Take a sample point\n",
    "sample_point = X_test[0]\n",
    "print(f\"\\nSample point: x = {sample_point}\")\n",
    "\n",
    "print(\"\\nLikelihood calculation breakdown:\")\n",
    "for idx, c in enumerate(gnb.classes):\n",
    "    print(f\"\\nClass {int(c)}:\")\n",
    "    \n",
    "    # Calculate individual feature likelihoods\n",
    "    for i in range(len(sample_point)):\n",
    "        likelihood_i = norm.pdf(sample_point[i], \n",
    "                               gnb.means[idx, i], \n",
    "                               np.sqrt(gnb.variances[idx, i]))\n",
    "        print(f\"  P(x_{i+1}={sample_point[i]:.2f} | C_{int(c)}) = {likelihood_i:.6f}\")\n",
    "    \n",
    "    # Total likelihood (product of individual likelihoods)\n",
    "    total_likelihood = np.prod([\n",
    "        norm.pdf(sample_point[i], gnb.means[idx, i], np.sqrt(gnb.variances[idx, i]))\n",
    "        for i in range(len(sample_point))\n",
    "    ])\n",
    "    print(f\"  P(x | C_{int(c)}) = {total_likelihood:.6f}\")\n",
    "    print(f\"  Prior P(C_{int(c)}) = {gnb.priors[idx]:.4f}\")\n",
    "    print(f\"  Unnormalized posterior ∝ {total_likelihood * gnb.priors[idx]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Simplicity**: Naive Bayes is computationally efficient, requiring only $O(n \\cdot K)$ parameters where $n$ is the number of features and $K$ is the number of classes.\n",
    "\n",
    "2. **Robustness**: Despite the naive independence assumption being rarely true in practice, the classifier often performs well because it only needs the correct *ranking* of probabilities, not their exact values.\n",
    "\n",
    "3. **Interpretability**: The model parameters (means and variances) have clear probabilistic interpretations.\n",
    "\n",
    "4. **Scalability**: Training is linear in the number of samples, making it suitable for large datasets.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- The independence assumption may lead to suboptimal decision boundaries when features are highly correlated\n",
    "- Probability estimates are often poorly calibrated\n",
    "- Cannot capture feature interactions without explicit feature engineering\n",
    "\n",
    "### Extensions\n",
    "\n",
    "- **Multinomial Naive Bayes**: For discrete count data (e.g., word frequencies)\n",
    "- **Bernoulli Naive Bayes**: For binary features\n",
    "- **Complement Naive Bayes**: For imbalanced datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
