{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Differentiation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Numerical differentiation is a fundamental technique in computational mathematics for approximating derivatives of functions when analytical differentiation is impractical or impossible. This notebook explores the theoretical foundations and practical implementations of finite difference methods.\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "### Taylor Series Expansion\n",
    "\n",
    "The foundation of numerical differentiation lies in the Taylor series expansion. For a sufficiently smooth function $f(x)$, we can write:\n",
    "\n",
    "$$f(x + h) = f(x) + hf'(x) + \\frac{h^2}{2!}f''(x) + \\frac{h^3}{3!}f'''(x) + \\mathcal{O}(h^4)$$\n",
    "\n",
    "Similarly:\n",
    "\n",
    "$$f(x - h) = f(x) - hf'(x) + \\frac{h^2}{2!}f''(x) - \\frac{h^3}{3!}f'''(x) + \\mathcal{O}(h^4)$$\n",
    "\n",
    "### Finite Difference Formulas\n",
    "\n",
    "#### Forward Difference\n",
    "\n",
    "Rearranging the first Taylor expansion:\n",
    "\n",
    "$$f'(x) = \\frac{f(x+h) - f(x)}{h} - \\frac{h}{2}f''(\\xi)$$\n",
    "\n",
    "The **forward difference approximation** is:\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "This has a truncation error of $\\mathcal{O}(h)$.\n",
    "\n",
    "#### Backward Difference\n",
    "\n",
    "Similarly, the **backward difference approximation** is:\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x) - f(x-h)}{h}$$\n",
    "\n",
    "Also with truncation error $\\mathcal{O}(h)$.\n",
    "\n",
    "#### Central Difference\n",
    "\n",
    "Subtracting the two Taylor expansions:\n",
    "\n",
    "$$f(x+h) - f(x-h) = 2hf'(x) + \\frac{2h^3}{3!}f'''(x) + \\mathcal{O}(h^5)$$\n",
    "\n",
    "The **central difference approximation** is:\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}$$\n",
    "\n",
    "This has a truncation error of $\\mathcal{O}(h^2)$, making it more accurate than forward or backward differences.\n",
    "\n",
    "### Second Derivative\n",
    "\n",
    "Adding the two Taylor expansions:\n",
    "\n",
    "$$f(x+h) + f(x-h) = 2f(x) + h^2 f''(x) + \\mathcal{O}(h^4)$$\n",
    "\n",
    "The **central difference for the second derivative** is:\n",
    "\n",
    "$$f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}$$\n",
    "\n",
    "with truncation error $\\mathcal{O}(h^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define finite difference functions\n",
    "def forward_difference(f, x, h):\n",
    "    \"\"\"Forward difference approximation of the first derivative.\"\"\"\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def backward_difference(f, x, h):\n",
    "    \"\"\"Backward difference approximation of the first derivative.\"\"\"\n",
    "    return (f(x) - f(x - h)) / h\n",
    "\n",
    "def central_difference(f, x, h):\n",
    "    \"\"\"Central difference approximation of the first derivative.\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "def second_derivative_central(f, x, h):\n",
    "    \"\"\"Central difference approximation of the second derivative.\"\"\"\n",
    "    return (f(x + h) - 2 * f(x) + f(x - h)) / (h ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function\n",
    "\n",
    "We will test our numerical differentiation methods using the function:\n",
    "\n",
    "$$f(x) = \\sin(x)$$\n",
    "\n",
    "The analytical derivatives are:\n",
    "\n",
    "$$f'(x) = \\cos(x)$$\n",
    "$$f''(x) = -\\sin(x)$$\n",
    "\n",
    "This allows us to compute exact errors for our approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function and its analytical derivatives\n",
    "def f(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def f_prime(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "def f_double_prime(x):\n",
    "    return -np.sin(x)\n",
    "\n",
    "# Test point\n",
    "x0 = 1.0\n",
    "\n",
    "# Range of step sizes\n",
    "h_values = np.logspace(-10, 0, 100)\n",
    "\n",
    "# Compute errors for each method\n",
    "forward_errors = np.abs(forward_difference(f, x0, h_values) - f_prime(x0))\n",
    "backward_errors = np.abs(backward_difference(f, x0, h_values) - f_prime(x0))\n",
    "central_errors = np.abs(central_difference(f, x0, h_values) - f_prime(x0))\n",
    "second_deriv_errors = np.abs(second_derivative_central(f, x0, h_values) - f_double_prime(x0))\n",
    "\n",
    "print(f\"Test point: x = {x0}\")\n",
    "print(f\"Exact f'({x0}) = {f_prime(x0):.10f}\")\n",
    "print(f\"Exact f''({x0}) = {f_double_prime(x0):.10f}\")\n",
    "print(\"\\nSample approximations with h = 0.01:\")\n",
    "h_sample = 0.01\n",
    "print(f\"Forward difference:  {forward_difference(f, x0, h_sample):.10f}\")\n",
    "print(f\"Backward difference: {backward_difference(f, x0, h_sample):.10f}\")\n",
    "print(f\"Central difference:  {central_difference(f, x0, h_sample):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "### Truncation Error vs. Round-off Error\n",
    "\n",
    "The total error in numerical differentiation has two components:\n",
    "\n",
    "1. **Truncation error**: Decreases as $h \\to 0$ (follows $\\mathcal{O}(h)$ or $\\mathcal{O}(h^2)$)\n",
    "2. **Round-off error**: Increases as $h \\to 0$ due to finite precision arithmetic\n",
    "\n",
    "The round-off error can be estimated as:\n",
    "\n",
    "$$\\epsilon_{\\text{round}} \\sim \\frac{\\epsilon_m |f(x)|}{h}$$\n",
    "\n",
    "where $\\epsilon_m \\approx 10^{-16}$ is machine epsilon for double precision.\n",
    "\n",
    "### Optimal Step Size\n",
    "\n",
    "For the central difference formula, the optimal step size that minimizes total error is approximately:\n",
    "\n",
    "$$h_{\\text{opt}} \\sim \\left(\\frac{\\epsilon_m |f(x)|}{|f'''(x)|}\\right)^{1/3} \\approx \\epsilon_m^{1/3} \\approx 10^{-5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Error convergence for first derivative methods\n",
    "ax1 = axes[0, 0]\n",
    "ax1.loglog(h_values, forward_errors, 'b-', label='Forward Difference', linewidth=1.5)\n",
    "ax1.loglog(h_values, backward_errors, 'g--', label='Backward Difference', linewidth=1.5)\n",
    "ax1.loglog(h_values, central_errors, 'r-', label='Central Difference', linewidth=1.5)\n",
    "\n",
    "# Reference lines for convergence rates\n",
    "h_ref = np.logspace(-6, -1, 50)\n",
    "ax1.loglog(h_ref, h_ref * 0.5, 'k:', alpha=0.5, label=r'$\\mathcal{O}(h)$')\n",
    "ax1.loglog(h_ref, h_ref**2 * 0.5, 'k--', alpha=0.5, label=r'$\\mathcal{O}(h^2)$')\n",
    "\n",
    "ax1.set_xlabel('Step size $h$', fontsize=11)\n",
    "ax1.set_ylabel('Absolute Error', fontsize=11)\n",
    "ax1.set_title('Error Convergence: First Derivative Methods', fontsize=12)\n",
    "ax1.legend(loc='best', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim([1e-10, 1])\n",
    "ax1.set_ylim([1e-12, 10])\n",
    "\n",
    "# Plot 2: Second derivative error\n",
    "ax2 = axes[0, 1]\n",
    "ax2.loglog(h_values, second_deriv_errors, 'm-', label='Central Difference (2nd deriv)', linewidth=1.5)\n",
    "ax2.loglog(h_ref, h_ref**2 * 0.5, 'k--', alpha=0.5, label=r'$\\mathcal{O}(h^2)$')\n",
    "\n",
    "ax2.set_xlabel('Step size $h$', fontsize=11)\n",
    "ax2.set_ylabel('Absolute Error', fontsize=11)\n",
    "ax2.set_title('Error Convergence: Second Derivative', fontsize=12)\n",
    "ax2.legend(loc='best', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim([1e-10, 1])\n",
    "ax2.set_ylim([1e-12, 10])\n",
    "\n",
    "# Plot 3: Derivative approximations over an interval\n",
    "ax3 = axes[1, 0]\n",
    "x = np.linspace(0, 2*np.pi, 200)\n",
    "h_demo = 0.5\n",
    "\n",
    "ax3.plot(x, f_prime(x), 'k-', label='Exact $f\\'(x)$', linewidth=2)\n",
    "ax3.plot(x, forward_difference(f, x, h_demo), 'b--', label=f'Forward (h={h_demo})', linewidth=1.5, alpha=0.8)\n",
    "ax3.plot(x, central_difference(f, x, h_demo), 'r-.', label=f'Central (h={h_demo})', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('$x$', fontsize=11)\n",
    "ax3.set_ylabel('$f\\'(x)$', fontsize=11)\n",
    "ax3.set_title(f'Derivative Approximations of $\\\\sin(x)$', fontsize=12)\n",
    "ax3.legend(loc='best', fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Effect of step size on central difference accuracy\n",
    "ax4 = axes[1, 1]\n",
    "h_test = [0.5, 0.1, 0.01]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "ax4.plot(x, f_prime(x), 'k-', label='Exact', linewidth=2)\n",
    "for h_val, color in zip(h_test, colors):\n",
    "    ax4.plot(x, central_difference(f, x, h_val), '--', color=color, \n",
    "             label=f'h = {h_val}', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "ax4.set_xlabel('$x$', fontsize=11)\n",
    "ax4.set_ylabel('$f\\'(x)$', fontsize=11)\n",
    "ax4.set_title('Central Difference: Effect of Step Size', fontsize=12)\n",
    "ax4.legend(loc='best', fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Convergence Rates**: The error plots confirm the theoretical convergence rates:\n",
    "   - Forward and backward differences: $\\mathcal{O}(h)$\n",
    "   - Central differences: $\\mathcal{O}(h^2)$\n",
    "\n",
    "2. **Round-off Error Plateau**: For very small $h$ ($< 10^{-8}$), round-off errors dominate, causing the error to increase.\n",
    "\n",
    "3. **Optimal Step Size**: The minimum error occurs around $h \\approx 10^{-5}$ to $10^{-8}$, depending on the method.\n",
    "\n",
    "4. **Central Difference Superiority**: Central differences achieve the same accuracy as forward/backward differences with a much larger step size, reducing round-off error effects.\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "- Use **central differences** when possible for better accuracy\n",
    "- Choose $h \\approx 10^{-5}$ to $10^{-8}$ for double precision\n",
    "- For noisy data, larger $h$ may be preferable to smooth out noise\n",
    "- Consider **Richardson extrapolation** or **complex step differentiation** for higher accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
