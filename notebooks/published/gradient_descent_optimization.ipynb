{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Gradient descent is a first-order iterative optimization algorithm used to find the minimum of a differentiable function. It is the cornerstone of modern machine learning and deep learning, serving as the primary method for training neural networks and optimizing loss functions.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### The Optimization Problem\n",
    "\n",
    "Given a differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, we seek to find:\n",
    "\n",
    "$$\\mathbf{x}^* = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x})$$\n",
    "\n",
    "### The Gradient\n",
    "\n",
    "The gradient of $f$ at point $\\mathbf{x}$ is the vector of partial derivatives:\n",
    "\n",
    "$$\\nabla f(\\mathbf{x}) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)^T$$\n",
    "\n",
    "The gradient points in the direction of steepest ascent. Therefore, to minimize $f$, we move in the opposite direction.\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "The fundamental gradient descent update rule is:\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}_k$ is the current position at iteration $k$\n",
    "- $\\alpha > 0$ is the **learning rate** (step size)\n",
    "- $\\nabla f(\\mathbf{x}_k)$ is the gradient evaluated at $\\mathbf{x}_k$\n",
    "\n",
    "### Convergence Conditions\n",
    "\n",
    "For a convex function $f$ with Lipschitz continuous gradients (i.e., $\\|\\nabla f(\\mathbf{x}) - \\nabla f(\\mathbf{y})\\| \\leq L\\|\\mathbf{x} - \\mathbf{y}\\|$), gradient descent converges when:\n",
    "\n",
    "$$0 < \\alpha < \\frac{2}{L}$$\n",
    "\n",
    "The optimal learning rate is $\\alpha^* = \\frac{1}{L}$, yielding convergence rate:\n",
    "\n",
    "$$f(\\mathbf{x}_k) - f(\\mathbf{x}^*) \\leq \\frac{L\\|\\mathbf{x}_0 - \\mathbf{x}^*\\|^2}{2k}$$\n",
    "\n",
    "## Test Functions\n",
    "\n",
    "We will demonstrate gradient descent on two classic optimization test functions:\n",
    "\n",
    "### 1. Rosenbrock Function (Banana Function)\n",
    "\n",
    "$$f(x, y) = (a - x)^2 + b(y - x^2)^2$$\n",
    "\n",
    "With $a = 1$ and $b = 100$, the global minimum is at $(1, 1)$ where $f(1, 1) = 0$.\n",
    "\n",
    "The gradient is:\n",
    "$$\\nabla f = \\begin{pmatrix} -2(a - x) - 4bx(y - x^2) \\\\ 2b(y - x^2) \\end{pmatrix}$$\n",
    "\n",
    "### 2. Beale's Function\n",
    "\n",
    "$$f(x, y) = (1.5 - x + xy)^2 + (2.25 - x + xy^2)^2 + (2.625 - x + xy^3)^2$$\n",
    "\n",
    "The global minimum is at $(3, 0.5)$ where $f(3, 0.5) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for publication-quality figures\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Define Test Functions and Their Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock function and gradient\n",
    "def rosenbrock(x, y, a=1, b=100):\n",
    "    \"\"\"Rosenbrock function: f(x,y) = (a-x)^2 + b(y-x^2)^2\"\"\"\n",
    "    return (a - x)**2 + b * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_gradient(x, y, a=1, b=100):\n",
    "    \"\"\"Gradient of Rosenbrock function\"\"\"\n",
    "    dx = -2 * (a - x) - 4 * b * x * (y - x**2)\n",
    "    dy = 2 * b * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Beale's function and gradient\n",
    "def beale(x, y):\n",
    "    \"\"\"Beale's function\"\"\"\n",
    "    term1 = (1.5 - x + x*y)**2\n",
    "    term2 = (2.25 - x + x*y**2)**2\n",
    "    term3 = (2.625 - x + x*y**3)**2\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "def beale_gradient(x, y):\n",
    "    \"\"\"Gradient of Beale's function\"\"\"\n",
    "    term1 = 1.5 - x + x*y\n",
    "    term2 = 2.25 - x + x*y**2\n",
    "    term3 = 2.625 - x + x*y**3\n",
    "    \n",
    "    dx = 2*term1*(y-1) + 2*term2*(y**2-1) + 2*term3*(y**3-1)\n",
    "    dy = 2*term1*x + 2*term2*2*x*y + 2*term3*3*x*y**2\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Simple quadratic for demonstration\n",
    "def quadratic(x, y):\n",
    "    \"\"\"Simple quadratic: f(x,y) = x^2 + 2y^2\"\"\"\n",
    "    return x**2 + 2*y**2\n",
    "\n",
    "def quadratic_gradient(x, y):\n",
    "    \"\"\"Gradient of quadratic function\"\"\"\n",
    "    return np.array([2*x, 4*y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(func, grad_func, x0, learning_rate=0.01, \n",
    "                     max_iterations=1000, tolerance=1e-8):\n",
    "    \"\"\"\n",
    "    Perform gradient descent optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    func : callable\n",
    "        Objective function f(x, y)\n",
    "    grad_func : callable\n",
    "        Gradient function returning [df/dx, df/dy]\n",
    "    x0 : array-like\n",
    "        Initial point [x, y]\n",
    "    learning_rate : float\n",
    "        Step size α\n",
    "    max_iterations : int\n",
    "        Maximum number of iterations\n",
    "    tolerance : float\n",
    "        Convergence tolerance for gradient norm\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    history : dict\n",
    "        Dictionary containing optimization trajectory\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    history = {\n",
    "        'x': [x.copy()],\n",
    "        'f': [func(x[0], x[1])],\n",
    "        'grad_norm': []\n",
    "    }\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        grad = grad_func(x[0], x[1])\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        history['grad_norm'].append(grad_norm)\n",
    "        \n",
    "        # Check convergence\n",
    "        if grad_norm < tolerance:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "        \n",
    "        # Gradient descent update\n",
    "        x = x - learning_rate * grad\n",
    "        \n",
    "        history['x'].append(x.copy())\n",
    "        history['f'].append(func(x[0], x[1]))\n",
    "    \n",
    "    history['x'] = np.array(history['x'])\n",
    "    history['f'] = np.array(history['f'])\n",
    "    history['grad_norm'] = np.array(history['grad_norm'])\n",
    "    history['iterations'] = len(history['f']) - 1\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Analysis\n",
    "\n",
    "### Effect of Learning Rate on Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate effect of learning rate on simple quadratic\n",
    "x0 = [4.0, 4.0]\n",
    "learning_rates = [0.01, 0.1, 0.3, 0.49]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Create contour plot background\n",
    "x_range = np.linspace(-5, 5, 100)\n",
    "y_range = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = quadratic(X, Y)\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "for idx, (lr, color) in enumerate(zip(learning_rates, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Run gradient descent\n",
    "    history = gradient_descent(quadratic, quadratic_gradient, x0, \n",
    "                               learning_rate=lr, max_iterations=50)\n",
    "    \n",
    "    # Plot contours\n",
    "    contour = ax.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "    ax.clabel(contour, inline=True, fontsize=8)\n",
    "    \n",
    "    # Plot trajectory\n",
    "    trajectory = history['x']\n",
    "    ax.plot(trajectory[:, 0], trajectory[:, 1], 'o-', color=color, \n",
    "            markersize=4, linewidth=1.5, label=f'Path ({len(trajectory)} steps)')\n",
    "    ax.plot(trajectory[0, 0], trajectory[0, 1], 'ko', markersize=10, label='Start')\n",
    "    ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'k*', markersize=15, label='End')\n",
    "    \n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_title(f'Learning Rate α = {lr}\\nFinal f = {history[\"f\"][-1]:.6f}')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Gradient Descent on Quadratic Function: $f(x,y) = x^2 + 2y^2$\\n' +\n",
    "             'Effect of Learning Rate on Convergence', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary of Learning Rate Effects:\")\n",
    "print(\"=\"*50)\n",
    "for lr in learning_rates:\n",
    "    history = gradient_descent(quadratic, quadratic_gradient, x0, \n",
    "                               learning_rate=lr, max_iterations=100)\n",
    "    print(f\"α = {lr:0.2f}: {history['iterations']:3d} iterations, \" +\n",
    "          f\"final f = {history['f'][-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization on Rosenbrock Function\n",
    "\n",
    "The Rosenbrock function is notoriously difficult to optimize due to its narrow, curved valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Rosenbrock function\n",
    "x0_rosenbrock = [-1.5, 2.0]\n",
    "\n",
    "# Run with a small learning rate (larger rates diverge)\n",
    "history_rosenbrock = gradient_descent(\n",
    "    rosenbrock, rosenbrock_gradient, x0_rosenbrock,\n",
    "    learning_rate=0.001, max_iterations=10000, tolerance=1e-8\n",
    ")\n",
    "\n",
    "print(f\"Rosenbrock Optimization Results:\")\n",
    "print(f\"Starting point: {x0_rosenbrock}\")\n",
    "print(f\"Final point: [{history_rosenbrock['x'][-1, 0]:.6f}, {history_rosenbrock['x'][-1, 1]:.6f}]\")\n",
    "print(f\"Final f value: {history_rosenbrock['f'][-1]:.2e}\")\n",
    "print(f\"Iterations: {history_rosenbrock['iterations']}\")\n",
    "print(f\"True minimum: [1.0, 1.0] with f = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Function value convergence\n",
    "ax1 = axes[0]\n",
    "ax1.semilogy(history_rosenbrock['f'], 'b-', linewidth=1.5)\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('$f(x_k)$')\n",
    "ax1.set_title('Rosenbrock: Function Value Convergence')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=1e-8, color='r', linestyle='--', label='Target')\n",
    "ax1.legend()\n",
    "\n",
    "# Right: Gradient norm convergence\n",
    "ax2 = axes[1]\n",
    "ax2.semilogy(history_rosenbrock['grad_norm'], 'g-', linewidth=1.5)\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('$\\\\|\\\\nabla f(x_k)\\\\|$')\n",
    "ax2.set_title('Rosenbrock: Gradient Norm Convergence')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topics\n",
    "\n",
    "### Momentum-Based Gradient Descent\n",
    "\n",
    "Standard gradient descent can be slow in narrow valleys. Momentum accelerates convergence by accumulating a velocity vector:\n",
    "\n",
    "$$\\mathbf{v}_{k+1} = \\beta \\mathbf{v}_k + \\nabla f(\\mathbf{x}_k)$$\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{v}_{k+1}$$\n",
    "\n",
    "where $\\beta \\in [0, 1)$ is the momentum coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_momentum(func, grad_func, x0, learning_rate=0.01,\n",
    "                               momentum=0.9, max_iterations=1000, tolerance=1e-8):\n",
    "    \"\"\"\n",
    "    Gradient descent with momentum.\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    v = np.zeros_like(x)  # velocity\n",
    "    \n",
    "    history = {\n",
    "        'x': [x.copy()],\n",
    "        'f': [func(x[0], x[1])],\n",
    "        'grad_norm': []\n",
    "    }\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        grad = grad_func(x[0], x[1])\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        history['grad_norm'].append(grad_norm)\n",
    "        \n",
    "        if grad_norm < tolerance:\n",
    "            break\n",
    "        \n",
    "        # Momentum update\n",
    "        v = momentum * v + grad\n",
    "        x = x - learning_rate * v\n",
    "        \n",
    "        history['x'].append(x.copy())\n",
    "        history['f'].append(func(x[0], x[1]))\n",
    "    \n",
    "    history['x'] = np.array(history['x'])\n",
    "    history['f'] = np.array(history['f'])\n",
    "    history['grad_norm'] = np.array(history['grad_norm'])\n",
    "    history['iterations'] = len(history['f']) - 1\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Compare standard GD vs momentum on Rosenbrock\n",
    "history_standard = gradient_descent(\n",
    "    rosenbrock, rosenbrock_gradient, x0_rosenbrock,\n",
    "    learning_rate=0.001, max_iterations=5000\n",
    ")\n",
    "\n",
    "history_momentum = gradient_descent_momentum(\n",
    "    rosenbrock, rosenbrock_gradient, x0_rosenbrock,\n",
    "    learning_rate=0.001, momentum=0.9, max_iterations=5000\n",
    ")\n",
    "\n",
    "print(\"Comparison: Standard GD vs Momentum\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Standard GD - Final f: {history_standard['f'][-1]:.2e}, Iterations: {history_standard['iterations']}\")\n",
    "print(f\"Momentum GD - Final f: {history_momentum['f'][-1]:.2e}, Iterations: {history_momentum['iterations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Standard vs Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Create contour for Rosenbrock\n",
    "x_range = np.linspace(-2, 2, 200)\n",
    "y_range = np.linspace(-1, 3, 200)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = rosenbrock(X, Y)\n",
    "\n",
    "# Plot trajectories\n",
    "for ax, history, title, color in [\n",
    "    (axes[0], history_standard, 'Standard Gradient Descent', 'red'),\n",
    "    (axes[1], history_momentum, 'Gradient Descent with Momentum', 'blue')\n",
    "]:\n",
    "    # Contour plot\n",
    "    contour = ax.contour(X, Y, np.log10(Z + 1), levels=30, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    # Trajectory (subsample for clarity)\n",
    "    traj = history['x']\n",
    "    step = max(1, len(traj) // 100)\n",
    "    ax.plot(traj[::step, 0], traj[::step, 1], 'o-', color=color, \n",
    "            markersize=2, linewidth=0.8, alpha=0.7)\n",
    "    \n",
    "    # Start and end points\n",
    "    ax.plot(traj[0, 0], traj[0, 1], 'ko', markersize=10, label='Start')\n",
    "    ax.plot(traj[-1, 0], traj[-1, 1], 'g*', markersize=15, label='End')\n",
    "    ax.plot(1, 1, 'r^', markersize=12, label='Global Min')\n",
    "    \n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_title(f'{title}\\nIterations: {history[\"iterations\"]}, Final f: {history[\"f\"][-1]:.2e}')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Rosenbrock Function Optimization Comparison', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the fundamentals of gradient descent optimization:\n",
    "\n",
    "1. **Learning Rate Selection**: The learning rate $\\alpha$ critically affects convergence:\n",
    "   - Too small: slow convergence\n",
    "   - Too large: oscillation or divergence\n",
    "   - Optimal: balance between speed and stability\n",
    "\n",
    "2. **Convergence Properties**: For convex functions with Lipschitz gradients, gradient descent achieves $O(1/k)$ convergence rate.\n",
    "\n",
    "3. **Momentum**: Adding momentum can significantly accelerate convergence, especially in narrow valleys like the Rosenbrock function.\n",
    "\n",
    "### Key Equations Summary\n",
    "\n",
    "| Method | Update Rule |\n",
    "|--------|-------------|\n",
    "| Standard GD | $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)$ |\n",
    "| Momentum | $\\mathbf{v}_{k+1} = \\beta \\mathbf{v}_k + \\nabla f(\\mathbf{x}_k)$, $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{v}_{k+1}$ |\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Nesterov, Y. (2004). *Introductory Lectures on Convex Optimization*\n",
    "- Boyd, S. & Vandenberghe, L. (2004). *Convex Optimization*\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*, Chapter 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
