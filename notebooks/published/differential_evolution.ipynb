{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Evolution: A Stochastic Global Optimization Algorithm\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "**Differential Evolution (DE)** is a population-based metaheuristic optimization algorithm introduced by Storn and Price in 1997. It is particularly effective for optimizing real-valued, multimodal, and non-differentiable objective functions where gradient-based methods fail.\n",
    "\n",
    "DE belongs to the family of evolutionary algorithms, which draw inspiration from biological evolution processes such as mutation, crossover (recombination), and selection.\n",
    "\n",
    "## 2. Mathematical Formulation\n",
    "\n",
    "### 2.1 Problem Statement\n",
    "\n",
    "We seek to minimize an objective function $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}$:\n",
    "\n",
    "$$\\mathbf{x}^* = \\arg\\min_{\\mathbf{x} \\in \\mathcal{S}} f(\\mathbf{x})$$\n",
    "\n",
    "where $\\mathcal{S} \\subseteq \\mathbb{R}^D$ is the search space defined by bounds $[\\mathbf{x}_{\\min}, \\mathbf{x}_{\\max}]$.\n",
    "\n",
    "### 2.2 Population Initialization\n",
    "\n",
    "DE maintains a population of $N_P$ candidate solutions (vectors). The initial population is typically generated uniformly at random:\n",
    "\n",
    "$$x_{i,j}^{(0)} = x_{j,\\min} + r_{i,j} \\cdot (x_{j,\\max} - x_{j,\\min})$$\n",
    "\n",
    "where $r_{i,j} \\sim \\mathcal{U}(0,1)$, $i = 1, \\ldots, N_P$, and $j = 1, \\ldots, D$.\n",
    "\n",
    "### 2.3 Mutation\n",
    "\n",
    "For each target vector $\\mathbf{x}_i^{(g)}$ in generation $g$, a mutant vector $\\mathbf{v}_i^{(g)}$ is created. The classic **DE/rand/1** mutation strategy is:\n",
    "\n",
    "$$\\mathbf{v}_i^{(g)} = \\mathbf{x}_{r_1}^{(g)} + F \\cdot (\\mathbf{x}_{r_2}^{(g)} - \\mathbf{x}_{r_3}^{(g)})$$\n",
    "\n",
    "where:\n",
    "- $r_1, r_2, r_3 \\in \\{1, \\ldots, N_P\\}$ are mutually distinct random indices, also different from $i$\n",
    "- $F \\in [0, 2]$ is the **mutation scale factor** (typically $F \\in [0.4, 1.0]$)\n",
    "\n",
    "Other common strategies include:\n",
    "- **DE/best/1**: $\\mathbf{v}_i = \\mathbf{x}_{\\text{best}} + F \\cdot (\\mathbf{x}_{r_1} - \\mathbf{x}_{r_2})$\n",
    "- **DE/rand/2**: $\\mathbf{v}_i = \\mathbf{x}_{r_1} + F \\cdot (\\mathbf{x}_{r_2} - \\mathbf{x}_{r_3}) + F \\cdot (\\mathbf{x}_{r_4} - \\mathbf{x}_{r_5})$\n",
    "\n",
    "### 2.4 Crossover (Recombination)\n",
    "\n",
    "The trial vector $\\mathbf{u}_i^{(g)}$ is formed by combining the mutant vector with the target vector using **binomial crossover**:\n",
    "\n",
    "$$u_{i,j}^{(g)} = \\begin{cases} v_{i,j}^{(g)} & \\text{if } r_j \\leq CR \\text{ or } j = j_{\\text{rand}} \\\\ x_{i,j}^{(g)} & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "where:\n",
    "- $CR \\in [0, 1]$ is the **crossover probability**\n",
    "- $r_j \\sim \\mathcal{U}(0,1)$ is a uniform random number for each dimension\n",
    "- $j_{\\text{rand}} \\in \\{1, \\ldots, D\\}$ ensures at least one component comes from the mutant\n",
    "\n",
    "### 2.5 Selection\n",
    "\n",
    "A greedy selection scheme determines which vector survives to the next generation:\n",
    "\n",
    "$$\\mathbf{x}_i^{(g+1)} = \\begin{cases} \\mathbf{u}_i^{(g)} & \\text{if } f(\\mathbf{u}_i^{(g)}) \\leq f(\\mathbf{x}_i^{(g)}) \\\\ \\mathbf{x}_i^{(g)} & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "### 2.6 Convergence\n",
    "\n",
    "The algorithm iterates until a stopping criterion is met:\n",
    "- Maximum number of generations $G_{\\max}$\n",
    "- Function value threshold: $f(\\mathbf{x}_{\\text{best}}) < \\epsilon$\n",
    "- Population diversity measure falls below threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "We will implement DE from scratch and apply it to the **Rastrigin function**, a classic multimodal benchmark:\n",
    "\n",
    "$$f(\\mathbf{x}) = An + \\sum_{i=1}^{n} \\left[ x_i^2 - A\\cos(2\\pi x_i) \\right]$$\n",
    "\n",
    "where $A = 10$. The global minimum is $f(\\mathbf{0}) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rastrigin(x):\n",
    "    \"\"\"\n",
    "    Rastrigin function - a non-convex function used as a performance test problem.\n",
    "    Global minimum at x = 0 with f(0) = 0.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Input vector of dimension D\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Function value\n",
    "    \"\"\"\n",
    "    A = 10\n",
    "    n = len(x)\n",
    "    return A * n + np.sum(x**2 - A * np.cos(2 * np.pi * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential_evolution(func, bounds, pop_size=50, F=0.8, CR=0.9, max_gen=200):\n",
    "    \"\"\"\n",
    "    Differential Evolution optimization algorithm (DE/rand/1/bin).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    func : callable\n",
    "        Objective function to minimize\n",
    "    bounds : array-like of shape (D, 2)\n",
    "        Bounds for each dimension [[min, max], ...]\n",
    "    pop_size : int\n",
    "        Population size (N_P)\n",
    "    F : float\n",
    "        Mutation scale factor in [0, 2]\n",
    "    CR : float\n",
    "        Crossover probability in [0, 1]\n",
    "    max_gen : int\n",
    "        Maximum number of generations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_solution : ndarray\n",
    "        Best solution found\n",
    "    best_fitness : float\n",
    "        Best fitness value\n",
    "    history : dict\n",
    "        Dictionary containing optimization history\n",
    "    \"\"\"\n",
    "    bounds = np.array(bounds)\n",
    "    D = len(bounds)  # Dimensionality\n",
    "    \n",
    "    # Initialize population uniformly within bounds\n",
    "    population = bounds[:, 0] + np.random.rand(pop_size, D) * (bounds[:, 1] - bounds[:, 0])\n",
    "    \n",
    "    # Evaluate initial population\n",
    "    fitness = np.array([func(ind) for ind in population])\n",
    "    \n",
    "    # Track best solution\n",
    "    best_idx = np.argmin(fitness)\n",
    "    best_solution = population[best_idx].copy()\n",
    "    best_fitness = fitness[best_idx]\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'best_fitness': [best_fitness],\n",
    "        'mean_fitness': [np.mean(fitness)],\n",
    "        'std_fitness': [np.std(fitness)],\n",
    "        'populations': [population.copy()]\n",
    "    }\n",
    "    \n",
    "    # Main evolution loop\n",
    "    for gen in range(max_gen):\n",
    "        for i in range(pop_size):\n",
    "            # Mutation: DE/rand/1\n",
    "            # Select three distinct individuals different from i\n",
    "            candidates = [idx for idx in range(pop_size) if idx != i]\n",
    "            r1, r2, r3 = np.random.choice(candidates, 3, replace=False)\n",
    "            \n",
    "            # Create mutant vector\n",
    "            mutant = population[r1] + F * (population[r2] - population[r3])\n",
    "            \n",
    "            # Clip to bounds\n",
    "            mutant = np.clip(mutant, bounds[:, 0], bounds[:, 1])\n",
    "            \n",
    "            # Crossover: binomial\n",
    "            trial = np.copy(population[i])\n",
    "            j_rand = np.random.randint(D)  # Ensure at least one component from mutant\n",
    "            \n",
    "            for j in range(D):\n",
    "                if np.random.rand() < CR or j == j_rand:\n",
    "                    trial[j] = mutant[j]\n",
    "            \n",
    "            # Selection\n",
    "            trial_fitness = func(trial)\n",
    "            if trial_fitness <= fitness[i]:\n",
    "                population[i] = trial\n",
    "                fitness[i] = trial_fitness\n",
    "                \n",
    "                # Update best if improved\n",
    "                if trial_fitness < best_fitness:\n",
    "                    best_solution = trial.copy()\n",
    "                    best_fitness = trial_fitness\n",
    "        \n",
    "        # Record history\n",
    "        history['best_fitness'].append(best_fitness)\n",
    "        history['mean_fitness'].append(np.mean(fitness))\n",
    "        history['std_fitness'].append(np.std(fitness))\n",
    "        \n",
    "        # Store population snapshots at key generations\n",
    "        if gen in [0, 10, 50, 100, max_gen-1]:\n",
    "            history['populations'].append(population.copy())\n",
    "    \n",
    "    return best_solution, best_fitness, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization Experiment\n",
    "\n",
    "Let us optimize the 2D Rastrigin function within the bounds $[-5.12, 5.12]^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization parameters\n",
    "D = 2  # Dimensions\n",
    "bounds = [[-5.12, 5.12]] * D\n",
    "\n",
    "# Run Differential Evolution\n",
    "best_solution, best_fitness, history = differential_evolution(\n",
    "    func=rastrigin,\n",
    "    bounds=bounds,\n",
    "    pop_size=50,\n",
    "    F=0.8,\n",
    "    CR=0.9,\n",
    "    max_gen=150\n",
    ")\n",
    "\n",
    "print(f\"Best solution found: x = [{best_solution[0]:.6f}, {best_solution[1]:.6f}]\")\n",
    "print(f\"Best fitness value: f(x) = {best_fitness:.6e}\")\n",
    "print(f\"True optimum: f([0, 0]) = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "\n",
    "We will create a comprehensive visualization showing:\n",
    "1. The Rastrigin function landscape with the optimization trajectory\n",
    "2. Convergence curves (best fitness over generations)\n",
    "3. Population evolution snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with multiple subplots\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# --- Plot 1: 3D Surface of Rastrigin Function ---\n",
    "ax1 = fig.add_subplot(2, 2, 1, projection='3d')\n",
    "\n",
    "x = np.linspace(-5.12, 5.12, 100)\n",
    "y = np.linspace(-5.12, 5.12, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = rastrigin(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.8, linewidth=0)\n",
    "ax1.scatter([best_solution[0]], [best_solution[1]], [best_fitness], \n",
    "            color='red', s=100, marker='*', label='Optimum found')\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_zlabel('$f(\\mathbf{x})$')\n",
    "ax1.set_title('Rastrigin Function Surface')\n",
    "ax1.view_init(elev=30, azim=45)\n",
    "\n",
    "# --- Plot 2: Contour Plot with Final Population ---\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "\n",
    "contour = ax2.contourf(X, Y, Z, levels=50, cmap=cm.viridis)\n",
    "plt.colorbar(contour, ax=ax2, label='$f(\\mathbf{x})$')\n",
    "\n",
    "# Plot final population\n",
    "final_pop = history['populations'][-1]\n",
    "ax2.scatter(final_pop[:, 0], final_pop[:, 1], c='white', s=30, \n",
    "            edgecolors='black', alpha=0.7, label='Final population')\n",
    "ax2.scatter([best_solution[0]], [best_solution[1]], c='red', s=200, \n",
    "            marker='*', edgecolors='black', linewidth=1, label='Best solution')\n",
    "ax2.scatter([0], [0], c='yellow', s=100, marker='o', \n",
    "            edgecolors='black', linewidth=1, label='Global optimum')\n",
    "\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_title('Contour Plot with Final Population')\n",
    "ax2.legend(loc='upper right', fontsize=8)\n",
    "ax2.set_xlim([-5.12, 5.12])\n",
    "ax2.set_ylim([-5.12, 5.12])\n",
    "\n",
    "# --- Plot 3: Convergence Curve ---\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "\n",
    "generations = range(len(history['best_fitness']))\n",
    "ax3.semilogy(generations, history['best_fitness'], 'b-', linewidth=2, label='Best fitness')\n",
    "ax3.semilogy(generations, history['mean_fitness'], 'g--', linewidth=1.5, \n",
    "             alpha=0.7, label='Mean fitness')\n",
    "\n",
    "# Add confidence band for mean ± std\n",
    "mean = np.array(history['mean_fitness'])\n",
    "std = np.array(history['std_fitness'])\n",
    "ax3.fill_between(generations, \n",
    "                 np.maximum(mean - std, 1e-10), \n",
    "                 mean + std, \n",
    "                 alpha=0.2, color='green', label='Mean $\\pm$ std')\n",
    "\n",
    "ax3.set_xlabel('Generation')\n",
    "ax3.set_ylabel('Fitness (log scale)')\n",
    "ax3.set_title('Convergence History')\n",
    "ax3.legend(loc='upper right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 4: Population Evolution ---\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "# Show contour as background\n",
    "ax4.contourf(X, Y, Z, levels=30, cmap=cm.viridis, alpha=0.5)\n",
    "\n",
    "# Plot population at different generations\n",
    "colors = ['red', 'orange', 'yellow', 'cyan', 'white']\n",
    "labels = ['Gen 0', 'Gen 10', 'Gen 50', 'Gen 100', 'Final']\n",
    "\n",
    "for idx, (pop, color, label) in enumerate(zip(history['populations'], colors, labels)):\n",
    "    alpha = 0.4 + 0.15 * idx\n",
    "    ax4.scatter(pop[:, 0], pop[:, 1], c=color, s=20, \n",
    "                edgecolors='black', linewidth=0.5, alpha=alpha, label=label)\n",
    "\n",
    "ax4.set_xlabel('$x_1$')\n",
    "ax4.set_ylabel('$x_2$')\n",
    "ax4.set_title('Population Evolution Over Generations')\n",
    "ax4.legend(loc='upper right', fontsize=8)\n",
    "ax4.set_xlim([-5.12, 5.12])\n",
    "ax4.set_ylim([-5.12, 5.12])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter Sensitivity Analysis\n",
    "\n",
    "Let us investigate how the key parameters $F$ and $CR$ affect the optimization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter sensitivity study\n",
    "F_values = [0.3, 0.5, 0.8, 1.0]\n",
    "CR_values = [0.3, 0.6, 0.9]\n",
    "\n",
    "results = np.zeros((len(F_values), len(CR_values)))\n",
    "\n",
    "for i, F in enumerate(F_values):\n",
    "    for j, CR in enumerate(CR_values):\n",
    "        # Run multiple trials and average\n",
    "        trials = 5\n",
    "        fitness_sum = 0\n",
    "        for _ in range(trials):\n",
    "            _, fitness, _ = differential_evolution(\n",
    "                func=rastrigin,\n",
    "                bounds=bounds,\n",
    "                pop_size=30,\n",
    "                F=F,\n",
    "                CR=CR,\n",
    "                max_gen=100\n",
    "            )\n",
    "            fitness_sum += fitness\n",
    "        results[i, j] = fitness_sum / trials\n",
    "\n",
    "# Display results as a table\n",
    "print(\"Average Best Fitness (5 trials each):\")\n",
    "print(\"\\nF \\\\ CR\\t\", end=\"\")\n",
    "for CR in CR_values:\n",
    "    print(f\"{CR:.1f}\\t\\t\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, F in enumerate(F_values):\n",
    "    print(f\"{F:.1f}\\t\", end=\"\")\n",
    "    for j in range(len(CR_values)):\n",
    "        print(f\"{results[i, j]:.2e}\\t\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "This notebook demonstrated the Differential Evolution algorithm, a powerful stochastic optimizer for continuous domains:\n",
    "\n",
    "1. **Algorithm Mechanics**: DE employs mutation, crossover, and selection operators to evolve a population toward the global optimum.\n",
    "\n",
    "2. **Key Parameters**:\n",
    "   - $F$ (mutation factor): Controls exploration vs. exploitation. Higher values increase diversity.\n",
    "   - $CR$ (crossover rate): Determines how much information is exchanged between parent and mutant.\n",
    "\n",
    "3. **Performance**: On the Rastrigin function, DE successfully navigates the multimodal landscape to find solutions very close to the global optimum at $\\mathbf{x}^* = \\mathbf{0}$.\n",
    "\n",
    "4. **Advantages of DE**:\n",
    "   - No gradient information required\n",
    "   - Robust to local optima\n",
    "   - Few control parameters\n",
    "   - Simple implementation\n",
    "\n",
    "5. **Applications**: DE is widely used in engineering design optimization, neural network training, signal processing, and many other domains where traditional optimization methods fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Storn, R., & Price, K. (1997). Differential Evolution – A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces. *Journal of Global Optimization*, 11(4), 341-359.\n",
    "\n",
    "2. Das, S., & Suganthan, P. N. (2011). Differential Evolution: A Survey of the State-of-the-Art. *IEEE Transactions on Evolutionary Computation*, 15(1), 4-31.\n",
    "\n",
    "3. Price, K., Storn, R., & Lampinen, J. (2005). *Differential Evolution: A Practical Approach to Global Optimization*. Springer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
