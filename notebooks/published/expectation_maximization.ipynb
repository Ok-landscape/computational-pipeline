{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization Algorithm\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **Expectation Maximization (EM)** algorithm is a powerful iterative method for finding maximum likelihood estimates of parameters in statistical models with latent (hidden) variables. It was formalized by Dempster, Laird, and Rubin in 1977 and has become a cornerstone of unsupervised learning and statistical inference.\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "Consider observed data $\\mathbf{X} = \\{x_1, x_2, \\ldots, x_n\\}$ and latent variables $\\mathbf{Z} = \\{z_1, z_2, \\ldots, z_n\\}$. We seek to maximize the log-likelihood:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\log p(\\mathbf{X} | \\theta) = \\log \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} | \\theta)$$\n",
    "\n",
    "Direct maximization is often intractable due to the summation inside the logarithm. The EM algorithm circumvents this by iteratively optimizing a lower bound.\n",
    "\n",
    "## The EM Algorithm\n",
    "\n",
    "The algorithm alternates between two steps:\n",
    "\n",
    "### E-Step (Expectation)\n",
    "\n",
    "Compute the expected value of the complete-data log-likelihood with respect to the conditional distribution of $\\mathbf{Z}$ given $\\mathbf{X}$ and current parameters $\\theta^{(t)}$:\n",
    "\n",
    "$$Q(\\theta | \\theta^{(t)}) = \\mathbb{E}_{\\mathbf{Z} | \\mathbf{X}, \\theta^{(t)}} \\left[ \\log p(\\mathbf{X}, \\mathbf{Z} | \\theta) \\right]$$\n",
    "\n",
    "### M-Step (Maximization)\n",
    "\n",
    "Find the parameters that maximize the Q-function:\n",
    "\n",
    "$$\\theta^{(t+1)} = \\arg\\max_{\\theta} Q(\\theta | \\theta^{(t)})$$\n",
    "\n",
    "## Convergence Guarantee\n",
    "\n",
    "A fundamental property of EM is that each iteration monotonically increases the log-likelihood:\n",
    "\n",
    "$$\\mathcal{L}(\\theta^{(t+1)}) \\geq \\mathcal{L}(\\theta^{(t)})$$\n",
    "\n",
    "This follows from Jensen's inequality and ensures convergence to a local maximum (or saddle point).\n",
    "\n",
    "## Application: Gaussian Mixture Models\n",
    "\n",
    "We demonstrate EM on a **Gaussian Mixture Model (GMM)**, where data is assumed to arise from $K$ Gaussian components:\n",
    "\n",
    "$$p(x | \\theta) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "where:\n",
    "- $\\pi_k$ are mixing coefficients with $\\sum_k \\pi_k = 1$\n",
    "- $\\mu_k$ and $\\Sigma_k$ are the mean and covariance of component $k$\n",
    "\n",
    "### GMM E-Step\n",
    "\n",
    "Compute responsibilities (posterior probability that point $x_i$ belongs to component $k$):\n",
    "\n",
    "$$\\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_i | \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "### GMM M-Step\n",
    "\n",
    "Update parameters using weighted statistics:\n",
    "\n",
    "$$N_k = \\sum_{i=1}^{n} \\gamma_{ik}$$\n",
    "\n",
    "$$\\mu_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{i=1}^{n} \\gamma_{ik} x_i$$\n",
    "\n",
    "$$\\Sigma_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{i=1}^{n} \\gamma_{ik} (x_i - \\mu_k^{\\text{new}})(x_i - \\mu_k^{\\text{new}})^T$$\n",
    "\n",
    "$$\\pi_k^{\\text{new}} = \\frac{N_k}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We generate synthetic data from a mixture of three 2D Gaussians with known parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters for data generation\n",
    "true_means = np.array([\n",
    "    [0, 0],\n",
    "    [5, 5],\n",
    "    [0, 5]\n",
    "])\n",
    "\n",
    "true_covs = np.array([\n",
    "    [[1, 0.5], [0.5, 1]],\n",
    "    [[1.5, -0.3], [-0.3, 1]],\n",
    "    [[0.8, 0], [0, 1.2]]\n",
    "])\n",
    "\n",
    "true_weights = np.array([0.3, 0.4, 0.3])\n",
    "\n",
    "# Generate samples\n",
    "n_samples = 500\n",
    "K = 3\n",
    "\n",
    "# Determine component assignments\n",
    "component_assignments = np.random.choice(K, size=n_samples, p=true_weights)\n",
    "\n",
    "# Generate data points\n",
    "X = np.zeros((n_samples, 2))\n",
    "for i in range(n_samples):\n",
    "    k = component_assignments[i]\n",
    "    X[i] = np.random.multivariate_normal(true_means[k], true_covs[k])\n",
    "\n",
    "print(f\"Generated {n_samples} samples from {K} Gaussian components\")\n",
    "print(f\"True mixing weights: {true_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X, K):\n",
    "    \"\"\"Initialize GMM parameters using random selection.\"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Random initialization of means from data points\n",
    "    indices = np.random.choice(n, K, replace=False)\n",
    "    means = X[indices].copy()\n",
    "    \n",
    "    # Initialize covariances as identity matrices\n",
    "    covs = np.array([np.eye(d) for _ in range(K)])\n",
    "    \n",
    "    # Uniform weights\n",
    "    weights = np.ones(K) / K\n",
    "    \n",
    "    return means, covs, weights\n",
    "\n",
    "\n",
    "def e_step(X, means, covs, weights):\n",
    "    \"\"\"E-Step: Compute responsibilities.\"\"\"\n",
    "    n = X.shape[0]\n",
    "    K = len(weights)\n",
    "    \n",
    "    # Compute probability density for each component\n",
    "    responsibilities = np.zeros((n, K))\n",
    "    \n",
    "    for k in range(K):\n",
    "        rv = multivariate_normal(mean=means[k], cov=covs[k])\n",
    "        responsibilities[:, k] = weights[k] * rv.pdf(X)\n",
    "    \n",
    "    # Normalize to get posterior probabilities\n",
    "    responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return responsibilities\n",
    "\n",
    "\n",
    "def m_step(X, responsibilities):\n",
    "    \"\"\"M-Step: Update parameters.\"\"\"\n",
    "    n, d = X.shape\n",
    "    K = responsibilities.shape[1]\n",
    "    \n",
    "    # Effective number of points per component\n",
    "    N_k = responsibilities.sum(axis=0)\n",
    "    \n",
    "    # Update weights\n",
    "    weights = N_k / n\n",
    "    \n",
    "    # Update means\n",
    "    means = np.zeros((K, d))\n",
    "    for k in range(K):\n",
    "        means[k] = (responsibilities[:, k:k+1].T @ X) / N_k[k]\n",
    "    \n",
    "    # Update covariances\n",
    "    covs = np.zeros((K, d, d))\n",
    "    for k in range(K):\n",
    "        diff = X - means[k]\n",
    "        covs[k] = (responsibilities[:, k:k+1] * diff).T @ diff / N_k[k]\n",
    "        # Add small regularization for numerical stability\n",
    "        covs[k] += 1e-6 * np.eye(d)\n",
    "    \n",
    "    return means, covs, weights\n",
    "\n",
    "\n",
    "def compute_log_likelihood(X, means, covs, weights):\n",
    "    \"\"\"Compute the log-likelihood of the data.\"\"\"\n",
    "    n = X.shape[0]\n",
    "    K = len(weights)\n",
    "    \n",
    "    likelihood = np.zeros(n)\n",
    "    for k in range(K):\n",
    "        rv = multivariate_normal(mean=means[k], cov=covs[k])\n",
    "        likelihood += weights[k] * rv.pdf(X)\n",
    "    \n",
    "    return np.sum(np.log(likelihood))\n",
    "\n",
    "\n",
    "def em_gmm(X, K, max_iters=100, tol=1e-6):\n",
    "    \"\"\"Run EM algorithm for Gaussian Mixture Model.\"\"\"\n",
    "    # Initialize parameters\n",
    "    means, covs, weights = initialize_parameters(X, K)\n",
    "    \n",
    "    log_likelihoods = []\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        # E-step\n",
    "        responsibilities = e_step(X, means, covs, weights)\n",
    "        \n",
    "        # M-step\n",
    "        means, covs, weights = m_step(X, responsibilities)\n",
    "        \n",
    "        # Compute log-likelihood\n",
    "        ll = compute_log_likelihood(X, means, covs, weights)\n",
    "        log_likelihoods.append(ll)\n",
    "        \n",
    "        # Check convergence\n",
    "        if iteration > 0 and abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
    "            print(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    return means, covs, weights, responsibilities, log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the EM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EM algorithm\n",
    "means, covs, weights, responsibilities, log_likelihoods = em_gmm(X, K, max_iters=100)\n",
    "\n",
    "print(\"\\nEstimated Parameters:\")\n",
    "print(\"=\"*50)\n",
    "for k in range(K):\n",
    "    print(f\"\\nComponent {k+1}:\")\n",
    "    print(f\"  Weight: {weights[k]:.3f}\")\n",
    "    print(f\"  Mean: [{means[k, 0]:.3f}, {means[k, 1]:.3f}]\")\n",
    "    print(f\"  Covariance diagonal: [{covs[k, 0, 0]:.3f}, {covs[k, 1, 1]:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We visualize the clustering results and the convergence of the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ellipse(mean, cov, ax, n_std=2, **kwargs):\n",
    "    \"\"\"Plot an ellipse representing a Gaussian component.\"\"\"\n",
    "    # Eigendecomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "    \n",
    "    # Angle of rotation\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "    \n",
    "    # Width and height (n_std standard deviations)\n",
    "    width, height = 2 * n_std * np.sqrt(eigenvalues)\n",
    "    \n",
    "    from matplotlib.patches import Ellipse\n",
    "    ellipse = Ellipse(xy=mean, width=width, height=height, angle=angle, **kwargs)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Data with true labels\n",
    "ax1 = axes[0]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "for k in range(K):\n",
    "    mask = component_assignments == k\n",
    "    ax1.scatter(X[mask, 0], X[mask, 1], c=colors[k], alpha=0.5, s=20, label=f'True {k+1}')\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_title('True Component Assignments')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: EM clustering results\n",
    "ax2 = axes[1]\n",
    "cluster_assignments = np.argmax(responsibilities, axis=1)\n",
    "for k in range(K):\n",
    "    mask = cluster_assignments == k\n",
    "    ax2.scatter(X[mask, 0], X[mask, 1], c=colors[k], alpha=0.5, s=20, label=f'Cluster {k+1}')\n",
    "    # Plot estimated mean\n",
    "    ax2.scatter(means[k, 0], means[k, 1], c='black', marker='x', s=200, linewidths=3)\n",
    "    # Plot covariance ellipse\n",
    "    plot_ellipse(means[k], covs[k], ax2, n_std=2, fill=False, \n",
    "                edgecolor=colors[k], linewidth=2)\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_title('EM Clustering Results')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Log-likelihood convergence\n",
    "ax3 = axes[2]\n",
    "ax3.plot(log_likelihoods, 'b-', linewidth=2)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Log-Likelihood')\n",
    "ax3.set_title('EM Convergence')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Discussion\n",
    "\n",
    "### Convergence Properties\n",
    "\n",
    "The log-likelihood plot demonstrates the monotonic increase guaranteed by the EM algorithm. The algorithm typically converges within 20-50 iterations for well-separated clusters.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Local Optima**: EM only guarantees convergence to a local maximum. Multiple random restarts are recommended in practice.\n",
    "\n",
    "2. **Initialization Sensitivity**: Poor initialization can lead to slow convergence or degenerate solutions.\n",
    "\n",
    "3. **Model Selection**: The number of components $K$ must be specified a priori. Techniques like BIC/AIC or cross-validation can help.\n",
    "\n",
    "### Extensions\n",
    "\n",
    "- **Online EM**: For streaming data\n",
    "- **Variational EM**: Incorporates Bayesian priors\n",
    "- **Regularized EM**: Prevents degenerate covariances\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Expectation Maximization algorithm provides an elegant and principled approach to parameter estimation with latent variables. Its application to Gaussian Mixture Models demonstrates how complex probability distributions can be learned from unlabeled data through iterative optimization of a tractable lower bound."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
