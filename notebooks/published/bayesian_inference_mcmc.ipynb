{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference with Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Bayesian inference provides a principled framework for updating our beliefs about unknown parameters in light of observed data. Unlike frequentist approaches that treat parameters as fixed but unknown quantities, Bayesian inference treats parameters as random variables with associated probability distributions.\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "The cornerstone of Bayesian inference is **Bayes' theorem**, which relates the posterior distribution to the prior and likelihood:\n",
    "\n",
    "$$P(\\theta | D) = \\frac{P(D | \\theta) \\cdot P(\\theta)}{P(D)}$$\n",
    "\n",
    "where:\n",
    "- $P(\\theta | D)$ is the **posterior distribution** — our updated belief about parameter $\\theta$ after observing data $D$\n",
    "- $P(D | \\theta)$ is the **likelihood** — the probability of observing the data given the parameter\n",
    "- $P(\\theta)$ is the **prior distribution** — our initial belief about the parameter before seeing data\n",
    "- $P(D) = \\int P(D | \\theta) P(\\theta) d\\theta$ is the **marginal likelihood** or **evidence**\n",
    "\n",
    "### The Computational Challenge\n",
    "\n",
    "The marginal likelihood $P(D)$ often involves intractable integrals, especially in high-dimensional parameter spaces:\n",
    "\n",
    "$$P(D) = \\int_{\\Theta} P(D | \\theta) P(\\theta) d\\theta$$\n",
    "\n",
    "This is where **Markov Chain Monte Carlo (MCMC)** methods become essential — they allow us to sample from the posterior distribution without explicitly computing the normalizing constant.\n",
    "\n",
    "## MCMC: The Metropolis-Hastings Algorithm\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "MCMC constructs a Markov chain whose stationary distribution is the target posterior distribution $\\pi(\\theta) = P(\\theta | D)$. The **Metropolis-Hastings algorithm** is a general-purpose MCMC method.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Given current state $\\theta_t$:\n",
    "\n",
    "1. **Propose** a candidate state $\\theta^*$ from proposal distribution $q(\\theta^* | \\theta_t)$\n",
    "\n",
    "2. **Compute** the acceptance probability:\n",
    "   $$\\alpha = \\min\\left(1, \\frac{\\pi(\\theta^*) q(\\theta_t | \\theta^*)}{\\pi(\\theta_t) q(\\theta^* | \\theta_t)}\\right)$$\n",
    "\n",
    "3. **Accept or Reject**:\n",
    "   - Draw $u \\sim \\text{Uniform}(0, 1)$\n",
    "   - If $u < \\alpha$, set $\\theta_{t+1} = \\theta^*$ (accept)\n",
    "   - Otherwise, set $\\theta_{t+1} = \\theta_t$ (reject)\n",
    "\n",
    "### Random Walk Metropolis\n",
    "\n",
    "A common choice is the symmetric random walk proposal:\n",
    "\n",
    "$$\\theta^* = \\theta_t + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "Since $q(\\theta^* | \\theta_t) = q(\\theta_t | \\theta^*)$, the acceptance ratio simplifies to:\n",
    "\n",
    "$$\\alpha = \\min\\left(1, \\frac{\\pi(\\theta^*)}{\\pi(\\theta_t)}\\right)$$\n",
    "\n",
    "### Convergence Properties\n",
    "\n",
    "Under mild regularity conditions, the Markov chain satisfies:\n",
    "- **Irreducibility**: Any state can be reached from any other state\n",
    "- **Aperiodicity**: The chain does not cycle deterministically\n",
    "- **Ergodicity**: Time averages converge to ensemble averages\n",
    "\n",
    "$$\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{t=1}^{N} f(\\theta_t) = \\mathbb{E}_{\\pi}[f(\\theta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Inferring Parameters of a Normal Distribution\n",
    "\n",
    "We will demonstrate Bayesian inference by estimating the mean $\\mu$ and standard deviation $\\sigma$ of a normal distribution from observed data.\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "**Likelihood:**\n",
    "$$X_i | \\mu, \\sigma \\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad i = 1, \\ldots, n$$\n",
    "\n",
    "**Priors:**\n",
    "$$\\mu \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$$\n",
    "$$\\sigma \\sim \\text{Half-Normal}(s)$$\n",
    "\n",
    "**Joint Likelihood:**\n",
    "$$P(D | \\mu, \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "**Log-posterior (up to a constant):**\n",
    "$$\\log P(\\mu, \\sigma | D) \\propto -\\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2 - \\frac{(\\mu - \\mu_0)^2}{2\\tau_0^2} - \\frac{\\sigma^2}{2s^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic observed data\n",
    "true_mu = 5.0\n",
    "true_sigma = 2.0\n",
    "n_observations = 100\n",
    "\n",
    "data = np.random.normal(true_mu, true_sigma, n_observations)\n",
    "\n",
    "print(f\"True parameters: μ = {true_mu}, σ = {true_sigma}\")\n",
    "print(f\"Sample mean: {data.mean():.3f}\")\n",
    "print(f\"Sample std: {data.std():.3f}\")\n",
    "print(f\"Number of observations: {n_observations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior hyperparameters\n",
    "mu_0 = 0.0       # Prior mean for μ\n",
    "tau_0 = 10.0     # Prior std for μ (weakly informative)\n",
    "s = 10.0         # Scale for half-normal prior on σ\n",
    "\n",
    "def log_prior(mu, sigma):\n",
    "    \"\"\"\n",
    "    Compute log of the prior distribution.\n",
    "    Prior on μ: Normal(mu_0, tau_0^2)\n",
    "    Prior on σ: Half-Normal(s) - only positive values\n",
    "    \"\"\"\n",
    "    if sigma <= 0:\n",
    "        return -np.inf\n",
    "    \n",
    "    # Log prior for mu (Normal)\n",
    "    log_prior_mu = -0.5 * ((mu - mu_0) / tau_0) ** 2\n",
    "    \n",
    "    # Log prior for sigma (Half-Normal)\n",
    "    log_prior_sigma = -0.5 * (sigma / s) ** 2\n",
    "    \n",
    "    return log_prior_mu + log_prior_sigma\n",
    "\n",
    "def log_likelihood(mu, sigma, data):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood for Normal distribution.\n",
    "    \"\"\"\n",
    "    if sigma <= 0:\n",
    "        return -np.inf\n",
    "    \n",
    "    n = len(data)\n",
    "    residuals = data - mu\n",
    "    \n",
    "    ll = -n * np.log(sigma) - 0.5 * np.sum(residuals**2) / sigma**2\n",
    "    return ll\n",
    "\n",
    "def log_posterior(mu, sigma, data):\n",
    "    \"\"\"\n",
    "    Compute unnormalized log-posterior.\n",
    "    \"\"\"\n",
    "    lp = log_prior(mu, sigma)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(mu, sigma, data)\n",
    "\n",
    "print(\"Log-posterior functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(log_posterior_fn, data, initial_params, \n",
    "                        n_iterations, proposal_std):\n",
    "    \"\"\"\n",
    "    Metropolis-Hastings MCMC sampler for 2D parameter space.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    log_posterior_fn : callable\n",
    "        Function that computes log-posterior given (mu, sigma, data)\n",
    "    data : array-like\n",
    "        Observed data\n",
    "    initial_params : tuple\n",
    "        Starting values (mu_init, sigma_init)\n",
    "    n_iterations : int\n",
    "        Number of MCMC iterations\n",
    "    proposal_std : tuple\n",
    "        Standard deviations for proposals (std_mu, std_sigma)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : ndarray of shape (n_iterations, 2)\n",
    "        MCMC samples [mu, sigma]\n",
    "    acceptance_rate : float\n",
    "        Fraction of accepted proposals\n",
    "    \"\"\"\n",
    "    mu_current, sigma_current = initial_params\n",
    "    std_mu, std_sigma = proposal_std\n",
    "    \n",
    "    samples = np.zeros((n_iterations, 2))\n",
    "    n_accepted = 0\n",
    "    \n",
    "    current_log_post = log_posterior_fn(mu_current, sigma_current, data)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Propose new parameters (random walk)\n",
    "        mu_proposed = mu_current + np.random.normal(0, std_mu)\n",
    "        sigma_proposed = sigma_current + np.random.normal(0, std_sigma)\n",
    "        \n",
    "        # Compute log-posterior for proposed parameters\n",
    "        proposed_log_post = log_posterior_fn(mu_proposed, sigma_proposed, data)\n",
    "        \n",
    "        # Compute acceptance probability (in log space)\n",
    "        log_alpha = proposed_log_post - current_log_post\n",
    "        \n",
    "        # Accept or reject\n",
    "        if np.log(np.random.random()) < log_alpha:\n",
    "            mu_current = mu_proposed\n",
    "            sigma_current = sigma_proposed\n",
    "            current_log_post = proposed_log_post\n",
    "            n_accepted += 1\n",
    "        \n",
    "        samples[i] = [mu_current, sigma_current]\n",
    "    \n",
    "    acceptance_rate = n_accepted / n_iterations\n",
    "    return samples, acceptance_rate\n",
    "\n",
    "print(\"Metropolis-Hastings sampler implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC Configuration\n",
    "n_iterations = 50000\n",
    "burn_in = 10000\n",
    "initial_params = (0.0, 1.0)  # Starting values\n",
    "proposal_std = (0.2, 0.1)    # Proposal standard deviations\n",
    "\n",
    "print(f\"Running Metropolis-Hastings with {n_iterations} iterations...\")\n",
    "print(f\"Burn-in period: {burn_in}\")\n",
    "print(f\"Initial parameters: μ = {initial_params[0]}, σ = {initial_params[1]}\")\n",
    "print(f\"Proposal std: μ ~ N(0, {proposal_std[0]}), σ ~ N(0, {proposal_std[1]})\")\n",
    "\n",
    "# Run MCMC\n",
    "samples, acceptance_rate = metropolis_hastings(\n",
    "    log_posterior, data, initial_params, n_iterations, proposal_std\n",
    ")\n",
    "\n",
    "print(f\"\\nAcceptance rate: {acceptance_rate:.3f}\")\n",
    "print(\"(Optimal acceptance rate for 2D: ~0.234)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard burn-in samples\n",
    "samples_post_burnin = samples[burn_in:]\n",
    "\n",
    "mu_samples = samples_post_burnin[:, 0]\n",
    "sigma_samples = samples_post_burnin[:, 1]\n",
    "\n",
    "# Compute posterior statistics\n",
    "mu_mean = np.mean(mu_samples)\n",
    "mu_std = np.std(mu_samples)\n",
    "mu_ci = np.percentile(mu_samples, [2.5, 97.5])\n",
    "\n",
    "sigma_mean = np.mean(sigma_samples)\n",
    "sigma_std = np.std(sigma_samples)\n",
    "sigma_ci = np.percentile(sigma_samples, [2.5, 97.5])\n",
    "\n",
    "print(\"Posterior Summary Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nParameter μ:\")\n",
    "print(f\"  Posterior mean: {mu_mean:.4f}\")\n",
    "print(f\"  Posterior std:  {mu_std:.4f}\")\n",
    "print(f\"  95% CI: [{mu_ci[0]:.4f}, {mu_ci[1]:.4f}]\")\n",
    "print(f\"  True value: {true_mu}\")\n",
    "\n",
    "print(f\"\\nParameter σ:\")\n",
    "print(f\"  Posterior mean: {sigma_mean:.4f}\")\n",
    "print(f\"  Posterior std:  {sigma_std:.4f}\")\n",
    "print(f\"  95% CI: [{sigma_ci[0]:.4f}, {sigma_ci[1]:.4f}]\")\n",
    "print(f\"  True value: {true_sigma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 1. Trace plot for μ\n",
    "ax1 = fig.add_subplot(3, 2, 1)\n",
    "ax1.plot(samples[:, 0], alpha=0.7, linewidth=0.5)\n",
    "ax1.axhline(true_mu, color='red', linestyle='--', linewidth=2, label=f'True μ = {true_mu}')\n",
    "ax1.axvline(burn_in, color='orange', linestyle=':', linewidth=2, label=f'Burn-in = {burn_in}')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('μ')\n",
    "ax1.set_title('Trace Plot: Mean (μ)')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# 2. Trace plot for σ\n",
    "ax2 = fig.add_subplot(3, 2, 2)\n",
    "ax2.plot(samples[:, 1], alpha=0.7, linewidth=0.5, color='green')\n",
    "ax2.axhline(true_sigma, color='red', linestyle='--', linewidth=2, label=f'True σ = {true_sigma}')\n",
    "ax2.axvline(burn_in, color='orange', linestyle=':', linewidth=2, label=f'Burn-in = {burn_in}')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('σ')\n",
    "ax2.set_title('Trace Plot: Standard Deviation (σ)')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# 3. Posterior histogram for μ\n",
    "ax3 = fig.add_subplot(3, 2, 3)\n",
    "ax3.hist(mu_samples, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax3.axvline(true_mu, color='red', linestyle='--', linewidth=2, label=f'True μ = {true_mu}')\n",
    "ax3.axvline(mu_mean, color='navy', linestyle='-', linewidth=2, label=f'Posterior mean = {mu_mean:.3f}')\n",
    "ax3.axvline(mu_ci[0], color='gray', linestyle=':', linewidth=1.5)\n",
    "ax3.axvline(mu_ci[1], color='gray', linestyle=':', linewidth=1.5, label='95% CI')\n",
    "ax3.set_xlabel('μ')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Posterior Distribution of μ')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Posterior histogram for σ\n",
    "ax4 = fig.add_subplot(3, 2, 4)\n",
    "ax4.hist(sigma_samples, bins=50, density=True, alpha=0.7, color='seagreen', edgecolor='black')\n",
    "ax4.axvline(true_sigma, color='red', linestyle='--', linewidth=2, label=f'True σ = {true_sigma}')\n",
    "ax4.axvline(sigma_mean, color='darkgreen', linestyle='-', linewidth=2, label=f'Posterior mean = {sigma_mean:.3f}')\n",
    "ax4.axvline(sigma_ci[0], color='gray', linestyle=':', linewidth=1.5)\n",
    "ax4.axvline(sigma_ci[1], color='gray', linestyle=':', linewidth=1.5, label='95% CI')\n",
    "ax4.set_xlabel('σ')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Posterior Distribution of σ')\n",
    "ax4.legend()\n",
    "\n",
    "# 5. Joint posterior (2D scatter/density)\n",
    "ax5 = fig.add_subplot(3, 2, 5)\n",
    "# Subsample for visualization\n",
    "subsample_idx = np.random.choice(len(mu_samples), size=min(5000, len(mu_samples)), replace=False)\n",
    "ax5.scatter(mu_samples[subsample_idx], sigma_samples[subsample_idx], \n",
    "           alpha=0.2, s=1, c='purple')\n",
    "ax5.scatter([true_mu], [true_sigma], color='red', s=100, marker='*', \n",
    "           edgecolors='black', linewidths=1, label='True values', zorder=5)\n",
    "ax5.scatter([mu_mean], [sigma_mean], color='yellow', s=100, marker='o',\n",
    "           edgecolors='black', linewidths=1, label='Posterior mean', zorder=5)\n",
    "ax5.set_xlabel('μ')\n",
    "ax5.set_ylabel('σ')\n",
    "ax5.set_title('Joint Posterior Distribution')\n",
    "ax5.legend(loc='upper right')\n",
    "\n",
    "# 6. Autocorrelation plot\n",
    "ax6 = fig.add_subplot(3, 2, 6)\n",
    "max_lag = 100\n",
    "lags = np.arange(max_lag)\n",
    "acf_mu = [np.corrcoef(mu_samples[:-lag] if lag > 0 else mu_samples, \n",
    "                       mu_samples[lag:] if lag > 0 else mu_samples)[0, 1] \n",
    "          for lag in lags]\n",
    "acf_sigma = [np.corrcoef(sigma_samples[:-lag] if lag > 0 else sigma_samples,\n",
    "                          sigma_samples[lag:] if lag > 0 else sigma_samples)[0, 1]\n",
    "             for lag in lags]\n",
    "\n",
    "ax6.plot(lags, acf_mu, label='μ', linewidth=2)\n",
    "ax6.plot(lags, acf_sigma, label='σ', linewidth=2)\n",
    "ax6.axhline(0, color='black', linewidth=0.5)\n",
    "ax6.axhline(0.05, color='gray', linestyle='--', linewidth=1, alpha=0.7)\n",
    "ax6.axhline(-0.05, color='gray', linestyle='--', linewidth=1, alpha=0.7)\n",
    "ax6.set_xlabel('Lag')\n",
    "ax6.set_ylabel('Autocorrelation')\n",
    "ax6.set_title('Autocorrelation Function')\n",
    "ax6.legend()\n",
    "ax6.set_xlim(0, max_lag)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics and Convergence Assessment\n",
    "\n",
    "### Effective Sample Size (ESS)\n",
    "\n",
    "The effective sample size accounts for autocorrelation in the MCMC chain:\n",
    "\n",
    "$$\\text{ESS} = \\frac{N}{1 + 2\\sum_{k=1}^{\\infty} \\rho_k}$$\n",
    "\n",
    "where $\\rho_k$ is the autocorrelation at lag $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effective_sample_size(samples, max_lag=None):\n",
    "    \"\"\"\n",
    "    Estimate the effective sample size of MCMC samples.\n",
    "    Uses the initial positive sequence estimator.\n",
    "    \"\"\"\n",
    "    n = len(samples)\n",
    "    if max_lag is None:\n",
    "        max_lag = n // 2\n",
    "    \n",
    "    # Compute autocorrelations\n",
    "    samples_centered = samples - np.mean(samples)\n",
    "    acf = np.correlate(samples_centered, samples_centered, mode='full')\n",
    "    acf = acf[n-1:n-1+max_lag] / acf[n-1]\n",
    "    \n",
    "    # Find first negative autocorrelation (initial positive sequence)\n",
    "    negative_idx = np.where(acf < 0)[0]\n",
    "    if len(negative_idx) > 0:\n",
    "        cutoff = negative_idx[0]\n",
    "    else:\n",
    "        cutoff = max_lag\n",
    "    \n",
    "    # ESS estimate\n",
    "    tau = 1 + 2 * np.sum(acf[1:cutoff])\n",
    "    ess = n / tau\n",
    "    \n",
    "    return ess\n",
    "\n",
    "ess_mu = effective_sample_size(mu_samples)\n",
    "ess_sigma = effective_sample_size(sigma_samples)\n",
    "\n",
    "print(\"Effective Sample Size (ESS)\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total post burn-in samples: {len(mu_samples)}\")\n",
    "print(f\"ESS for μ: {ess_mu:.1f}\")\n",
    "print(f\"ESS for σ: {ess_sigma:.1f}\")\n",
    "print(f\"\\nEfficiency (ESS/N):\")\n",
    "print(f\"  μ: {ess_mu/len(mu_samples)*100:.1f}%\")\n",
    "print(f\"  σ: {ess_sigma/len(sigma_samples)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive Check\n",
    "\n",
    "A crucial validation step is comparing the posterior predictive distribution with the observed data:\n",
    "\n",
    "$$p(\\tilde{x} | D) = \\int p(\\tilde{x} | \\theta) p(\\theta | D) d\\theta$$\n",
    "\n",
    "We approximate this by sampling from the posterior and generating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive sampling\n",
    "n_posterior_samples = 1000\n",
    "n_predictions = 100\n",
    "\n",
    "# Randomly select posterior samples\n",
    "idx = np.random.choice(len(mu_samples), size=n_posterior_samples, replace=False)\n",
    "posterior_predictive = np.zeros((n_posterior_samples, n_predictions))\n",
    "\n",
    "for i, j in enumerate(idx):\n",
    "    posterior_predictive[i] = np.random.normal(mu_samples[j], sigma_samples[j], n_predictions)\n",
    "\n",
    "# Flatten for comparison\n",
    "pp_samples = posterior_predictive.flatten()\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.hist(data, bins=30, density=True, alpha=0.6, color='steelblue', \n",
    "        edgecolor='black', label='Observed Data')\n",
    "ax.hist(pp_samples, bins=50, density=True, alpha=0.4, color='orange',\n",
    "        edgecolor='darkorange', label='Posterior Predictive')\n",
    "\n",
    "# True distribution\n",
    "x_grid = np.linspace(data.min() - 2, data.max() + 2, 200)\n",
    "ax.plot(x_grid, stats.norm.pdf(x_grid, true_mu, true_sigma), \n",
    "       'r--', linewidth=2, label='True Distribution')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Posterior Predictive Check')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Posterior predictive check complete.\")\n",
    "print(f\"Observed data mean: {data.mean():.3f}, Posterior predictive mean: {pp_samples.mean():.3f}\")\n",
    "print(f\"Observed data std: {data.std():.3f}, Posterior predictive std: {pp_samples.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the fundamentals of **Bayesian inference using Markov Chain Monte Carlo (MCMC)**:\n",
    "\n",
    "1. **Bayes' theorem** provides the theoretical foundation for updating beliefs given data\n",
    "2. **Metropolis-Hastings** algorithm enables sampling from complex posterior distributions\n",
    "3. **Convergence diagnostics** (trace plots, ESS, autocorrelation) ensure reliable inference\n",
    "4. **Posterior predictive checks** validate model fit\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- MCMC methods bypass the need to compute intractable normalizing constants\n",
    "- Proposal distribution tuning affects acceptance rate and mixing efficiency\n",
    "- Burn-in period is essential to discard samples before convergence\n",
    "- The posterior distribution fully characterizes parameter uncertainty\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Gelman, A., et al. (2013). *Bayesian Data Analysis*, 3rd ed.\n",
    "- Brooks, S., et al. (2011). *Handbook of Markov Chain Monte Carlo*\n",
    "- Robert, C. & Casella, G. (2004). *Monte Carlo Statistical Methods*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
