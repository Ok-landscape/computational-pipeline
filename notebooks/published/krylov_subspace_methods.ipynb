{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krylov Subspace Methods for Iterative Linear System Solving\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Krylov subspace methods are a class of iterative algorithms for solving large sparse linear systems of the form $A\\mathbf{x} = \\mathbf{b}$, where $A \\in \\mathbb{R}^{n \\times n}$ is a large sparse matrix and $\\mathbf{b} \\in \\mathbb{R}^n$ is the right-hand side vector. These methods are fundamental to computational science, finding applications in numerical PDEs, optimization, machine learning, and scientific computing.\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### Definition of Krylov Subspace\n",
    "\n",
    "Given a matrix $A$ and vector $\\mathbf{v}$, the **Krylov subspace** of dimension $k$ is defined as:\n",
    "\n",
    "$$\\mathcal{K}_k(A, \\mathbf{v}) = \\text{span}\\{\\mathbf{v}, A\\mathbf{v}, A^2\\mathbf{v}, \\ldots, A^{k-1}\\mathbf{v}\\}$$\n",
    "\n",
    "This subspace captures how the matrix $A$ acts on the initial vector through successive multiplications. The key insight is that good approximations to the solution often lie within low-dimensional Krylov subspaces.\n",
    "\n",
    "### Why Krylov Methods Work\n",
    "\n",
    "The power of Krylov methods stems from the **Cayley-Hamilton theorem**: every matrix satisfies its own characteristic polynomial. For a matrix $A$ with characteristic polynomial $p(\\lambda) = \\det(\\lambda I - A)$, we have $p(A) = 0$. This means:\n",
    "\n",
    "$$A^{-1} = -\\frac{1}{p(0)}\\left(A^{n-1} + c_{n-2}A^{n-2} + \\ldots + c_1 A + c_0 I\\right)$$\n",
    "\n",
    "Therefore, $A^{-1}\\mathbf{b} \\in \\mathcal{K}_n(A, \\mathbf{b})$, meaning the exact solution lies in the $n$-dimensional Krylov subspace.\n",
    "\n",
    "## Key Krylov Methods\n",
    "\n",
    "### 1. Conjugate Gradient (CG) Method\n",
    "\n",
    "For **symmetric positive definite (SPD)** matrices, CG minimizes the $A$-norm of the error:\n",
    "\n",
    "$$\\|\\mathbf{e}_k\\|_A = \\|\\mathbf{x} - \\mathbf{x}_k\\|_A = \\sqrt{(\\mathbf{x} - \\mathbf{x}_k)^T A (\\mathbf{x} - \\mathbf{x}_k)}$$\n",
    "\n",
    "The algorithm generates $A$-orthogonal (conjugate) search directions $\\mathbf{p}_k$:\n",
    "\n",
    "$$\\mathbf{p}_i^T A \\mathbf{p}_j = 0 \\quad \\text{for } i \\neq j$$\n",
    "\n",
    "**Convergence rate**: \n",
    "$$\\|\\mathbf{e}_k\\|_A \\leq 2\\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^k \\|\\mathbf{e}_0\\|_A$$\n",
    "\n",
    "where $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ is the condition number.\n",
    "\n",
    "### 2. GMRES (Generalized Minimal Residual)\n",
    "\n",
    "For **general non-symmetric** matrices, GMRES minimizes the 2-norm of the residual:\n",
    "\n",
    "$$\\mathbf{x}_k = \\arg\\min_{\\mathbf{x} \\in \\mathcal{K}_k(A, \\mathbf{r}_0)} \\|\\mathbf{b} - A\\mathbf{x}\\|_2$$\n",
    "\n",
    "GMRES uses the **Arnoldi iteration** to build an orthonormal basis $\\{\\mathbf{q}_1, \\ldots, \\mathbf{q}_k\\}$ for $\\mathcal{K}_k(A, \\mathbf{r}_0)$.\n",
    "\n",
    "### 3. BiCGSTAB (Bi-Conjugate Gradient Stabilized)\n",
    "\n",
    "A stabilized variant of the bi-conjugate gradient method for non-symmetric systems, combining bi-CG with GMRES(1) to smooth convergence.\n",
    "\n",
    "## The Arnoldi Process\n",
    "\n",
    "The Arnoldi iteration constructs an orthonormal basis for the Krylov subspace:\n",
    "\n",
    "$$AQ_k = Q_{k+1}\\tilde{H}_k$$\n",
    "\n",
    "where $Q_k = [\\mathbf{q}_1, \\ldots, \\mathbf{q}_k]$ and $\\tilde{H}_k$ is an upper Hessenberg matrix.\n",
    "\n",
    "For symmetric $A$, this reduces to the **Lanczos iteration** with a tridiagonal matrix $T_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags, csr_matrix\n",
    "from scipy.sparse.linalg import cg, gmres, bicgstab\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully.\")\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Conjugate Gradient Method\n",
    "\n",
    "We implement CG from scratch to understand its mechanics:\n",
    "\n",
    "**Algorithm:**\n",
    "1. Initialize: $\\mathbf{r}_0 = \\mathbf{b} - A\\mathbf{x}_0$, $\\mathbf{p}_0 = \\mathbf{r}_0$\n",
    "2. For $k = 0, 1, 2, \\ldots$:\n",
    "   - $\\alpha_k = \\frac{\\mathbf{r}_k^T \\mathbf{r}_k}{\\mathbf{p}_k^T A \\mathbf{p}_k}$\n",
    "   - $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$\n",
    "   - $\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k A \\mathbf{p}_k$\n",
    "   - $\\beta_k = \\frac{\\mathbf{r}_{k+1}^T \\mathbf{r}_{k+1}}{\\mathbf{r}_k^T \\mathbf{r}_k}$\n",
    "   - $\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b, x0=None, tol=1e-10, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Conjugate Gradient method for solving Ax = b.\n",
    "    \n",
    "    Parameters:\n",
    "        A: Symmetric positive definite matrix (can be sparse)\n",
    "        b: Right-hand side vector\n",
    "        x0: Initial guess (default: zero vector)\n",
    "        tol: Convergence tolerance\n",
    "        max_iter: Maximum iterations\n",
    "    \n",
    "    Returns:\n",
    "        x: Solution vector\n",
    "        residuals: List of residual norms at each iteration\n",
    "    \"\"\"\n",
    "    n = len(b)\n",
    "    if x0 is None:\n",
    "        x = np.zeros(n)\n",
    "    else:\n",
    "        x = x0.copy()\n",
    "    \n",
    "    r = b - A @ x  # Initial residual\n",
    "    p = r.copy()    # Initial search direction\n",
    "    rs_old = np.dot(r, r)\n",
    "    \n",
    "    residuals = [np.sqrt(rs_old)]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        Ap = A @ p\n",
    "        alpha = rs_old / np.dot(p, Ap)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        rs_new = np.dot(r, r)\n",
    "        \n",
    "        residuals.append(np.sqrt(rs_new))\n",
    "        \n",
    "        if np.sqrt(rs_new) < tol:\n",
    "            break\n",
    "            \n",
    "        beta = rs_new / rs_old\n",
    "        p = r + beta * p\n",
    "        rs_old = rs_new\n",
    "    \n",
    "    return x, residuals\n",
    "\n",
    "print(\"Conjugate Gradient implementation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: GMRES with Arnoldi Iteration\n",
    "\n",
    "GMRES builds an orthonormal basis using the Arnoldi process and solves a least-squares problem:\n",
    "\n",
    "$$\\min_{\\mathbf{y} \\in \\mathbb{R}^k} \\|\\beta \\mathbf{e}_1 - \\tilde{H}_k \\mathbf{y}\\|_2$$\n",
    "\n",
    "where $\\beta = \\|\\mathbf{r}_0\\|_2$ and $\\mathbf{x}_k = \\mathbf{x}_0 + Q_k \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmres_method(A, b, x0=None, tol=1e-10, max_iter=1000):\n",
    "    \"\"\"\n",
    "    GMRES method for solving Ax = b.\n",
    "    \n",
    "    Parameters:\n",
    "        A: Matrix (can be non-symmetric)\n",
    "        b: Right-hand side vector\n",
    "        x0: Initial guess\n",
    "        tol: Convergence tolerance\n",
    "        max_iter: Maximum iterations\n",
    "    \n",
    "    Returns:\n",
    "        x: Solution vector\n",
    "        residuals: List of residual norms\n",
    "    \"\"\"\n",
    "    n = len(b)\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros(n)\n",
    "    \n",
    "    r0 = b - A @ x0\n",
    "    beta = np.linalg.norm(r0)\n",
    "    \n",
    "    if beta < tol:\n",
    "        return x0, [beta]\n",
    "    \n",
    "    # Initialize Arnoldi basis\n",
    "    Q = np.zeros((n, max_iter + 1))\n",
    "    H = np.zeros((max_iter + 1, max_iter))\n",
    "    Q[:, 0] = r0 / beta\n",
    "    \n",
    "    residuals = [beta]\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        # Arnoldi step\n",
    "        v = A @ Q[:, k]\n",
    "        \n",
    "        # Modified Gram-Schmidt orthogonalization\n",
    "        for j in range(k + 1):\n",
    "            H[j, k] = np.dot(Q[:, j], v)\n",
    "            v = v - H[j, k] * Q[:, j]\n",
    "        \n",
    "        H[k + 1, k] = np.linalg.norm(v)\n",
    "        \n",
    "        if H[k + 1, k] > 1e-12:\n",
    "            Q[:, k + 1] = v / H[k + 1, k]\n",
    "        \n",
    "        # Solve least squares problem\n",
    "        e1 = np.zeros(k + 2)\n",
    "        e1[0] = beta\n",
    "        y, _, _, _ = np.linalg.lstsq(H[:k + 2, :k + 1], e1, rcond=None)\n",
    "        \n",
    "        # Compute residual\n",
    "        res = np.linalg.norm(e1 - H[:k + 2, :k + 1] @ y)\n",
    "        residuals.append(res)\n",
    "        \n",
    "        if res < tol:\n",
    "            break\n",
    "    \n",
    "    # Compute solution\n",
    "    x = x0 + Q[:, :k + 1] @ y\n",
    "    \n",
    "    return x, residuals\n",
    "\n",
    "print(\"GMRES implementation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Problem: 2D Poisson Equation\n",
    "\n",
    "We discretize the Poisson equation on a unit square:\n",
    "\n",
    "$$-\\nabla^2 u = f \\quad \\text{in } \\Omega = [0,1]^2$$\n",
    "\n",
    "Using central finite differences with grid spacing $h$:\n",
    "\n",
    "$$-\\frac{u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} - 4u_{i,j}}{h^2} = f_{i,j}$$\n",
    "\n",
    "This yields a sparse SPD matrix with condition number $\\kappa = O(h^{-2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poisson_2d(n):\n",
    "    \"\"\"\n",
    "    Create the 2D Poisson matrix using 5-point stencil.\n",
    "    \n",
    "    Parameters:\n",
    "        n: Number of interior grid points in each direction\n",
    "    \n",
    "    Returns:\n",
    "        A: Sparse matrix of size (n^2, n^2)\n",
    "        h: Grid spacing\n",
    "    \"\"\"\n",
    "    N = n * n  # Total unknowns\n",
    "    h = 1.0 / (n + 1)  # Grid spacing\n",
    "    \n",
    "    # Main diagonal: 4/h^2\n",
    "    main_diag = 4.0 * np.ones(N) / h**2\n",
    "    \n",
    "    # Off-diagonals: -1/h^2\n",
    "    off_diag = -np.ones(N - 1) / h**2\n",
    "    # Zero out connections between rows\n",
    "    for i in range(1, n):\n",
    "        off_diag[i * n - 1] = 0\n",
    "    \n",
    "    # Far off-diagonals: -1/h^2\n",
    "    far_diag = -np.ones(N - n) / h**2\n",
    "    \n",
    "    # Construct sparse matrix\n",
    "    A = diags([far_diag, off_diag, main_diag, off_diag, far_diag],\n",
    "              [-n, -1, 0, 1, n], format='csr')\n",
    "    \n",
    "    return A, h\n",
    "\n",
    "# Create test problem\n",
    "n = 50  # 50x50 interior grid\n",
    "A_poisson, h = create_poisson_2d(n)\n",
    "N = n * n\n",
    "\n",
    "print(f\"Matrix size: {N} x {N}\")\n",
    "print(f\"Number of non-zeros: {A_poisson.nnz}\")\n",
    "print(f\"Sparsity: {100 * (1 - A_poisson.nnz / N**2):.2f}%\")\n",
    "print(f\"Grid spacing h = {h:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create right-hand side (corresponding to known solution)\n",
    "# Use manufactured solution: u(x,y) = sin(pi*x) * sin(pi*y)\n",
    "# Then f = 2*pi^2 * sin(pi*x) * sin(pi*y)\n",
    "\n",
    "x = np.linspace(h, 1 - h, n)\n",
    "y = np.linspace(h, 1 - h, n)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# True solution\n",
    "u_true = np.sin(np.pi * X) * np.sin(np.pi * Y)\n",
    "\n",
    "# Right-hand side\n",
    "f = 2 * np.pi**2 * np.sin(np.pi * X) * np.sin(np.pi * Y)\n",
    "b = f.flatten()\n",
    "\n",
    "print(f\"Right-hand side vector created, norm = {np.linalg.norm(b):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Comparison\n",
    "\n",
    "We compare the convergence of:\n",
    "1. Our implemented CG\n",
    "2. Our implemented GMRES\n",
    "3. SciPy's CG\n",
    "4. SciPy's GMRES\n",
    "5. SciPy's BiCGSTAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve with our CG implementation\n",
    "print(\"Solving with custom CG...\")\n",
    "x_cg, residuals_cg = conjugate_gradient(A_poisson.toarray(), b, tol=1e-10, max_iter=500)\n",
    "error_cg = np.linalg.norm(x_cg - u_true.flatten()) / np.linalg.norm(u_true.flatten())\n",
    "print(f\"Custom CG: {len(residuals_cg)-1} iterations, relative error = {error_cg:.2e}\")\n",
    "\n",
    "# Solve with our GMRES implementation\n",
    "print(\"\\nSolving with custom GMRES...\")\n",
    "x_gmres, residuals_gmres = gmres_method(A_poisson.toarray(), b, tol=1e-10, max_iter=500)\n",
    "error_gmres = np.linalg.norm(x_gmres - u_true.flatten()) / np.linalg.norm(u_true.flatten())\n",
    "print(f\"Custom GMRES: {len(residuals_gmres)-1} iterations, relative error = {error_gmres:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Callback functions to record residuals for SciPy solvers\nresiduals_scipy_cg = []\nresiduals_scipy_gmres = []\nresiduals_scipy_bicgstab = []\n\ndef callback_cg(xk):\n    res = np.linalg.norm(b - A_poisson @ xk)\n    residuals_scipy_cg.append(res)\n\ndef callback_gmres(rk):\n    residuals_scipy_gmres.append(rk)\n\ndef callback_bicgstab(xk):\n    res = np.linalg.norm(b - A_poisson @ xk)\n    residuals_scipy_bicgstab.append(res)\n\n# SciPy CG\nprint(\"Solving with SciPy CG...\")\nresiduals_scipy_cg = [np.linalg.norm(b)]  # Initial residual\nx_scipy_cg, info_cg = cg(A_poisson, b, rtol=1e-10, maxiter=500, callback=callback_cg)\nprint(f\"SciPy CG: {len(residuals_scipy_cg)-1} iterations, info = {info_cg}\")\n\n# SciPy GMRES\nprint(\"\\nSolving with SciPy GMRES...\")\nresiduals_scipy_gmres = []\nx_scipy_gmres, info_gmres = gmres(A_poisson, b, rtol=1e-10, maxiter=500, callback=callback_gmres)\nresiduals_scipy_gmres = [np.linalg.norm(b)] + residuals_scipy_gmres\nprint(f\"SciPy GMRES: {len(residuals_scipy_gmres)-1} iterations, info = {info_gmres}\")\n\n# SciPy BiCGSTAB\nprint(\"\\nSolving with SciPy BiCGSTAB...\")\nresiduals_scipy_bicgstab = [np.linalg.norm(b)]  # Initial residual\nx_scipy_bicgstab, info_bicgstab = bicgstab(A_poisson, b, rtol=1e-10, maxiter=500, callback=callback_bicgstab)\nprint(f\"SciPy BiCGSTAB: {len(residuals_scipy_bicgstab)-1} iterations, info = {info_bicgstab}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Condition Number on Convergence\n",
    "\n",
    "The convergence rate of Krylov methods depends critically on the eigenvalue distribution. For CG:\n",
    "\n",
    "$$\\|\\mathbf{e}_k\\|_A \\leq 2\\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^k \\|\\mathbf{e}_0\\|_A$$\n",
    "\n",
    "We demonstrate this by solving problems with different condition numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different grid sizes (different condition numbers)\n",
    "grid_sizes = [10, 20, 30, 40]\n",
    "convergence_histories = []\n",
    "\n",
    "print(\"Testing CG convergence for different grid sizes:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for n_test in grid_sizes:\n",
    "    A_test, h_test = create_poisson_2d(n_test)\n",
    "    \n",
    "    # Create corresponding RHS\n",
    "    x_test = np.linspace(h_test, 1 - h_test, n_test)\n",
    "    y_test = np.linspace(h_test, 1 - h_test, n_test)\n",
    "    X_test, Y_test = np.meshgrid(x_test, y_test)\n",
    "    f_test = 2 * np.pi**2 * np.sin(np.pi * X_test) * np.sin(np.pi * Y_test)\n",
    "    b_test = f_test.flatten()\n",
    "    \n",
    "    # Solve\n",
    "    _, residuals_test = conjugate_gradient(A_test.toarray(), b_test, tol=1e-10, max_iter=500)\n",
    "    convergence_histories.append(residuals_test)\n",
    "    \n",
    "    # Estimate condition number (using inverse of smallest eigenvalue ratio)\n",
    "    kappa_approx = (4 * np.sin(np.pi * n_test / (2 * (n_test + 1)))**2) / (4 * np.sin(np.pi / (2 * (n_test + 1)))**2)\n",
    "    \n",
    "    print(f\"n={n_test:3d}: {len(residuals_test)-1:4d} iterations, \"\n",
    "          f\"κ ≈ {kappa_approx:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Convergence comparison of different methods\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax1.semilogy(residuals_cg, 'b-', linewidth=2, label='Custom CG', marker='o', markevery=20)\n",
    "ax1.semilogy(residuals_gmres, 'r-', linewidth=2, label='Custom GMRES', marker='s', markevery=20)\n",
    "ax1.semilogy(residuals_scipy_cg, 'g--', linewidth=1.5, label='SciPy CG')\n",
    "ax1.semilogy(residuals_scipy_gmres, 'm--', linewidth=1.5, label='SciPy GMRES')\n",
    "ax1.semilogy(residuals_scipy_bicgstab, 'c--', linewidth=1.5, label='SciPy BiCGSTAB')\n",
    "ax1.set_xlabel('Iteration', fontsize=11)\n",
    "ax1.set_ylabel('Residual Norm', fontsize=11)\n",
    "ax1.set_title('Convergence of Krylov Methods', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='upper right', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim([0, max(len(residuals_cg), len(residuals_gmres))])\n",
    "\n",
    "# Plot 2: Effect of condition number\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(grid_sizes)))\n",
    "for i, (n_test, hist) in enumerate(zip(grid_sizes, convergence_histories)):\n",
    "    # Normalize residuals\n",
    "    normalized = np.array(hist) / hist[0]\n",
    "    ax2.semilogy(normalized, color=colors[i], linewidth=2, \n",
    "                 label=f'n={n_test} ({n_test**2} unknowns)')\n",
    "ax2.set_xlabel('Iteration', fontsize=11)\n",
    "ax2.set_ylabel('Normalized Residual', fontsize=11)\n",
    "ax2.set_title('CG Convergence vs Grid Size', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='upper right', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Solution visualization\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "solution_2d = x_cg.reshape((n, n))\n",
    "im = ax3.contourf(X, Y, solution_2d, levels=50, cmap='RdBu_r')\n",
    "plt.colorbar(im, ax=ax3, label='u(x,y)')\n",
    "ax3.set_xlabel('x', fontsize=11)\n",
    "ax3.set_ylabel('y', fontsize=11)\n",
    "ax3.set_title('Numerical Solution', fontsize=12, fontweight='bold')\n",
    "ax3.set_aspect('equal')\n",
    "\n",
    "# Plot 4: Error distribution\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "error_2d = np.abs(solution_2d - u_true)\n",
    "im4 = ax4.contourf(X, Y, error_2d, levels=50, cmap='hot')\n",
    "plt.colorbar(im4, ax=ax4, label='|Error|')\n",
    "ax4.set_xlabel('x', fontsize=11)\n",
    "ax4.set_ylabel('y', fontsize=11)\n",
    "ax4.set_title(f'Absolute Error (max = {np.max(error_2d):.2e})', fontsize=12, fontweight='bold')\n",
    "ax4.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Discussion\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **CG vs GMRES for SPD systems**: Both methods converge at similar rates for symmetric positive definite matrices, but CG requires less memory (short recurrences) while GMRES stores the full Krylov basis.\n",
    "\n",
    "2. **Condition Number Effect**: Convergence slows as the grid becomes finer (larger condition number). For the Poisson problem, $\\kappa = O(n^2)$, so iterations scale as $O(n)$.\n",
    "\n",
    "3. **Preconditioning Importance**: For large-scale problems, preconditioning is essential. Common preconditioners include:\n",
    "   - Incomplete LU/Cholesky factorization\n",
    "   - Multigrid methods\n",
    "   - Domain decomposition\n",
    "\n",
    "### Theoretical Convergence Rates\n",
    "\n",
    "For the model Poisson problem with condition number $\\kappa$:\n",
    "\n",
    "| Method | Convergence Factor | Iterations to reduce by $10^{-p}$ |\n",
    "|--------|-------------------|-----------------------------------|\n",
    "| CG | $(\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1)$ | $O(\\sqrt{\\kappa} \\cdot p)$ |\n",
    "| GMRES | Problem-dependent | $O(n)$ worst case |\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "- **CG**: SPD matrices only. Optimal for well-conditioned problems.\n",
    "- **GMRES**: General matrices. Memory grows with iterations.\n",
    "- **BiCGSTAB**: Non-symmetric matrices. Fixed memory but may have irregular convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY: Krylov Subspace Methods Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nProblem size: {N} unknowns ({n}×{n} grid)\")\n",
    "print(f\"Target tolerance: 1e-10\")\n",
    "print(\"\\nMethod Performance:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Method':<20} {'Iterations':<12} {'Final Residual':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Custom CG':<20} {len(residuals_cg)-1:<12} {residuals_cg[-1]:<15.2e}\")\n",
    "print(f\"{'Custom GMRES':<20} {len(residuals_gmres)-1:<12} {residuals_gmres[-1]:<15.2e}\")\n",
    "print(f\"{'SciPy CG':<20} {len(residuals_scipy_cg)-1:<12} {residuals_scipy_cg[-1]:<15.2e}\")\n",
    "print(f\"{'SciPy GMRES':<20} {len(residuals_scipy_gmres)-1:<12} {residuals_scipy_gmres[-1]:<15.2e}\")\n",
    "print(f\"{'SciPy BiCGSTAB':<20} {len(residuals_scipy_bicgstab)-1:<12} {residuals_scipy_bicgstab[-1]:<15.2e}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nSolution accuracy (relative L2 error): {error_cg:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Krylov subspace methods are indispensable tools for solving large sparse linear systems. Key takeaways:\n",
    "\n",
    "1. **Krylov subspaces** provide a natural framework for iterative methods, exploiting the structure of matrix-vector products.\n",
    "\n",
    "2. **CG** is optimal for SPD systems with short recurrences and guaranteed convergence.\n",
    "\n",
    "3. **GMRES** handles general matrices by minimizing residuals over growing Krylov subspaces.\n",
    "\n",
    "4. **Convergence depends critically on eigenvalue distribution**, making preconditioning essential for difficult problems.\n",
    "\n",
    "5. Modern scientific computing relies heavily on these methods for problems in computational physics, engineering, and data science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}