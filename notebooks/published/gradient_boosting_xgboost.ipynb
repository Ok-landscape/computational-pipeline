{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting and XGBoost: Theory and Implementation\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Gradient Boosting is a powerful ensemble machine learning technique that builds models sequentially, with each new model attempting to correct the errors of the combined ensemble of all previous models. XGBoost (eXtreme Gradient Boosting) is an optimized implementation of gradient boosting that has become one of the most successful algorithms in machine learning competitions and industrial applications.\n",
    "\n",
    "## 2. Mathematical Foundation\n",
    "\n",
    "### 2.1 The Boosting Framework\n",
    "\n",
    "Given a training dataset $\\{(x_i, y_i)\\}_{i=1}^{n}$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$ (for regression), we seek to find a function $F(x)$ that minimizes a loss function $L(y, F(x))$.\n",
    "\n",
    "In gradient boosting, we construct $F(x)$ as an additive model:\n",
    "\n",
    "$$F_M(x) = \\sum_{m=1}^{M} \\gamma_m h_m(x)$$\n",
    "\n",
    "where $h_m(x)$ are base learners (typically decision trees) and $\\gamma_m$ are the corresponding weights.\n",
    "\n",
    "### 2.2 Gradient Descent in Function Space\n",
    "\n",
    "The key insight of gradient boosting is to perform gradient descent in function space. At each iteration $m$, we fit a new base learner to the negative gradient of the loss function:\n",
    "\n",
    "$$r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F=F_{m-1}}$$\n",
    "\n",
    "For squared error loss $L(y, F) = \\frac{1}{2}(y - F)^2$, the negative gradient simplifies to the residual:\n",
    "\n",
    "$$r_{im} = y_i - F_{m-1}(x_i)$$\n",
    "\n",
    "### 2.3 XGBoost Objective Function\n",
    "\n",
    "XGBoost uses a regularized objective function:\n",
    "\n",
    "$$\\mathcal{L}(\\phi) = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)$$\n",
    "\n",
    "where the regularization term is:\n",
    "\n",
    "$$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^{T} w_j^2$$\n",
    "\n",
    "Here, $T$ is the number of leaves in the tree, $w_j$ are the leaf weights, and $\\gamma$, $\\lambda$ are regularization parameters.\n",
    "\n",
    "### 2.4 Second-Order Approximation\n",
    "\n",
    "XGBoost uses a second-order Taylor expansion of the loss function. For adding a new tree $f_t$:\n",
    "\n",
    "$$\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^{n} \\left[g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)\\right] + \\Omega(f_t)$$\n",
    "\n",
    "where:\n",
    "- $g_i = \\frac{\\partial L(y_i, \\hat{y}_i^{(t-1)})}{\\partial \\hat{y}_i^{(t-1)}}$ (first-order gradient)\n",
    "- $h_i = \\frac{\\partial^2 L(y_i, \\hat{y}_i^{(t-1)})}{\\partial (\\hat{y}_i^{(t-1)})^2}$ (second-order gradient, Hessian)\n",
    "\n",
    "### 2.5 Optimal Leaf Weights\n",
    "\n",
    "For a fixed tree structure, the optimal weight for leaf $j$ is:\n",
    "\n",
    "$$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}$$\n",
    "\n",
    "where $I_j$ is the set of instances assigned to leaf $j$.\n",
    "\n",
    "### 2.6 Split Gain\n",
    "\n",
    "The gain from splitting a leaf into left ($I_L$) and right ($I_R$) children is:\n",
    "\n",
    "$$\\text{Gain} = \\frac{1}{2}\\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\\right] - \\gamma$$\n",
    "\n",
    "where $G_L = \\sum_{i \\in I_L} g_i$, $H_L = \\sum_{i \\in I_L} h_i$, and similarly for $G_R$, $H_R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation from Scratch\n",
    "\n",
    "We will implement a simplified gradient boosting algorithm using decision trees as base learners to demonstrate the core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressorFromScratch:\n",
    "    \"\"\"\n",
    "    A simplified Gradient Boosting Regressor implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators : int\n",
    "        Number of boosting stages (trees)\n",
    "    learning_rate : float\n",
    "        Shrinkage parameter to prevent overfitting\n",
    "    max_depth : int\n",
    "        Maximum depth of individual trees\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the gradient boosting model.\n",
    "        \n",
    "        The algorithm:\n",
    "        1. Initialize F_0(x) = mean(y)\n",
    "        2. For m = 1 to M:\n",
    "           a. Compute residuals r_im = y_i - F_{m-1}(x_i)\n",
    "           b. Fit tree h_m to residuals\n",
    "           c. Update: F_m(x) = F_{m-1}(x) + learning_rate * h_m(x)\n",
    "        \"\"\"\n",
    "        # Initialize with mean prediction\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        F = np.full(len(y), self.initial_prediction)\n",
    "        \n",
    "        self.training_losses = []\n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "            # Compute negative gradient (residuals for MSE loss)\n",
    "            residuals = y - F\n",
    "            \n",
    "            # Fit a decision tree to residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Update predictions\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Track training loss\n",
    "            loss = mean_squared_error(y, F)\n",
    "            self.training_losses.append(loss)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the ensemble.\n",
    "        \"\"\"\n",
    "        F = np.full(len(X), self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "        return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demonstration on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, \n",
    "                       noise=10, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our custom gradient boosting model\n",
    "gb_custom = GradientBoostingRegressorFromScratch(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=3\n",
    ")\n",
    "gb_custom.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = gb_custom.predict(X_train)\n",
    "y_pred_test = gb_custom.predict(X_test)\n",
    "\n",
    "print(f\"Train MSE: {mean_squared_error(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, y_pred_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost-Style Implementation with Regularization\n",
    "\n",
    "Now let's implement a more sophisticated version that includes XGBoost-style regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostStyleRegressor:\n",
    "    \"\"\"\n",
    "    XGBoost-style gradient boosting with regularization.\n",
    "    \n",
    "    This implementation includes:\n",
    "    - Second-order gradient information (Hessian)\n",
    "    - L2 regularization on leaf weights (lambda)\n",
    "    - Minimum split gain threshold (gamma)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "                 reg_lambda=1.0, gamma=0.0, min_child_weight=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda  # L2 regularization\n",
    "        self.gamma = gamma  # Minimum gain for split\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "        \n",
    "    def _compute_gradients(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute first and second order gradients for MSE loss.\n",
    "        \n",
    "        For L(y, F) = 0.5 * (y - F)^2:\n",
    "        - g = dL/dF = -(y - F) = F - y\n",
    "        - h = d²L/dF² = 1\n",
    "        \"\"\"\n",
    "        g = y_pred - y_true  # First-order gradient\n",
    "        h = np.ones_like(y_true)  # Second-order gradient (Hessian)\n",
    "        return g, h\n",
    "    \n",
    "    def _compute_optimal_weight(self, g, h):\n",
    "        \"\"\"\n",
    "        Compute optimal leaf weight with L2 regularization.\n",
    "        w* = -G / (H + lambda)\n",
    "        \"\"\"\n",
    "        return -np.sum(g) / (np.sum(h) + self.reg_lambda)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the XGBoost-style model.\n",
    "        \"\"\"\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        y_pred = np.full(len(y), self.initial_prediction)\n",
    "        \n",
    "        self.training_losses = []\n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "            # Compute gradients\n",
    "            g, h = self._compute_gradients(y, y_pred)\n",
    "            \n",
    "            # For MSE, fitting to -g/h is equivalent to fitting to residuals\n",
    "            # In general XGBoost, the tree construction uses g and h directly\n",
    "            # Here we use a simplified approach with sklearn trees\n",
    "            pseudo_residuals = -g / h  # = y - y_pred for MSE\n",
    "            \n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, pseudo_residuals)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Update predictions with shrinkage\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            loss = mean_squared_error(y, y_pred)\n",
    "            self.training_losses.append(loss)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(len(X), self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost-style model\n",
    "xgb_style = XGBoostStyleRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    reg_lambda=1.0\n",
    ")\n",
    "xgb_style.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb_train = xgb_style.predict(X_train)\n",
    "y_pred_xgb_test = xgb_style.predict(X_test)\n",
    "\n",
    "print(f\"XGBoost-style Train MSE: {mean_squared_error(y_train, y_pred_xgb_train):.4f}\")\n",
    "print(f\"XGBoost-style Test MSE: {mean_squared_error(y_test, y_pred_xgb_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing the Boosting Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D example for visualization\n",
    "np.random.seed(42)\n",
    "X_1d = np.linspace(0, 10, 200).reshape(-1, 1)\n",
    "y_1d = np.sin(X_1d.ravel()) + 0.5 * np.cos(2 * X_1d.ravel()) + np.random.normal(0, 0.1, 200)\n",
    "\n",
    "# Train models with different numbers of estimators\n",
    "n_stages = [1, 5, 10, 50, 100]\n",
    "predictions_at_stages = {}\n",
    "\n",
    "for n in n_stages:\n",
    "    model = GradientBoostingRegressorFromScratch(n_estimators=n, learning_rate=0.1, max_depth=2)\n",
    "    model.fit(X_1d, y_1d)\n",
    "    predictions_at_stages[n] = model.predict(X_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 1: Data and predictions at different stages\n",
    "ax = axes[0]\n",
    "ax.scatter(X_1d, y_1d, alpha=0.3, s=10, label='Data')\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(n_stages)))\n",
    "for (n, pred), color in zip(predictions_at_stages.items(), colors):\n",
    "    ax.plot(X_1d, pred, label=f'n={n}', color=color, linewidth=2)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Gradient Boosting: Sequential Improvement')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training loss convergence\n",
    "ax = axes[1]\n",
    "full_model = GradientBoostingRegressorFromScratch(n_estimators=100, learning_rate=0.1, max_depth=2)\n",
    "full_model.fit(X_1d, y_1d)\n",
    "ax.plot(range(1, 101), full_model.training_losses, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Number of Trees')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Training Loss Convergence')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 3: Effect of learning rate\n",
    "ax = axes[2]\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3, 0.5]\n",
    "for lr in learning_rates:\n",
    "    model = GradientBoostingRegressorFromScratch(n_estimators=100, learning_rate=lr, max_depth=2)\n",
    "    model.fit(X_1d, y_1d)\n",
    "    ax.plot(range(1, 101), model.training_losses, label=f'η={lr}', linewidth=1.5)\n",
    "ax.set_xlabel('Number of Trees')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Effect of Learning Rate')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 4: Effect of tree depth\n",
    "ax = axes[3]\n",
    "depths = [1, 2, 3, 4, 5]\n",
    "for d in depths:\n",
    "    model = GradientBoostingRegressorFromScratch(n_estimators=100, learning_rate=0.1, max_depth=d)\n",
    "    model.fit(X_1d, y_1d)\n",
    "    ax.plot(range(1, 101), model.training_losses, label=f'depth={d}', linewidth=1.5)\n",
    "ax.set_xlabel('Number of Trees')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Effect of Tree Depth')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 5: Residuals at different stages\n",
    "ax = axes[4]\n",
    "stages_to_show = [1, 5, 20, 100]\n",
    "colors = plt.cm.plasma(np.linspace(0.2, 0.8, len(stages_to_show)))\n",
    "\n",
    "for stage, color in zip(stages_to_show, colors):\n",
    "    model = GradientBoostingRegressorFromScratch(n_estimators=stage, learning_rate=0.1, max_depth=2)\n",
    "    model.fit(X_1d, y_1d)\n",
    "    residuals = y_1d - model.predict(X_1d)\n",
    "    ax.scatter(X_1d, residuals, alpha=0.4, s=10, label=f'n={stage}', color=color)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Residual')\n",
    "ax.set_title('Residuals at Different Stages')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Bias-Variance Tradeoff illustration\n",
    "ax = axes[5]\n",
    "n_trees_range = np.arange(1, 101, 5)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Split 1D data\n",
    "X_1d_train, X_1d_test, y_1d_train, y_1d_test = train_test_split(\n",
    "    X_1d, y_1d, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "for n in n_trees_range:\n",
    "    model = GradientBoostingRegressorFromScratch(n_estimators=n, learning_rate=0.1, max_depth=2)\n",
    "    model.fit(X_1d_train, y_1d_train)\n",
    "    train_errors.append(mean_squared_error(y_1d_train, model.predict(X_1d_train)))\n",
    "    test_errors.append(mean_squared_error(y_1d_test, model.predict(X_1d_test)))\n",
    "\n",
    "ax.plot(n_trees_range, train_errors, 'b-', label='Train MSE', linewidth=2)\n",
    "ax.plot(n_trees_range, test_errors, 'r-', label='Test MSE', linewidth=2)\n",
    "ax.set_xlabel('Number of Trees')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Bias-Variance Tradeoff')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification with Gradient Boosting\n",
    "\n",
    "For binary classification, we use the logistic loss (cross-entropy):\n",
    "\n",
    "$$L(y, F) = -[y \\log(p) + (1-y) \\log(1-p)]$$\n",
    "\n",
    "where $p = \\sigma(F) = \\frac{1}{1 + e^{-F}}$ is the sigmoid function.\n",
    "\n",
    "The gradients become:\n",
    "- $g_i = p_i - y_i$\n",
    "- $h_i = p_i(1 - p_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingClassifierFromScratch:\n",
    "    \"\"\"\n",
    "    Gradient Boosting Classifier using logistic loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def _log_odds(self, p):\n",
    "        p = np.clip(p, 1e-10, 1 - 1e-10)\n",
    "        return np.log(p / (1 - p))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize with log-odds of mean\n",
    "        p_mean = np.mean(y)\n",
    "        self.initial_prediction = self._log_odds(p_mean)\n",
    "        F = np.full(len(y), self.initial_prediction)\n",
    "        \n",
    "        self.training_losses = []\n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "            # Compute probabilities and gradients\n",
    "            p = self._sigmoid(F)\n",
    "            g = p - y  # Gradient\n",
    "            h = p * (1 - p)  # Hessian\n",
    "            \n",
    "            # Fit tree to negative gradient\n",
    "            residuals = -g\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Update predictions\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Log loss\n",
    "            p_new = self._sigmoid(F)\n",
    "            p_new = np.clip(p_new, 1e-10, 1 - 1e-10)\n",
    "            loss = -np.mean(y * np.log(p_new) + (1 - y) * np.log(1 - p_new))\n",
    "            self.training_losses.append(loss)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        F = np.full(len(X), self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "        return self._sigmoid(F)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "X_clf, y_clf = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=5,\n",
    "    n_redundant=2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train classifier\n",
    "gb_clf = GradientBoostingClassifierFromScratch(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3\n",
    ")\n",
    "gb_clf.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train_clf, gb_clf.predict(X_train_clf))\n",
    "test_acc = accuracy_score(y_test_clf, gb_clf.predict(X_test_clf))\n",
    "\n",
    "print(f\"Classification Results:\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### Gradient Boosting Core Concepts:\n",
    "\n",
    "1. **Sequential Learning**: Each tree corrects errors made by the ensemble of previous trees\n",
    "\n",
    "2. **Gradient Descent in Function Space**: The algorithm fits trees to negative gradients of the loss function\n",
    "\n",
    "3. **Shrinkage (Learning Rate)**: The parameter $\\eta$ controls the contribution of each tree:\n",
    "   - Small $\\eta$: More trees needed, better generalization, slower training\n",
    "   - Large $\\eta$: Fewer trees sufficient, risk of overfitting, faster training\n",
    "\n",
    "### XGBoost Innovations:\n",
    "\n",
    "1. **Regularization**: $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda||w||^2$ prevents overfitting\n",
    "\n",
    "2. **Second-Order Optimization**: Uses both gradient $g$ and Hessian $h$ for better optimization\n",
    "\n",
    "3. **Optimal Split Finding**: Gain-based criterion with regularization:\n",
    "   $$\\text{Gain} = \\frac{1}{2}\\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\\right] - \\gamma$$\n",
    "\n",
    "4. **Sparsity Awareness**: Efficient handling of missing values\n",
    "\n",
    "5. **Parallel Processing**: Column block structure enables parallel tree construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. References\n",
    "\n",
    "1. Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. *Annals of Statistics*, 29(5), 1189-1232.\n",
    "\n",
    "2. Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*.\n",
    "\n",
    "3. Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* (2nd ed.). Springer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
