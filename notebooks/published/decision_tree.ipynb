{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees: Theory and Implementation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Decision trees are fundamental supervised learning algorithms used for both classification and regression tasks. They partition the feature space into rectangular regions through a series of binary splits, creating an interpretable hierarchical structure that mimics human decision-making.\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### The Decision Tree Model\n",
    "\n",
    "A decision tree recursively partitions the input space $\\mathcal{X} \\subseteq \\mathbb{R}^d$ into disjoint regions $\\{R_1, R_2, \\ldots, R_M\\}$. For each region $R_m$, we assign a prediction:\n",
    "\n",
    "$$\\hat{y}(\\mathbf{x}) = \\sum_{m=1}^{M} c_m \\cdot \\mathbb{1}(\\mathbf{x} \\in R_m)$$\n",
    "\n",
    "where $c_m$ is the prediction for region $R_m$ and $\\mathbb{1}(\\cdot)$ is the indicator function.\n",
    "\n",
    "### Splitting Criteria\n",
    "\n",
    "#### Information Gain and Entropy\n",
    "\n",
    "For classification, we use **entropy** to measure impurity. The entropy of a node $t$ with class distribution $(p_1, p_2, \\ldots, p_K)$ is:\n",
    "\n",
    "$$H(t) = -\\sum_{k=1}^{K} p_k \\log_2(p_k)$$\n",
    "\n",
    "The **information gain** from a split using feature $j$ at threshold $\\theta$ is:\n",
    "\n",
    "$$\\text{IG}(t, j, \\theta) = H(t) - \\frac{|t_L|}{|t|} H(t_L) - \\frac{|t_R|}{|t|} H(t_R)$$\n",
    "\n",
    "where $t_L$ and $t_R$ are the left and right child nodes.\n",
    "\n",
    "#### Gini Impurity\n",
    "\n",
    "An alternative measure is the **Gini impurity**:\n",
    "\n",
    "$$G(t) = 1 - \\sum_{k=1}^{K} p_k^2 = \\sum_{k=1}^{K} p_k(1 - p_k)$$\n",
    "\n",
    "The Gini impurity reaches its minimum (0) when all samples belong to a single class and maximum when samples are uniformly distributed.\n",
    "\n",
    "### Tree Construction Algorithm\n",
    "\n",
    "The CART (Classification and Regression Trees) algorithm uses a greedy approach:\n",
    "\n",
    "1. At each node $t$, find the optimal split $(j^*, \\theta^*)$:\n",
    "   $$j^*, \\theta^* = \\arg\\min_{j, \\theta} \\left[ \\frac{|t_L|}{|t|} \\text{Impurity}(t_L) + \\frac{|t_R|}{|t|} \\text{Impurity}(t_R) \\right]$$\n",
    "\n",
    "2. Create child nodes and recurse\n",
    "\n",
    "3. Stop when a termination criterion is met (max depth, min samples, pure node)\n",
    "\n",
    "### Complexity Analysis\n",
    "\n",
    "For $n$ samples and $d$ features:\n",
    "- Building: $O(d \\cdot n \\log n)$ per node (sorting)\n",
    "- Total: $O(d \\cdot n^2 \\log n)$ worst case\n",
    "- Prediction: $O(\\log n)$ average, $O(n)$ worst case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "We implement a decision tree classifier using Gini impurity as the splitting criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"A node in the decision tree.\"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature      # Index of feature to split on\n",
    "        self.threshold = threshold  # Threshold value for split\n",
    "        self.left = left           # Left subtree\n",
    "        self.right = right         # Right subtree\n",
    "        self.value = value         # Prediction value (for leaf nodes)\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"Decision Tree Classifier using Gini impurity.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        \"\"\"Calculate Gini impurity: G = 1 - Σ p_k²\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Calculate entropy: H = -Σ p_k log₂(p_k)\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find the best split for a node using Gini impurity.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if n_samples < self.min_samples_split:\n",
    "            return None, None\n",
    "        \n",
    "        # Current impurity\n",
    "        current_gini = self._gini(y)\n",
    "        best_gain = 0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            # Get unique values for thresholds\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                # Split the data\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate weighted Gini impurity after split\n",
    "                n_left = np.sum(left_mask)\n",
    "                n_right = np.sum(right_mask)\n",
    "                \n",
    "                gini_left = self._gini(y[left_mask])\n",
    "                gini_right = self._gini(y[right_mask])\n",
    "                \n",
    "                weighted_gini = (n_left * gini_left + n_right * gini_right) / n_samples\n",
    "                gain = current_gini - weighted_gini\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively build the decision tree.\"\"\"\n",
    "        n_samples = len(y)\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if depth >= self.max_depth or n_classes == 1 or n_samples < self.min_samples_split:\n",
    "            # Return leaf node with majority class\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        \n",
    "        if best_feature is None:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Create child nodes\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return Node(feature=best_feature, threshold=best_threshold,\n",
    "                   left=left_subtree, right=right_subtree)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the decision tree to training data.\"\"\"\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.root = self._build_tree(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _predict_single(self, x, node):\n",
    "        \"\"\"Predict class for a single sample.\"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_single(x, node.left)\n",
    "        else:\n",
    "            return self._predict_single(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for all samples.\"\"\"\n",
    "        return np.array([self._predict_single(x, self.root) for x in X])\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate accuracy score.\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We create a synthetic dataset with two features and three classes arranged in a pattern that demonstrates the axis-aligned decision boundaries of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiclass_data(n_samples=300):\n",
    "    \"\"\"Generate synthetic data with three classes.\"\"\"\n",
    "    n_per_class = n_samples // 3\n",
    "    \n",
    "    # Class 0: Lower left region\n",
    "    X0 = np.random.randn(n_per_class, 2) * 0.8 + np.array([-2, -2])\n",
    "    y0 = np.zeros(n_per_class, dtype=int)\n",
    "    \n",
    "    # Class 1: Upper right region\n",
    "    X1 = np.random.randn(n_per_class, 2) * 0.8 + np.array([2, 2])\n",
    "    y1 = np.ones(n_per_class, dtype=int)\n",
    "    \n",
    "    # Class 2: Middle region (more challenging)\n",
    "    X2 = np.random.randn(n_per_class, 2) * 1.0 + np.array([0, 0])\n",
    "    y2 = np.full(n_per_class, 2, dtype=int)\n",
    "    \n",
    "    X = np.vstack([X0, X1, X2])\n",
    "    y = np.concatenate([y0, y1, y2])\n",
    "    \n",
    "    # Shuffle the data\n",
    "    shuffle_idx = np.random.permutation(len(y))\n",
    "    return X[shuffle_idx], y[shuffle_idx]\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_multiclass_data(n_samples=300)\n",
    "\n",
    "# Split into train and test sets\n",
    "n_train = int(0.8 * len(y))\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")\n",
    "print(f\"Class distribution (train): {dict(Counter(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the decision tree\n",
    "tree = DecisionTreeClassifier(max_depth=5, min_samples_split=5)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_accuracy = tree.score(X_train, y_train)\n",
    "test_accuracy = tree.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Decision Boundaries\n",
    "\n",
    "We visualize the decision boundaries created by the tree, demonstrating the characteristic axis-aligned rectangular regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Tree Boundaries\"):\n",
    "    \"\"\"Plot decision boundaries and data points.\"\"\"\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Decision regions\n",
    "    contour = ax.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
    "    ax.contour(xx, yy, Z, colors='k', linewidths=0.5, alpha=0.5)\n",
    "    \n",
    "    # Data points\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                        edgecolors='black', s=50, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Feature $x_1$', fontsize=12)\n",
    "    ax.set_ylabel('Feature $x_2$', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Class', fontsize=10)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_decision_boundary(tree, X, y, \n",
    "    f\"Decision Tree Classification\\nTrain Acc: {train_accuracy:.2%}, Test Acc: {test_accuracy:.2%}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Effect of Tree Depth\n",
    "\n",
    "We examine how tree depth affects the bias-variance tradeoff by training trees with different maximum depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze effect of tree depth\n",
    "depths = range(1, 15)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree_temp = DecisionTreeClassifier(max_depth=depth, min_samples_split=2)\n",
    "    tree_temp.fit(X_train, y_train)\n",
    "    train_scores.append(tree_temp.score(X_train, y_train))\n",
    "    test_scores.append(tree_temp.score(X_test, y_test))\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Decision boundaries\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "Z = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "ax1.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
    "ax1.contour(xx, yy, Z, colors='k', linewidths=0.5, alpha=0.5)\n",
    "scatter1 = ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                       edgecolors='black', s=40, alpha=0.8)\n",
    "ax1.set_xlabel('Feature $x_1$', fontsize=10)\n",
    "ax1.set_ylabel('Feature $x_2$', fontsize=10)\n",
    "ax1.set_title('Decision Boundaries (depth=5)', fontsize=12)\n",
    "\n",
    "# Plot 2: Depth vs Accuracy\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax2.plot(depths, train_scores, 'b-o', label='Training', markersize=6)\n",
    "ax2.plot(depths, test_scores, 'r-s', label='Test', markersize=6)\n",
    "ax2.axhline(y=max(test_scores), color='g', linestyle='--', alpha=0.5, \n",
    "            label=f'Best Test: {max(test_scores):.2%}')\n",
    "ax2.set_xlabel('Tree Depth', fontsize=10)\n",
    "ax2.set_ylabel('Accuracy', fontsize=10)\n",
    "ax2.set_title('Effect of Tree Depth on Accuracy', fontsize=12)\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0.5, 1.05])\n",
    "\n",
    "# Plot 3: Shallow tree (underfitting)\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "shallow_tree = DecisionTreeClassifier(max_depth=2, min_samples_split=2)\n",
    "shallow_tree.fit(X_train, y_train)\n",
    "Z_shallow = shallow_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "ax3.contourf(xx, yy, Z_shallow, alpha=0.4, cmap='viridis')\n",
    "ax3.contour(xx, yy, Z_shallow, colors='k', linewidths=0.5, alpha=0.5)\n",
    "ax3.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "           edgecolors='black', s=40, alpha=0.8)\n",
    "shallow_test_acc = shallow_tree.score(X_test, y_test)\n",
    "ax3.set_xlabel('Feature $x_1$', fontsize=10)\n",
    "ax3.set_ylabel('Feature $x_2$', fontsize=10)\n",
    "ax3.set_title(f'Shallow Tree (depth=2)\\nTest Acc: {shallow_test_acc:.2%}', fontsize=12)\n",
    "\n",
    "# Plot 4: Deep tree (overfitting)\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "deep_tree = DecisionTreeClassifier(max_depth=15, min_samples_split=2)\n",
    "deep_tree.fit(X_train, y_train)\n",
    "Z_deep = deep_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "ax4.contourf(xx, yy, Z_deep, alpha=0.4, cmap='viridis')\n",
    "ax4.contour(xx, yy, Z_deep, colors='k', linewidths=0.5, alpha=0.5)\n",
    "ax4.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "           edgecolors='black', s=40, alpha=0.8)\n",
    "deep_test_acc = deep_tree.score(X_test, y_test)\n",
    "ax4.set_xlabel('Feature $x_1$', fontsize=10)\n",
    "ax4.set_ylabel('Feature $x_2$', fontsize=10)\n",
    "ax4.set_title(f'Deep Tree (depth=15)\\nTest Acc: {deep_test_acc:.2%}', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "### Characteristics of Decision Trees\n",
    "\n",
    "1. **Axis-aligned boundaries**: Decision trees create rectangular decision regions aligned with feature axes, visible in the stepped boundaries.\n",
    "\n",
    "2. **Interpretability**: Each prediction can be traced through a series of simple if-then rules.\n",
    "\n",
    "3. **Bias-Variance Tradeoff**:\n",
    "   - Shallow trees (high bias): Underfit the data, missing important patterns\n",
    "   - Deep trees (high variance): Overfit to training noise, creating complex boundaries\n",
    "\n",
    "### Mathematical Insights\n",
    "\n",
    "The Gini impurity $G = 1 - \\sum_k p_k^2$ provides computational efficiency over entropy while yielding similar splits in practice. For binary classification:\n",
    "\n",
    "$$G = 2p(1-p)$$\n",
    "\n",
    "reaching maximum at $p = 0.5$.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Cannot capture diagonal or curved boundaries efficiently\n",
    "- Sensitive to small changes in data (high variance)\n",
    "- Greedy optimization may miss globally optimal splits\n",
    "\n",
    "These limitations motivate ensemble methods like Random Forests and Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 50)\n",
    "print(\"DECISION TREE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nDataset: {len(y)} samples, {X.shape[1]} features, {len(np.unique(y))} classes\")\n",
    "print(f\"Train/Test split: {len(y_train)}/{len(y_test)}\")\n",
    "print(f\"\\nOptimal Tree (depth=5):\")\n",
    "print(f\"  Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"\\nDepth Analysis:\")\n",
    "print(f\"  Best test accuracy: {max(test_scores):.4f} at depth {depths[np.argmax(test_scores)]}\")\n",
    "print(f\"  Shallow tree (d=2): {shallow_test_acc:.4f}\")\n",
    "print(f\"  Deep tree (d=15): {deep_test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
