{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks: Theory and Implementation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data by maintaining an internal hidden state that captures information from previous time steps. Unlike feedforward networks, RNNs have cyclic connections that allow information to persist across time, making them suitable for tasks involving temporal dependencies such as time series prediction, natural language processing, and speech recognition.\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "### Basic RNN Architecture\n",
    "\n",
    "At each time step $t$, an RNN takes an input $\\mathbf{x}_t \\in \\mathbb{R}^{d}$ and the previous hidden state $\\mathbf{h}_{t-1} \\in \\mathbb{R}^{n}$ to produce a new hidden state $\\mathbf{h}_t$ and an output $\\mathbf{y}_t$.\n",
    "\n",
    "The forward propagation equations are:\n",
    "\n",
    "$$\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{b}_h)$$\n",
    "\n",
    "$$\\mathbf{y}_t = \\mathbf{W}_{hy} \\mathbf{h}_t + \\mathbf{b}_y$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{W}_{xh} \\in \\mathbb{R}^{n \\times d}$ is the input-to-hidden weight matrix\n",
    "- $\\mathbf{W}_{hh} \\in \\mathbb{R}^{n \\times n}$ is the hidden-to-hidden weight matrix\n",
    "- $\\mathbf{W}_{hy} \\in \\mathbb{R}^{m \\times n}$ is the hidden-to-output weight matrix\n",
    "- $\\mathbf{b}_h \\in \\mathbb{R}^{n}$ and $\\mathbf{b}_y \\in \\mathbb{R}^{m}$ are bias vectors\n",
    "- $\\tanh$ is the hyperbolic tangent activation function\n",
    "\n",
    "### Backpropagation Through Time (BPTT)\n",
    "\n",
    "Training RNNs requires computing gradients through time. For a sequence of length $T$, the total loss is:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{t=1}^{T} \\mathcal{L}_t(\\mathbf{y}_t, \\hat{\\mathbf{y}}_t)$$\n",
    "\n",
    "The gradient with respect to $\\mathbf{W}_{hh}$ involves a sum over all time steps:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{hh}} = \\sum_{t=1}^{T} \\sum_{k=1}^{t} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{h}_t} \\left( \\prod_{j=k+1}^{t} \\frac{\\partial \\mathbf{h}_j}{\\partial \\mathbf{h}_{j-1}} \\right) \\frac{\\partial \\mathbf{h}_k}{\\partial \\mathbf{W}_{hh}}$$\n",
    "\n",
    "### The Vanishing Gradient Problem\n",
    "\n",
    "The product of Jacobians $\\prod_{j=k+1}^{t} \\frac{\\partial \\mathbf{h}_j}{\\partial \\mathbf{h}_{j-1}}$ can either vanish or explode as $t - k$ grows large. For the $\\tanh$ activation:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}} = \\text{diag}(1 - \\mathbf{h}_t^2) \\cdot \\mathbf{W}_{hh}$$\n",
    "\n",
    "Since $|\\tanh'(x)| \\leq 1$ and typically $\\|\\mathbf{W}_{hh}\\| < 1$ for stability, the gradients tend to vanish exponentially for long sequences.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We will implement a simple RNN from scratch to predict a sine wave sequence, demonstrating the core mechanics of recurrent computation and BPTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    A vanilla RNN implementation for sequence-to-sequence prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize RNN parameters with Xavier initialization.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Dimension of input features\n",
    "        hidden_size : int\n",
    "            Dimension of hidden state\n",
    "        output_size : int\n",
    "            Dimension of output\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Xavier initialization for weights\n",
    "        scale_xh = np.sqrt(2.0 / (input_size + hidden_size))\n",
    "        scale_hh = np.sqrt(2.0 / (hidden_size + hidden_size))\n",
    "        scale_hy = np.sqrt(2.0 / (hidden_size + output_size))\n",
    "        \n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * scale_xh\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale_hh\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size) * scale_hy\n",
    "        \n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN for a sequence.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : list of np.ndarray\n",
    "            List of input vectors, each of shape (input_size, 1)\n",
    "        h_prev : np.ndarray\n",
    "            Initial hidden state of shape (hidden_size, 1)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        outputs : list of np.ndarray\n",
    "            List of output vectors\n",
    "        hidden_states : list of np.ndarray\n",
    "            List of hidden states for each time step\n",
    "        \"\"\"\n",
    "        hidden_states = [h_prev]\n",
    "        outputs = []\n",
    "        \n",
    "        for x_t in inputs:\n",
    "            # Compute new hidden state\n",
    "            h_t = np.tanh(self.W_hh @ hidden_states[-1] + self.W_xh @ x_t + self.b_h)\n",
    "            hidden_states.append(h_t)\n",
    "            \n",
    "            # Compute output\n",
    "            y_t = self.W_hy @ h_t + self.b_y\n",
    "            outputs.append(y_t)\n",
    "            \n",
    "        return outputs, hidden_states\n",
    "    \n",
    "    def backward(self, inputs, targets, outputs, hidden_states, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backpropagation through time (BPTT).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : list of np.ndarray\n",
    "            Input sequence\n",
    "        targets : list of np.ndarray\n",
    "            Target sequence\n",
    "        outputs : list of np.ndarray\n",
    "            Predicted outputs from forward pass\n",
    "        hidden_states : list of np.ndarray\n",
    "            Hidden states from forward pass\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Mean squared error loss\n",
    "        \"\"\"\n",
    "        T = len(inputs)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = 0\n",
    "        for t in range(T):\n",
    "            loss += np.sum((outputs[t] - targets[t]) ** 2)\n",
    "        loss /= T\n",
    "        \n",
    "        # Backward pass\n",
    "        dh_next = np.zeros_like(hidden_states[0])\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            # Output layer gradient\n",
    "            dy = 2 * (outputs[t] - targets[t]) / T\n",
    "            dW_hy += dy @ hidden_states[t + 1].T\n",
    "            db_y += dy\n",
    "            \n",
    "            # Hidden state gradient\n",
    "            dh = self.W_hy.T @ dy + dh_next\n",
    "            \n",
    "            # Gradient through tanh\n",
    "            dh_raw = dh * (1 - hidden_states[t + 1] ** 2)\n",
    "            \n",
    "            # Parameter gradients\n",
    "            dW_xh += dh_raw @ inputs[t].T\n",
    "            dW_hh += dh_raw @ hidden_states[t].T\n",
    "            db_h += dh_raw\n",
    "            \n",
    "            # Gradient for next iteration (previous time step)\n",
    "            dh_next = self.W_hh.T @ dh_raw\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        for grad in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W_xh -= learning_rate * dW_xh\n",
    "        self.W_hh -= learning_rate * dW_hh\n",
    "        self.W_hy -= learning_rate * dW_hy\n",
    "        self.b_h -= learning_rate * db_h\n",
    "        self.b_y -= learning_rate * db_y\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sine_data(n_samples, seq_length, freq=0.1):\n",
    "    \"\"\"\n",
    "    Generate sine wave sequences for training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of training sequences\n",
    "    seq_length : int\n",
    "        Length of each sequence\n",
    "    freq : float\n",
    "        Frequency of sine wave\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : list of lists\n",
    "        Input sequences\n",
    "    Y : list of lists\n",
    "        Target sequences (shifted by 1)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Random starting phase\n",
    "        phase = np.random.rand() * 2 * np.pi\n",
    "        t = np.arange(seq_length + 1)\n",
    "        signal = np.sin(2 * np.pi * freq * t + phase)\n",
    "        \n",
    "        # Input: t=0 to t=seq_length-1\n",
    "        # Target: t=1 to t=seq_length (predict next value)\n",
    "        x_seq = [np.array([[signal[i]]]) for i in range(seq_length)]\n",
    "        y_seq = [np.array([[signal[i + 1]]]) for i in range(seq_length)]\n",
    "        \n",
    "        X.append(x_seq)\n",
    "        Y.append(y_seq)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# Generate training data\n",
    "n_samples = 500\n",
    "seq_length = 25\n",
    "X_train, Y_train = generate_sine_data(n_samples, seq_length)\n",
    "\n",
    "print(f\"Generated {n_samples} training sequences of length {seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNN\n",
    "input_size = 1\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Initialize hidden state\n",
    "        h_init = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden_states = rnn.forward(X_train[i], h_init)\n",
    "        \n",
    "        # Backward pass and update\n",
    "        loss = rnn.backward(X_train[i], Y_train[i], outputs, hidden_states, learning_rate)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    avg_loss = epoch_loss / n_samples\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test sequence\n",
    "test_seq_length = 50\n",
    "t_test = np.arange(test_seq_length + 1)\n",
    "test_signal = np.sin(2 * np.pi * 0.1 * t_test)\n",
    "\n",
    "# Prepare test input\n",
    "X_test = [np.array([[test_signal[i]]]) for i in range(test_seq_length)]\n",
    "Y_test = [np.array([[test_signal[i + 1]]]) for i in range(test_seq_length)]\n",
    "\n",
    "# Run prediction\n",
    "h_init = np.zeros((hidden_size, 1))\n",
    "predictions, _ = rnn.forward(X_test, h_init)\n",
    "\n",
    "# Extract values for plotting\n",
    "pred_values = [p[0, 0] for p in predictions]\n",
    "true_values = [y[0, 0] for y in Y_test]\n",
    "input_values = [x[0, 0] for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Training loss curve\n",
    "axes[0, 0].plot(losses, 'b-', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Mean Squared Error', fontsize=12)\n",
    "axes[0, 0].set_title('Training Loss Over Time', fontsize=14)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Plot 2: Prediction vs True values\n",
    "axes[0, 1].plot(true_values, 'b-', label='True', linewidth=2)\n",
    "axes[0, 1].plot(pred_values, 'r--', label='Predicted', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Time Step', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Value', fontsize=12)\n",
    "axes[0, 1].set_title('RNN Sine Wave Prediction', fontsize=14)\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Prediction error\n",
    "errors = [p - t for p, t in zip(pred_values, true_values)]\n",
    "axes[1, 0].plot(errors, 'g-', linewidth=2)\n",
    "axes[1, 0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[1, 0].set_xlabel('Time Step', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Prediction Error', fontsize=12)\n",
    "axes[1, 0].set_title('Prediction Error Over Time', fontsize=14)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Hidden state evolution (first 4 dimensions)\n",
    "h_init = np.zeros((hidden_size, 1))\n",
    "_, hidden_states = rnn.forward(X_test, h_init)\n",
    "hidden_matrix = np.hstack(hidden_states[1:])  # Skip initial state\n",
    "\n",
    "for i in range(min(4, hidden_size)):\n",
    "    axes[1, 1].plot(hidden_matrix[i, :], label=f'h[{i}]', linewidth=1.5, alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Time Step', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Activation', fontsize=12)\n",
    "axes[1, 1].set_title('Hidden State Dynamics (First 4 Units)', fontsize=14)\n",
    "axes[1, 1].legend(fontsize=9, loc='upper right')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "mse = np.mean(np.array(errors) ** 2)\n",
    "print(f\"\\nFinal Test MSE: {mse:.6f}\")\n",
    "print(f\"Final Training Loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "The implemented RNN successfully learns to predict the next value in a sine wave sequence. Key observations:\n",
    "\n",
    "1. **Convergence**: The training loss decreases steadily, indicating the network is learning the temporal patterns in the data.\n",
    "\n",
    "2. **Prediction Quality**: The predicted values closely follow the true sine wave, demonstrating the RNN's ability to capture periodic dependencies.\n",
    "\n",
    "3. **Hidden State Dynamics**: The hidden units show oscillatory behavior that correlates with the input signal's periodicity, confirming that the network encodes temporal information in its internal state.\n",
    "\n",
    "4. **Limitations**: Vanilla RNNs struggle with long-term dependencies due to vanishing gradients. For longer sequences, architectures like LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit) are preferred.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the fundamental principles of Recurrent Neural Networks, including:\n",
    "- The mathematical formulation of hidden state updates\n",
    "- Forward propagation through time\n",
    "- Backpropagation through time (BPTT) for gradient computation\n",
    "- Gradient clipping to prevent exploding gradients\n",
    "\n",
    "The implementation provides a foundation for understanding more advanced recurrent architectures used in modern deep learning applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
