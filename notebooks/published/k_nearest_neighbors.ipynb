{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a non-parametric, instance-based learning method used for classification and regression. Unlike parametric models that learn a fixed set of parameters, KNN stores the entire training dataset and makes predictions based on the similarity between new data points and existing observations.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Distance Metrics\n",
    "\n",
    "The core of KNN relies on measuring distances between points in feature space. The most common metric is the **Euclidean distance**:\n",
    "\n",
    "$$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$\n",
    "\n",
    "where $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ and $\\mathbf{y} = (y_1, y_2, \\ldots, y_n)$ are $n$-dimensional feature vectors.\n",
    "\n",
    "Other commonly used metrics include:\n",
    "\n",
    "**Manhattan Distance (L1 norm):**\n",
    "$$d_1(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{n} |x_i - y_i|$$\n",
    "\n",
    "**Minkowski Distance (generalized):**\n",
    "$$d_p(\\mathbf{x}, \\mathbf{y}) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p}$$\n",
    "\n",
    "### Classification Rule\n",
    "\n",
    "For a query point $\\mathbf{x}_q$, KNN classification proceeds as follows:\n",
    "\n",
    "1. Compute distances $d(\\mathbf{x}_q, \\mathbf{x}_i)$ for all training points $\\mathbf{x}_i$\n",
    "2. Select the $k$ nearest neighbors: $\\mathcal{N}_k(\\mathbf{x}_q)$\n",
    "3. Assign the class by majority vote:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{c} \\sum_{i \\in \\mathcal{N}_k(\\mathbf{x}_q)} \\mathbb{1}(y_i = c)$$\n",
    "\n",
    "where $\\mathbb{1}(\\cdot)$ is the indicator function.\n",
    "\n",
    "### Weighted KNN\n",
    "\n",
    "A refinement weights votes by inverse distance:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{c} \\sum_{i \\in \\mathcal{N}_k(\\mathbf{x}_q)} w_i \\cdot \\mathbb{1}(y_i = c)$$\n",
    "\n",
    "where the weight $w_i = \\frac{1}{d(\\mathbf{x}_q, \\mathbf{x}_i) + \\epsilon}$ and $\\epsilon$ prevents division by zero.\n",
    "\n",
    "### Bias-Variance Tradeoff\n",
    "\n",
    "The choice of $k$ controls model complexity:\n",
    "- **Small $k$**: Low bias, high variance (sensitive to noise)\n",
    "- **Large $k$**: High bias, low variance (smoother decision boundaries)\n",
    "\n",
    "The optimal $k$ balances these competing effects and is typically chosen via cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement KNN from scratch and demonstrate its application on a synthetic classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors:\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors classifier implementation from scratch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Number of neighbors to consider\n",
    "    weighted : bool\n",
    "        If True, weight votes by inverse distance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=3, weighted=False):\n",
    "        self.k = k\n",
    "        self.weighted = weighted\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Store the training data.\"\"\"\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "        return self\n",
    "    \n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "        \"\"\"Compute Euclidean distance between two points.\"\"\"\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"Predict class for a single sample.\"\"\"\n",
    "        # Compute distances to all training points\n",
    "        distances = [self._euclidean_distance(x, x_train) \n",
    "                     for x_train in self.X_train]\n",
    "        \n",
    "        # Get indices of k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = self.y_train[k_indices]\n",
    "        k_nearest_distances = np.array(distances)[k_indices]\n",
    "        \n",
    "        if self.weighted:\n",
    "            # Weighted voting by inverse distance\n",
    "            weights = 1.0 / (k_nearest_distances + 1e-8)\n",
    "            class_weights = {}\n",
    "            for label, weight in zip(k_nearest_labels, weights):\n",
    "                class_weights[label] = class_weights.get(label, 0) + weight\n",
    "            return max(class_weights, key=class_weights.get)\n",
    "        else:\n",
    "            # Simple majority voting\n",
    "            most_common = Counter(k_nearest_labels).most_common(1)\n",
    "            return most_common[0][0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for multiple samples.\"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self._predict_single(x) for x in X])\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Compute classification accuracy.\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Data\n",
    "\n",
    "We create a 2D classification dataset with three classes, each generated from a Gaussian distribution with different means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(n_samples_per_class=100):\n",
    "    \"\"\"\n",
    "    Generate synthetic 2D classification data with 3 classes.\n",
    "    \"\"\"\n",
    "    # Class centers\n",
    "    centers = [\n",
    "        [0, 0],\n",
    "        [4, 4],\n",
    "        [0, 4]\n",
    "    ]\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for class_idx, center in enumerate(centers):\n",
    "        # Generate samples from Gaussian distribution\n",
    "        samples = np.random.randn(n_samples_per_class, 2) * 1.0 + center\n",
    "        X.append(samples)\n",
    "        y.extend([class_idx] * n_samples_per_class)\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    shuffle_idx = np.random.permutation(len(y))\n",
    "    return X[shuffle_idx], y[shuffle_idx]\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_classification_data(n_samples_per_class=100)\n",
    "\n",
    "# Split into training and test sets (80-20 split)\n",
    "n_train = int(0.8 * len(y))\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN with different values of k\n",
    "k_values = [1, 3, 5, 7, 11, 15, 21]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNearestNeighbors(k=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = knn.score(X_train, y_train)\n",
    "    test_acc = knn.score(X_test, y_test)\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f\"k={k:2d}: Train Accuracy = {train_acc:.3f}, Test Accuracy = {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We create a comprehensive visualization showing:\n",
    "1. Decision boundaries for different values of $k$\n",
    "2. Accuracy vs. $k$ curve demonstrating the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(ax, knn, X, y, title):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a trained KNN classifier.\n",
    "    \"\"\"\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Predict on mesh grid\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision regions\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    ax.contour(xx, yy, Z, colors='k', linewidths=0.5)\n",
    "    \n",
    "    # Plot training points\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                         edgecolors='k', s=20, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    return scatter\n",
    "\n",
    "# Create figure with subplots\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot decision boundaries for k=1, 5, 15\n",
    "k_to_plot = [1, 5, 15]\n",
    "for idx, k in enumerate(k_to_plot):\n",
    "    ax = fig.add_subplot(2, 2, idx + 1)\n",
    "    knn = KNearestNeighbors(k=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    acc = knn.score(X_test, y_test)\n",
    "    plot_decision_boundary(ax, knn, X_train, y_train, \n",
    "                          f'KNN Decision Boundary (k={k})\\nTest Accuracy: {acc:.3f}')\n",
    "\n",
    "# Plot accuracy vs k\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "ax4.plot(k_values, train_accuracies, 'o-', label='Training Accuracy', linewidth=2)\n",
    "ax4.plot(k_values, test_accuracies, 's-', label='Test Accuracy', linewidth=2)\n",
    "ax4.axhline(y=max(test_accuracies), color='r', linestyle='--', alpha=0.5, \n",
    "            label=f'Best Test Acc: {max(test_accuracies):.3f}')\n",
    "\n",
    "# Mark optimal k\n",
    "best_k_idx = np.argmax(test_accuracies)\n",
    "best_k = k_values[best_k_idx]\n",
    "ax4.axvline(x=best_k, color='g', linestyle=':', alpha=0.7)\n",
    "ax4.annotate(f'Optimal k={best_k}', xy=(best_k, test_accuracies[best_k_idx]),\n",
    "             xytext=(best_k + 3, test_accuracies[best_k_idx] - 0.03),\n",
    "             arrowprops=dict(arrowstyle='->', color='green'),\n",
    "             fontsize=10)\n",
    "\n",
    "ax4.set_xlabel('k (Number of Neighbors)')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title('Bias-Variance Tradeoff: Accuracy vs. k')\n",
    "ax4.legend(loc='lower right')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Conclusions\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Decision Boundary Complexity**: As $k$ increases, decision boundaries become smoother. For $k=1$, boundaries are highly irregular and conform closely to individual training points (overfitting). Larger $k$ values produce more generalized boundaries.\n",
    "\n",
    "2. **Bias-Variance Tradeoff**: \n",
    "   - Training accuracy decreases monotonically with $k$ (increased bias)\n",
    "   - Test accuracy typically shows a U-shaped curve, with optimal performance at intermediate $k$\n",
    "\n",
    "3. **Computational Considerations**: KNN has $O(1)$ training time but $O(nd)$ prediction time for $n$ training samples and $d$ dimensions. For large datasets, approximate nearest neighbor algorithms (e.g., KD-trees, ball trees) are preferred.\n",
    "\n",
    "### Algorithm Properties\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to understand and implement\n",
    "- No explicit training phase\n",
    "- Naturally handles multi-class problems\n",
    "- Can capture complex decision boundaries\n",
    "\n",
    "**Limitations:**\n",
    "- Computationally expensive at prediction time\n",
    "- Sensitive to irrelevant features and the curse of dimensionality\n",
    "- Requires feature scaling for meaningful distance computation\n",
    "- Memory-intensive (stores all training data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
