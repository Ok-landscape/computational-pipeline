{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning and Fine-Tuning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Transfer learning** is a machine learning paradigm where a model trained on one task (the *source task*) is repurposed as the starting point for a model on a different but related task (the *target task*). This approach leverages the knowledge encoded in pre-trained models to accelerate learning and improve performance, particularly when labeled data for the target task is scarce.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Domain and Task Definitions\n",
    "\n",
    "Let us formalize the transfer learning framework. A **domain** $\\mathcal{D}$ consists of a feature space $\\mathcal{X}$ and a marginal probability distribution $P(X)$, where $X = \\{x_1, x_2, \\ldots, x_n\\} \\in \\mathcal{X}$:\n",
    "\n",
    "$$\\mathcal{D} = \\{\\mathcal{X}, P(X)\\}$$\n",
    "\n",
    "A **task** $\\mathcal{T}$ is defined by a label space $\\mathcal{Y}$ and a predictive function $f(\\cdot)$:\n",
    "\n",
    "$$\\mathcal{T} = \\{\\mathcal{Y}, f(\\cdot)\\}$$\n",
    "\n",
    "where $f(x) = P(Y|X)$ can be learned from training data.\n",
    "\n",
    "### Transfer Learning Objective\n",
    "\n",
    "Given a source domain $\\mathcal{D}_S$ with task $\\mathcal{T}_S$ and a target domain $\\mathcal{D}_T$ with task $\\mathcal{T}_T$, transfer learning aims to improve the learning of the target predictive function $f_T(\\cdot)$ using knowledge from $\\mathcal{D}_S$ and $\\mathcal{T}_S$, where $\\mathcal{D}_S \\neq \\mathcal{D}_T$ or $\\mathcal{T}_S \\neq \\mathcal{T}_T$.\n",
    "\n",
    "### Neural Network Representation\n",
    "\n",
    "In deep learning, a neural network can be decomposed into a **feature extractor** $\\phi(\\cdot; \\theta_\\phi)$ and a **classifier** $g(\\cdot; \\theta_g)$:\n",
    "\n",
    "$$f(x) = g(\\phi(x; \\theta_\\phi); \\theta_g)$$\n",
    "\n",
    "The feature extractor maps inputs to a learned representation space:\n",
    "\n",
    "$$\\phi: \\mathcal{X} \\rightarrow \\mathbb{R}^d$$\n",
    "\n",
    "where $d$ is the dimensionality of the feature representation.\n",
    "\n",
    "## Fine-Tuning Strategies\n",
    "\n",
    "### Feature Extraction (Frozen Backbone)\n",
    "\n",
    "In this approach, the pre-trained feature extractor parameters $\\theta_\\phi$ are frozen, and only the classifier parameters $\\theta_g$ are updated:\n",
    "\n",
    "$$\\theta_g^* = \\arg\\min_{\\theta_g} \\mathcal{L}(g(\\phi(X; \\theta_\\phi^{\\text{pre}}); \\theta_g), Y)$$\n",
    "\n",
    "### Full Fine-Tuning\n",
    "\n",
    "All parameters are updated with a smaller learning rate $\\eta_{\\text{fine}}$:\n",
    "\n",
    "$$\\theta^* = \\arg\\min_{\\theta_\\phi, \\theta_g} \\mathcal{L}(f(X; \\theta_\\phi, \\theta_g), Y)$$\n",
    "\n",
    "### Layer-wise Learning Rate Decay\n",
    "\n",
    "Earlier layers (closer to input) receive smaller learning rates. For layer $l$:\n",
    "\n",
    "$$\\eta_l = \\eta_{\\text{base}} \\cdot \\gamma^{L-l}$$\n",
    "\n",
    "where $L$ is the total number of layers and $\\gamma < 1$ is the decay factor.\n",
    "\n",
    "### Gradual Unfreezing\n",
    "\n",
    "Layers are progressively unfrozen during training, starting from the top (closest to output):\n",
    "\n",
    "$$\\theta_l^{(t)} = \\begin{cases} \\theta_l^{(t-1)} - \\eta_l \\nabla_{\\theta_l} \\mathcal{L} & \\text{if } l \\geq L - k(t) \\\\ \\theta_l^{(t-1)} & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "where $k(t)$ increases with training epoch $t$.\n",
    "\n",
    "## Loss Functions and Regularization\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "For classification tasks, the cross-entropy loss is:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{CE}} = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})$$\n",
    "\n",
    "where $N$ is the batch size and $C$ is the number of classes.\n",
    "\n",
    "### L2 Regularization (Weight Decay)\n",
    "\n",
    "To prevent catastrophic forgetting and overfitting:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{CE}} + \\lambda \\|\\theta - \\theta^{\\text{pre}}\\|_2^2$$\n",
    "\n",
    "This penalizes large deviations from the pre-trained weights $\\theta^{\\text{pre}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Simulating Transfer Learning\n",
    "\n",
    "We will simulate a transfer learning scenario using synthetic data and a simple neural network architecture. This demonstration illustrates the key concepts without requiring large datasets or GPU computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation\n",
    "\n",
    "We create two related classification tasks:\n",
    "- **Source Task**: Classification with abundant labeled data\n",
    "- **Target Task**: Related classification with limited labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(n_samples, n_features, n_classes, noise=0.1, transformation=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic classification data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    n_features : int\n",
    "        Dimensionality of feature space\n",
    "    n_classes : int\n",
    "        Number of classes\n",
    "    noise : float\n",
    "        Standard deviation of Gaussian noise\n",
    "    transformation : ndarray or None\n",
    "        Optional linear transformation to apply (for domain shift)\n",
    "    \"\"\"\n",
    "    # Generate class centers\n",
    "    centers = np.random.randn(n_classes, n_features) * 2\n",
    "    \n",
    "    # Generate samples\n",
    "    X = []\n",
    "    y = []\n",
    "    samples_per_class = n_samples // n_classes\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        X_c = centers[c] + np.random.randn(samples_per_class, n_features) * noise\n",
    "        X.append(X_c)\n",
    "        y.extend([c] * samples_per_class)\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Apply domain transformation if provided\n",
    "    if transformation is not None:\n",
    "        X = X @ transformation\n",
    "    \n",
    "    # Shuffle data\n",
    "    idx = np.random.permutation(len(y))\n",
    "    return X[idx], y[idx], centers\n",
    "\n",
    "# Parameters\n",
    "n_features = 10\n",
    "n_classes = 5\n",
    "n_source = 1000  # Abundant source data\n",
    "n_target = 100   # Limited target data\n",
    "\n",
    "# Generate source domain data\n",
    "X_source, y_source, source_centers = generate_classification_data(\n",
    "    n_source, n_features, n_classes, noise=0.5\n",
    ")\n",
    "\n",
    "# Generate target domain data with slight domain shift\n",
    "# Apply a rotation/scaling transformation to simulate domain shift\n",
    "domain_shift = np.eye(n_features) + np.random.randn(n_features, n_features) * 0.1\n",
    "X_target, y_target, target_centers = generate_classification_data(\n",
    "    n_target, n_features, n_classes, noise=0.5, transformation=domain_shift\n",
    ")\n",
    "\n",
    "print(f\"Source domain: {X_source.shape[0]} samples\")\n",
    "print(f\"Target domain: {X_target.shape[0]} samples\")\n",
    "print(f\"Feature dimensionality: {n_features}\")\n",
    "print(f\"Number of classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Implementation\n",
    "\n",
    "We implement a simple two-layer neural network with ReLU activation, decomposed into feature extractor and classifier components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple neural network for classification with transfer learning support.\n",
    "    \n",
    "    Architecture:\n",
    "    - Feature extractor: Input -> Hidden1 -> ReLU -> Hidden2 -> ReLU\n",
    "    - Classifier: Hidden2 -> Output (softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        self.weights = {}\n",
    "        \n",
    "        # Feature extractor layers\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(dims) - 1):\n",
    "            scale = np.sqrt(2.0 / dims[i])\n",
    "            self.weights[f'W{i}'] = np.random.randn(dims[i], dims[i+1]) * scale\n",
    "            self.weights[f'b{i}'] = np.zeros(dims[i+1])\n",
    "        \n",
    "        # Classifier layer\n",
    "        scale = np.sqrt(2.0 / hidden_dims[-1])\n",
    "        self.weights['W_out'] = np.random.randn(hidden_dims[-1], output_dim) * scale\n",
    "        self.weights['b_out'] = np.zeros(output_dim)\n",
    "        \n",
    "        self.n_feature_layers = len(hidden_dims)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def extract_features(self, X):\n",
    "        \"\"\"Forward pass through feature extractor only.\"\"\"\n",
    "        activations = [X]\n",
    "        h = X\n",
    "        \n",
    "        for i in range(self.n_feature_layers):\n",
    "            z = h @ self.weights[f'W{i}'] + self.weights[f'b{i}']\n",
    "            h = self.relu(z)\n",
    "            activations.append(h)\n",
    "        \n",
    "        return h, activations\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Full forward pass.\"\"\"\n",
    "        features, activations = self.extract_features(X)\n",
    "        logits = features @ self.weights['W_out'] + self.weights['b_out']\n",
    "        probs = softmax(logits, axis=1)\n",
    "        return probs, features, activations\n",
    "    \n",
    "    def compute_loss(self, X, y, reg_lambda=0.001):\n",
    "        \"\"\"Compute cross-entropy loss with L2 regularization.\"\"\"\n",
    "        probs, _, _ = self.forward(X)\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        log_probs = -np.log(probs[np.arange(n_samples), y] + 1e-10)\n",
    "        ce_loss = np.mean(log_probs)\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_loss = 0\n",
    "        for key in self.weights:\n",
    "            if key.startswith('W'):\n",
    "                l2_loss += np.sum(self.weights[key] ** 2)\n",
    "        l2_loss *= reg_lambda / 2\n",
    "        \n",
    "        return ce_loss + l2_loss\n",
    "    \n",
    "    def compute_gradients(self, X, y, reg_lambda=0.001, frozen_layers=None):\n",
    "        \"\"\"Compute gradients via backpropagation.\"\"\"\n",
    "        if frozen_layers is None:\n",
    "            frozen_layers = []\n",
    "        \n",
    "        n_samples = len(y)\n",
    "        probs, features, activations = self.forward(X)\n",
    "        \n",
    "        gradients = {}\n",
    "        \n",
    "        # Gradient of softmax + cross-entropy\n",
    "        dlogits = probs.copy()\n",
    "        dlogits[np.arange(n_samples), y] -= 1\n",
    "        dlogits /= n_samples\n",
    "        \n",
    "        # Classifier layer gradients\n",
    "        gradients['W_out'] = features.T @ dlogits + reg_lambda * self.weights['W_out']\n",
    "        gradients['b_out'] = np.sum(dlogits, axis=0)\n",
    "        \n",
    "        # Backpropagate through feature extractor\n",
    "        dh = dlogits @ self.weights['W_out'].T\n",
    "        \n",
    "        for i in range(self.n_feature_layers - 1, -1, -1):\n",
    "            # ReLU derivative\n",
    "            dz = dh * self.relu_derivative(activations[i+1])\n",
    "            \n",
    "            if i not in frozen_layers:\n",
    "                gradients[f'W{i}'] = activations[i].T @ dz + reg_lambda * self.weights[f'W{i}']\n",
    "                gradients[f'b{i}'] = np.sum(dz, axis=0)\n",
    "            else:\n",
    "                gradients[f'W{i}'] = np.zeros_like(self.weights[f'W{i}'])\n",
    "                gradients[f'b{i}'] = np.zeros_like(self.weights[f'b{i}'])\n",
    "            \n",
    "            if i > 0:\n",
    "                dh = dz @ self.weights[f'W{i}'].T\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        probs, _, _ = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"Compute classification accuracy.\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    \n",
    "    def copy_weights(self):\n",
    "        \"\"\"Return a deep copy of the weights.\"\"\"\n",
    "        return {k: v.copy() for k, v in self.weights.items()}\n",
    "    \n",
    "    def load_weights(self, weights):\n",
    "        \"\"\"Load weights from a dictionary.\"\"\"\n",
    "        for k, v in weights.items():\n",
    "            self.weights[k] = v.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, n_epochs=100, learning_rate=0.01, batch_size=32, \n",
    "                reg_lambda=0.001, frozen_layers=None, layer_lrs=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the neural network using mini-batch gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : SimpleNeuralNetwork\n",
    "        The neural network to train\n",
    "    X : ndarray\n",
    "        Training features\n",
    "    y : ndarray\n",
    "        Training labels\n",
    "    n_epochs : int\n",
    "        Number of training epochs\n",
    "    learning_rate : float\n",
    "        Base learning rate\n",
    "    batch_size : int\n",
    "        Mini-batch size\n",
    "    reg_lambda : float\n",
    "        L2 regularization coefficient\n",
    "    frozen_layers : list or None\n",
    "        List of layer indices to freeze\n",
    "    layer_lrs : dict or None\n",
    "        Dictionary mapping layer names to learning rate multipliers\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    history : dict\n",
    "        Training history with losses and accuracies\n",
    "    \"\"\"\n",
    "    if frozen_layers is None:\n",
    "        frozen_layers = []\n",
    "    if layer_lrs is None:\n",
    "        layer_lrs = {}\n",
    "    \n",
    "    n_samples = len(y)\n",
    "    n_batches = max(1, n_samples // batch_size)\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle data\n",
    "        idx = np.random.permutation(n_samples)\n",
    "        X_shuffled = X[idx]\n",
    "        y_shuffled = y[idx]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in range(n_batches):\n",
    "            start = batch * batch_size\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # Compute gradients\n",
    "            gradients = model.compute_gradients(X_batch, y_batch, reg_lambda, frozen_layers)\n",
    "            \n",
    "            # Update weights\n",
    "            for key in model.weights:\n",
    "                lr = learning_rate * layer_lrs.get(key, 1.0)\n",
    "                model.weights[key] -= lr * gradients[key]\n",
    "            \n",
    "            epoch_loss += model.compute_loss(X_batch, y_batch, reg_lambda)\n",
    "        \n",
    "        epoch_loss /= n_batches\n",
    "        accuracy = model.accuracy(X, y)\n",
    "        \n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Comparing Transfer Learning Strategies\n",
    "\n",
    "We will compare four approaches:\n",
    "1. **From Scratch**: Training on target data only without transfer\n",
    "2. **Feature Extraction**: Frozen backbone, only train classifier\n",
    "3. **Full Fine-Tuning**: Update all layers with same learning rate\n",
    "4. **Discriminative Fine-Tuning**: Layer-wise learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture\n",
    "hidden_dims = [32, 16]\n",
    "\n",
    "# Step 1: Pre-train on source domain\n",
    "print(\"=\"*60)\n",
    "print(\"Pre-training on Source Domain\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "source_model = SimpleNeuralNetwork(n_features, hidden_dims, n_classes)\n",
    "source_history = train_model(\n",
    "    source_model, X_source, y_source, \n",
    "    n_epochs=100, learning_rate=0.1, batch_size=64\n",
    ")\n",
    "\n",
    "print(f\"\\nSource domain final accuracy: {source_model.accuracy(X_source, y_source):.4f}\")\n",
    "print(f\"Source model on target domain (no adaptation): {source_model.accuracy(X_target, y_target):.4f}\")\n",
    "\n",
    "# Save pre-trained weights\n",
    "pretrained_weights = source_model.copy_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compare transfer learning strategies on target domain\n",
    "results = {}\n",
    "\n",
    "# Strategy 1: Training from scratch (no transfer)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Strategy 1: Training from Scratch (No Transfer)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scratch_model = SimpleNeuralNetwork(n_features, hidden_dims, n_classes)\n",
    "scratch_history = train_model(\n",
    "    scratch_model, X_target, y_target,\n",
    "    n_epochs=100, learning_rate=0.1, batch_size=16\n",
    ")\n",
    "results['From Scratch'] = {\n",
    "    'history': scratch_history,\n",
    "    'final_accuracy': scratch_model.accuracy(X_target, y_target)\n",
    "}\n",
    "print(f\"Final accuracy: {results['From Scratch']['final_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Feature Extraction (frozen backbone)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Strategy 2: Feature Extraction (Frozen Backbone)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_model = SimpleNeuralNetwork(n_features, hidden_dims, n_classes)\n",
    "feature_model.load_weights(pretrained_weights)\n",
    "\n",
    "# Reinitialize classifier layer\n",
    "scale = np.sqrt(2.0 / hidden_dims[-1])\n",
    "feature_model.weights['W_out'] = np.random.randn(hidden_dims[-1], n_classes) * scale\n",
    "feature_model.weights['b_out'] = np.zeros(n_classes)\n",
    "\n",
    "# Freeze all feature extractor layers\n",
    "frozen_layers = list(range(len(hidden_dims)))\n",
    "\n",
    "feature_history = train_model(\n",
    "    feature_model, X_target, y_target,\n",
    "    n_epochs=100, learning_rate=0.1, batch_size=16,\n",
    "    frozen_layers=frozen_layers\n",
    ")\n",
    "results['Feature Extraction'] = {\n",
    "    'history': feature_history,\n",
    "    'final_accuracy': feature_model.accuracy(X_target, y_target)\n",
    "}\n",
    "print(f\"Final accuracy: {results['Feature Extraction']['final_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Full Fine-Tuning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Strategy 3: Full Fine-Tuning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "full_ft_model = SimpleNeuralNetwork(n_features, hidden_dims, n_classes)\n",
    "full_ft_model.load_weights(pretrained_weights)\n",
    "\n",
    "# Reinitialize classifier layer\n",
    "scale = np.sqrt(2.0 / hidden_dims[-1])\n",
    "full_ft_model.weights['W_out'] = np.random.randn(hidden_dims[-1], n_classes) * scale\n",
    "full_ft_model.weights['b_out'] = np.zeros(n_classes)\n",
    "\n",
    "full_ft_history = train_model(\n",
    "    full_ft_model, X_target, y_target,\n",
    "    n_epochs=100, learning_rate=0.01, batch_size=16  # Lower learning rate\n",
    ")\n",
    "results['Full Fine-Tuning'] = {\n",
    "    'history': full_ft_history,\n",
    "    'final_accuracy': full_ft_model.accuracy(X_target, y_target)\n",
    "}\n",
    "print(f\"Final accuracy: {results['Full Fine-Tuning']['final_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 4: Discriminative Fine-Tuning (layer-wise learning rates)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Strategy 4: Discriminative Fine-Tuning (Layer-wise LR)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "disc_model = SimpleNeuralNetwork(n_features, hidden_dims, n_classes)\n",
    "disc_model.load_weights(pretrained_weights)\n",
    "\n",
    "# Reinitialize classifier layer\n",
    "scale = np.sqrt(2.0 / hidden_dims[-1])\n",
    "disc_model.weights['W_out'] = np.random.randn(hidden_dims[-1], n_classes) * scale\n",
    "disc_model.weights['b_out'] = np.zeros(n_classes)\n",
    "\n",
    "# Layer-wise learning rate decay (gamma = 0.5)\n",
    "gamma = 0.5\n",
    "layer_lrs = {\n",
    "    'W0': gamma**2, 'b0': gamma**2,  # First layer (smallest LR)\n",
    "    'W1': gamma**1, 'b1': gamma**1,  # Second layer\n",
    "    'W_out': 1.0, 'b_out': 1.0       # Classifier (full LR)\n",
    "}\n",
    "\n",
    "disc_history = train_model(\n",
    "    disc_model, X_target, y_target,\n",
    "    n_epochs=100, learning_rate=0.05, batch_size=16,\n",
    "    layer_lrs=layer_lrs\n",
    ")\n",
    "results['Discriminative FT'] = {\n",
    "    'history': disc_history,\n",
    "    'final_accuracy': disc_model.accuracy(X_target, y_target)\n",
    "}\n",
    "print(f\"Final accuracy: {results['Discriminative FT']['final_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "colors = {\n",
    "    'From Scratch': '#e74c3c',\n",
    "    'Feature Extraction': '#3498db',\n",
    "    'Full Fine-Tuning': '#2ecc71',\n",
    "    'Discriminative FT': '#9b59b6'\n",
    "}\n",
    "\n",
    "# Plot 1: Training Loss Curves\n",
    "ax1 = axes[0, 0]\n",
    "for name, data in results.items():\n",
    "    ax1.plot(data['history']['loss'], label=name, color=colors[name], linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss Comparison')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 100)\n",
    "\n",
    "# Plot 2: Training Accuracy Curves\n",
    "ax2 = axes[0, 1]\n",
    "for name, data in results.items():\n",
    "    ax2.plot(data['history']['accuracy'], label=name, color=colors[name], linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Accuracy Comparison')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 100)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "# Plot 3: Final Accuracy Bar Chart\n",
    "ax3 = axes[1, 0]\n",
    "names = list(results.keys())\n",
    "accuracies = [results[name]['final_accuracy'] for name in names]\n",
    "bar_colors = [colors[name] for name in names]\n",
    "bars = ax3.bar(names, accuracies, color=bar_colors, edgecolor='black', linewidth=1.5)\n",
    "ax3.set_ylabel('Final Accuracy')\n",
    "ax3.set_title('Final Accuracy by Strategy')\n",
    "ax3.set_ylim(0, 1.0)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "ax3.set_xticklabels(names, rotation=15, ha='right')\n",
    "\n",
    "# Plot 4: Convergence Speed (epochs to reach 80% of final accuracy)\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "def epochs_to_threshold(history, threshold_fraction=0.9):\n",
    "    \"\"\"Find number of epochs to reach threshold fraction of final accuracy.\"\"\"\n",
    "    final_acc = history['accuracy'][-1]\n",
    "    threshold = threshold_fraction * final_acc\n",
    "    for i, acc in enumerate(history['accuracy']):\n",
    "        if acc >= threshold:\n",
    "            return i + 1\n",
    "    return len(history['accuracy'])\n",
    "\n",
    "convergence_epochs = [epochs_to_threshold(results[name]['history']) for name in names]\n",
    "bars = ax4.bar(names, convergence_epochs, color=bar_colors, edgecolor='black', linewidth=1.5)\n",
    "ax4.set_ylabel('Epochs to 90% of Final Accuracy')\n",
    "ax4.set_title('Convergence Speed Comparison')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, epochs in zip(bars, convergence_epochs):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{epochs}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax4.set_xticklabels(names, rotation=15, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Findings\n",
    "\n",
    "### Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRANSFER LEARNING EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSource domain samples: {n_source}\")\n",
    "print(f\"Target domain samples: {n_target} (10x less data)\")\n",
    "print(f\"\\n{'Strategy':<25} {'Final Accuracy':<18} {'Convergence (epochs)'}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for name in names:\n",
    "    acc = results[name]['final_accuracy']\n",
    "    conv = epochs_to_threshold(results[name]['history'])\n",
    "    print(f\"{name:<25} {acc:<18.4f} {conv}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY OBSERVATIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. **Transfer Learning Benefit**: All transfer learning strategies outperform\n",
    "   training from scratch when target data is limited.\n",
    "\n",
    "2. **Feature Extraction**: Provides stable, fast convergence by leveraging\n",
    "   learned representations. Best when source and target domains are similar.\n",
    "\n",
    "3. **Full Fine-Tuning**: Can achieve highest accuracy but requires careful\n",
    "   learning rate selection to avoid catastrophic forgetting.\n",
    "\n",
    "4. **Discriminative Fine-Tuning**: Balances adaptation and preservation by\n",
    "   using smaller learning rates for earlier layers that capture general features.\n",
    "\n",
    "5. **Convergence Speed**: Transfer learning methods typically converge faster\n",
    "   than training from scratch due to better initialization.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Summary\n",
    "\n",
    "The key insight of transfer learning is that neural networks learn hierarchical representations:\n",
    "\n",
    "- **Early layers** learn general, task-agnostic features (edges, textures, basic patterns)\n",
    "- **Later layers** learn task-specific features\n",
    "\n",
    "The optimal fine-tuning strategy depends on:\n",
    "\n",
    "1. **Domain similarity**: $\\text{sim}(\\mathcal{D}_S, \\mathcal{D}_T)$\n",
    "   - High similarity → Feature extraction or conservative fine-tuning\n",
    "   - Low similarity → Full fine-tuning with domain adaptation\n",
    "\n",
    "2. **Target data size**: $|\\mathcal{D}_T|$\n",
    "   - Small → Freeze more layers to prevent overfitting\n",
    "   - Large → Fine-tune more layers for better adaptation\n",
    "\n",
    "The discriminative fine-tuning approach provides a principled middle ground by respecting the hierarchical nature of learned representations:\n",
    "\n",
    "$$\\eta_l = \\eta_{\\text{base}} \\cdot \\gamma^{L-l}, \\quad \\gamma \\in (0, 1)$$\n",
    "\n",
    "This allows task-specific layers to adapt quickly while preserving the general-purpose features that transfer well across domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
