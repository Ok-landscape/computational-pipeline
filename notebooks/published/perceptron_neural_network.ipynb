{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Neural Network\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **perceptron** is the fundamental building block of artificial neural networks, introduced by Frank Rosenblatt in 1958. It represents a simplified model of a biological neuron and serves as a binary linear classifier.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### The Perceptron Model\n",
    "\n",
    "A perceptron takes a vector of inputs $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ and produces a binary output. The computation proceeds in two stages:\n",
    "\n",
    "**1. Weighted Sum (Net Input):**\n",
    "\n",
    "$$z = \\sum_{i=1}^{n} w_i x_i + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{w} = (w_1, w_2, \\ldots, w_n)$ are the synaptic weights\n",
    "- $b$ is the bias term (threshold)\n",
    "\n",
    "**2. Activation Function (Heaviside Step):**\n",
    "\n",
    "$$\\hat{y} = \\phi(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0 \\end{cases}$$\n",
    "\n",
    "### Learning Algorithm\n",
    "\n",
    "The perceptron learns through an iterative weight update rule. For each training sample $(\\mathbf{x}^{(i)}, y^{(i)})$:\n",
    "\n",
    "$$w_j \\leftarrow w_j + \\eta \\cdot (y^{(i)} - \\hat{y}^{(i)}) \\cdot x_j^{(i)}$$\n",
    "\n",
    "$$b \\leftarrow b + \\eta \\cdot (y^{(i)} - \\hat{y}^{(i)})$$\n",
    "\n",
    "where $\\eta$ is the learning rate (typically $0 < \\eta \\leq 1$).\n",
    "\n",
    "### Convergence Theorem\n",
    "\n",
    "**Perceptron Convergence Theorem:** If the training data is linearly separable, the perceptron learning algorithm will converge to a solution in a finite number of steps.\n",
    "\n",
    "The decision boundary is a hyperplane defined by:\n",
    "\n",
    "$$\\mathbf{w}^T \\mathbf{x} + b = 0$$\n",
    "\n",
    "In 2D, this simplifies to the line:\n",
    "\n",
    "$$w_1 x_1 + w_2 x_2 + b = 0$$\n",
    "\n",
    "### Geometric Interpretation\n",
    "\n",
    "The weight vector $\\mathbf{w}$ is perpendicular to the decision boundary, and the bias $b$ controls the offset from the origin. Points on one side of the hyperplane are classified as class 1, while points on the other side are classified as class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Implementation\n",
    "\n",
    "We implement a Perceptron class with methods for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Single-layer Perceptron classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, n_iterations=100):\n",
    "        \"\"\"\n",
    "        Initialize the Perceptron.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        learning_rate : float\n",
    "            Learning rate (between 0.0 and 1.0)\n",
    "        n_iterations : int\n",
    "            Number of passes over the training dataset\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors_per_epoch = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training vectors\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values (0 or 1)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias to zero\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "        self.errors_per_epoch = []\n",
    "        \n",
    "        for epoch in range(self.n_iterations):\n",
    "            errors = 0\n",
    "            for xi, yi in zip(X, y):\n",
    "                # Compute prediction\n",
    "                prediction = self.predict_single(xi)\n",
    "                \n",
    "                # Calculate error\n",
    "                error = yi - prediction\n",
    "                \n",
    "                # Update weights and bias\n",
    "                update = self.learning_rate * error\n",
    "                self.weights += update * xi\n",
    "                self.bias += update\n",
    "                \n",
    "                # Count misclassifications\n",
    "                errors += int(error != 0)\n",
    "            \n",
    "            self.errors_per_epoch.append(errors)\n",
    "            \n",
    "            # Early stopping if no errors\n",
    "            if errors == 0:\n",
    "                print(f\"Converged after {epoch + 1} epochs\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input z = w^T * x + b\"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        \"\"\"Return class label for a single sample\"\"\"\n",
    "        return 1 if self.net_input(x) >= 0 else 0\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label for samples in X\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Linearly Separable Dataset\n",
    "\n",
    "We create a synthetic dataset with two classes that are linearly separable. This demonstrates the perceptron's ability to find a decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linearly_separable_data(n_samples=100, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate linearly separable 2D data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples per class\n",
    "    noise : float\n",
    "        Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray\n",
    "        Feature matrix\n",
    "    y : ndarray\n",
    "        Class labels\n",
    "    \"\"\"\n",
    "    # Class 0: centered at (-1.5, -1.5)\n",
    "    X0 = np.random.randn(n_samples, 2) * noise + np.array([-1.5, -1.5])\n",
    "    \n",
    "    # Class 1: centered at (1.5, 1.5)\n",
    "    X1 = np.random.randn(n_samples, 2) * noise + np.array([1.5, 1.5])\n",
    "    \n",
    "    # Combine data\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.hstack([np.zeros(n_samples), np.ones(n_samples)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate dataset\n",
    "X, y = generate_linearly_separable_data(n_samples=50, noise=0.5)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train perceptron\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iterations=50)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Display learned parameters\n",
    "print(f\"\\nLearned weights: w = [{perceptron.weights[0]:.4f}, {perceptron.weights[1]:.4f}]\")\n",
    "print(f\"Learned bias: b = {perceptron.bias:.4f}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = perceptron.predict(X)\n",
    "accuracy = np.mean(predictions == y) * 100\n",
    "print(f\"Training accuracy: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We create a comprehensive visualization showing:\n",
    "1. The decision boundary learned by the perceptron\n",
    "2. The convergence of errors over epochs\n",
    "3. The geometric interpretation of the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, perceptron, ax):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary and data points.\n",
    "    \"\"\"\n",
    "    # Define plot boundaries\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    # Create mesh grid\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision regions\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    ax.contour(xx, yy, Z, colors='k', linewidths=0.5)\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter0 = ax.scatter(X[y == 0, 0], X[y == 0, 1], \n",
    "                          c='blue', marker='o', s=50, \n",
    "                          edgecolors='k', label='Class 0')\n",
    "    scatter1 = ax.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "                          c='red', marker='s', s=50, \n",
    "                          edgecolors='k', label='Class 1')\n",
    "    \n",
    "    # Plot decision boundary line\n",
    "    if perceptron.weights[1] != 0:\n",
    "        x_boundary = np.linspace(x_min, x_max, 100)\n",
    "        y_boundary = -(perceptron.weights[0] * x_boundary + perceptron.bias) / perceptron.weights[1]\n",
    "        ax.plot(x_boundary, y_boundary, 'k-', linewidth=2, label='Decision Boundary')\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_title('Perceptron Decision Boundary', fontsize=14)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Decision boundary\n",
    "plot_decision_boundary(X, y, perceptron, axes[0])\n",
    "\n",
    "# Plot 2: Error convergence\n",
    "epochs = range(1, len(perceptron.errors_per_epoch) + 1)\n",
    "axes[1].plot(epochs, perceptron.errors_per_epoch, 'b-o', markersize=6)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Misclassifications', fontsize=12)\n",
    "axes[1].set_title('Perceptron Convergence', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(0, len(perceptron.errors_per_epoch) + 1)\n",
    "axes[1].set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating the XOR Problem\n",
    "\n",
    "A fundamental limitation of the single-layer perceptron is its inability to solve non-linearly separable problems. The classic example is the XOR (exclusive or) function.\n",
    "\n",
    "The XOR truth table:\n",
    "\n",
    "| $x_1$ | $x_2$ | $y$ |\n",
    "|-------|-------|-----|\n",
    "| 0     | 0     | 0   |\n",
    "| 0     | 1     | 1   |\n",
    "| 1     | 0     | 1   |\n",
    "| 1     | 1     | 0   |\n",
    "\n",
    "No single hyperplane can separate the classes $(0,0), (1,1)$ from $(0,1), (1,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR problem\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Train perceptron on XOR\n",
    "perceptron_xor = Perceptron(learning_rate=0.1, n_iterations=100)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "# Check predictions\n",
    "predictions_xor = perceptron_xor.predict(X_xor)\n",
    "accuracy_xor = np.mean(predictions_xor == y_xor) * 100\n",
    "\n",
    "print(\"XOR Problem Results:\")\n",
    "print(f\"Predictions: {predictions_xor}\")\n",
    "print(f\"True labels: {y_xor}\")\n",
    "print(f\"Accuracy: {accuracy_xor:.1f}%\")\n",
    "print(f\"\\nThe perceptron fails to solve XOR because it is not linearly separable.\")\n",
    "print(f\"Final errors per epoch: {perceptron_xor.errors_per_epoch[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The perceptron represents a foundational concept in neural networks and machine learning:\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Linear Classification**: The perceptron finds a hyperplane that separates two classes\n",
    "\n",
    "2. **Convergence Guarantee**: For linearly separable data, the algorithm is guaranteed to converge\n",
    "\n",
    "3. **Limitations**: Cannot solve non-linearly separable problems (XOR problem)\n",
    "\n",
    "4. **Historical Importance**: The perceptron's limitations motivated the development of multi-layer networks (MLPs) and the backpropagation algorithm\n",
    "\n",
    "**Mathematical Summary:**\n",
    "\n",
    "$$\\hat{y} = \\phi\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)$$\n",
    "\n",
    "where $\\phi$ is the Heaviside step function. The perceptron learning rule updates weights proportionally to the error signal, enabling supervised learning of linear decision boundaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
