{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Neural Network\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **perceptron** is the simplest form of a neural network, introduced by Frank Rosenblatt in 1958. It serves as the fundamental building block for understanding more complex neural architectures and provides key insights into linear classification problems.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### The Perceptron Model\n",
    "\n",
    "A perceptron takes multiple input signals $x_1, x_2, \\ldots, x_n$ and produces a single binary output. The mathematical model is defined as:\n",
    "\n",
    "$$y = \\phi\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = \\phi(\\mathbf{w}^T \\mathbf{x} + b)$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T$ is the input vector\n",
    "- $\\mathbf{w} = [w_1, w_2, \\ldots, w_n]^T$ is the weight vector\n",
    "- $b$ is the bias term\n",
    "- $\\phi$ is the activation function\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "The classical perceptron uses the **Heaviside step function** as its activation:\n",
    "\n",
    "$$\\phi(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0 \\end{cases}$$\n",
    "\n",
    "This produces a binary classification: the perceptron \"fires\" (outputs 1) when the weighted sum exceeds the threshold.\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "The perceptron defines a **hyperplane** in the input space that separates the two classes. For a 2D input space, the decision boundary is:\n",
    "\n",
    "$$w_1 x_1 + w_2 x_2 + b = 0$$\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "$$x_2 = -\\frac{w_1}{w_2} x_1 - \\frac{b}{w_2}$$\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "### Perceptron Learning Rule\n",
    "\n",
    "The perceptron learns by adjusting its weights based on classification errors. For each training sample $(\\mathbf{x}^{(i)}, y^{(i)})$:\n",
    "\n",
    "1. Compute the prediction: $\\hat{y}^{(i)} = \\phi(\\mathbf{w}^T \\mathbf{x}^{(i)} + b)$\n",
    "\n",
    "2. Update weights if misclassified:\n",
    "   $$\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta (y^{(i)} - \\hat{y}^{(i)}) \\mathbf{x}^{(i)}$$\n",
    "   $$b \\leftarrow b + \\eta (y^{(i)} - \\hat{y}^{(i)})$$\n",
    "\n",
    "where $\\eta$ is the **learning rate**.\n",
    "\n",
    "### Convergence Theorem\n",
    "\n",
    "The **Perceptron Convergence Theorem** states that if the training data is **linearly separable**, the perceptron learning algorithm will converge to a solution in a finite number of steps. The number of updates is bounded by:\n",
    "\n",
    "$$k \\leq \\frac{R^2 \\|\\mathbf{w}^*\\|^2}{\\gamma^2}$$\n",
    "\n",
    "where:\n",
    "- $R = \\max_i \\|\\mathbf{x}^{(i)}\\|$ is the maximum norm of input vectors\n",
    "- $\\mathbf{w}^*$ is any separating weight vector\n",
    "- $\\gamma = \\min_i y^{(i)}(\\mathbf{w}^{*T} \\mathbf{x}^{(i)})$ is the margin\n",
    "\n",
    "## Limitations\n",
    "\n",
    "The perceptron can only solve **linearly separable** problems. The famous example of XOR demonstrates this limitationâ€”no single hyperplane can separate the XOR truth table. This led to the development of **multi-layer perceptrons (MLPs)** which can learn non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement a perceptron from scratch and demonstrate its learning capabilities on a linearly separable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Perceptron classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Number of passes over the training dataset\n",
    "    random_state : int\n",
    "        Random number generator seed for weight initialization\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting\n",
    "    b_ : float\n",
    "        Bias unit after fitting\n",
    "    errors_ : list\n",
    "        Number of misclassifications in each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.1, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training vectors\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values (0 or 1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = 0.0\n",
    "        self.errors_ = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_ += update * xi\n",
    "                self.b_ += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input: z = w^T x + b\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset\n",
    "\n",
    "We create a linearly separable dataset consisting of two classes with distinct cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linearly separable data\n",
    "n_samples = 100\n",
    "\n",
    "# Class 0: centered at (-2, -2)\n",
    "X_class0 = np.random.randn(n_samples // 2, 2) * 0.8 + np.array([-2, -2])\n",
    "y_class0 = np.zeros(n_samples // 2)\n",
    "\n",
    "# Class 1: centered at (2, 2)\n",
    "X_class1 = np.random.randn(n_samples // 2, 2) * 0.8 + np.array([2, 2])\n",
    "y_class1 = np.ones(n_samples // 2)\n",
    "\n",
    "# Combine the data\n",
    "X = np.vstack([X_class0, X_class1])\n",
    "y = np.hstack([y_class0, y_class1])\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffle_idx = np.random.permutation(n_samples)\n",
    "X = X[shuffle_idx]\n",
    "y = y[shuffle_idx]\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the perceptron\n",
    "ppn = Perceptron(eta=0.1, n_iter=20)\n",
    "ppn.fit(X, y)\n",
    "\n",
    "print(f\"Learned weights: w1 = {ppn.w_[0]:.4f}, w2 = {ppn.w_[1]:.4f}\")\n",
    "print(f\"Learned bias: b = {ppn.b_:.4f}\")\n",
    "print(f\"Final accuracy: {np.mean(ppn.predict(X) == y) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We create a comprehensive visualization showing:\n",
    "1. The decision boundary and data points\n",
    "2. The classification regions\n",
    "3. The convergence of the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    \"\"\"\n",
    "    Plot decision regions for a 2D dataset.\n",
    "    \"\"\"\n",
    "    # Setup marker generator and color map\n",
    "    markers = ('o', 's')\n",
    "    colors = ('lightblue', 'lightcoral')\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                          np.arange(x2_min, x2_max, resolution))\n",
    "    \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    \n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # Plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                   y=X[y == cl, 1],\n",
    "                   alpha=0.8, \n",
    "                   c=[colors[idx]],\n",
    "                   marker=markers[idx], \n",
    "                   label=f'Class {int(cl)}',\n",
    "                   edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the comprehensive visualization\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Decision boundary and regions\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "plot_decision_regions(X, y, ppn)\n",
    "\n",
    "# Plot the decision boundary line\n",
    "x_boundary = np.array([X[:, 0].min() - 1, X[:, 0].max() + 1])\n",
    "y_boundary = -(ppn.w_[0] * x_boundary + ppn.b_) / ppn.w_[1]\n",
    "ax1.plot(x_boundary, y_boundary, 'k--', linewidth=2, label='Decision boundary')\n",
    "\n",
    "ax1.set_xlabel('$x_1$', fontsize=12)\n",
    "ax1.set_ylabel('$x_2$', fontsize=12)\n",
    "ax1.set_title('Perceptron Decision Regions', fontsize=14)\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot 2: Convergence (errors per epoch)\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "ax2.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o', \n",
    "         color='steelblue', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Number of Misclassifications', fontsize=12)\n",
    "ax2.set_title('Perceptron Convergence', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Weight vector visualization\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "\n",
    "# Plot data points\n",
    "ax3.scatter(X[y == 0, 0], X[y == 0, 1], c='lightblue', marker='o', \n",
    "           edgecolor='black', alpha=0.6, label='Class 0')\n",
    "ax3.scatter(X[y == 1, 0], X[y == 1, 1], c='lightcoral', marker='s', \n",
    "           edgecolor='black', alpha=0.6, label='Class 1')\n",
    "\n",
    "# Plot the weight vector (perpendicular to decision boundary)\n",
    "origin = np.array([0, 0])\n",
    "weight_scale = 1.5\n",
    "ax3.quiver(origin[0], origin[1], ppn.w_[0] * weight_scale, ppn.w_[1] * weight_scale,\n",
    "          angles='xy', scale_units='xy', scale=1, color='red', \n",
    "          width=0.02, label=f'Weight vector $\\\\mathbf{{w}}$')\n",
    "\n",
    "# Decision boundary\n",
    "ax3.plot(x_boundary, y_boundary, 'k--', linewidth=2)\n",
    "\n",
    "ax3.set_xlabel('$x_1$', fontsize=12)\n",
    "ax3.set_ylabel('$x_2$', fontsize=12)\n",
    "ax3.set_title('Weight Vector Visualization', fontsize=14)\n",
    "ax3.set_xlim(X[:, 0].min() - 1, X[:, 0].max() + 1)\n",
    "ax3.set_ylim(X[:, 1].min() - 1, X[:, 1].max() + 1)\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Problem Demonstration\n",
    "\n",
    "To illustrate the fundamental limitation of a single-layer perceptron, we demonstrate its inability to solve the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])  # XOR truth table\n",
    "\n",
    "# Train perceptron on XOR\n",
    "ppn_xor = Perceptron(eta=0.1, n_iter=100)\n",
    "ppn_xor.fit(X_xor, y_xor)\n",
    "\n",
    "# Check accuracy\n",
    "predictions = ppn_xor.predict(X_xor)\n",
    "accuracy = np.mean(predictions == y_xor) * 100\n",
    "\n",
    "print(\"XOR Problem:\")\n",
    "print(\"-\" * 30)\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"Input: {X_xor[i]} | Target: {y_xor[i]} | Prediction: {predictions[i]}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Accuracy: {accuracy:.1f}%\")\n",
    "print(f\"\\nFinal errors per epoch: {ppn_xor.errors_[-5:]}\")\n",
    "print(\"\\nThe perceptron fails to converge on the XOR problem,\")\n",
    "print(\"demonstrating that it cannot learn non-linearly separable patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. **Derived the mathematical model** of the perceptron, including its activation function and decision boundary\n",
    "\n",
    "2. **Implemented the perceptron learning algorithm** from scratch using NumPy\n",
    "\n",
    "3. **Trained the perceptron** on a linearly separable dataset and observed convergence\n",
    "\n",
    "4. **Visualized** the decision regions, convergence behavior, and weight vector interpretation\n",
    "\n",
    "5. **Demonstrated the XOR limitation**, showing why multi-layer networks are necessary for non-linear problems\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The perceptron is a **linear classifier** that finds a hyperplane to separate classes\n",
    "- It **converges** on linearly separable data but **fails** on non-linearly separable problems\n",
    "- The **weight vector** is perpendicular to the decision boundary\n",
    "- Despite its limitations, the perceptron laid the foundation for modern deep learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
