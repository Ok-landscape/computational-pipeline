{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Basics: Attention Is All You Need\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Transformer architecture, introduced by Vaswani et al. (2017), revolutionized sequence-to-sequence modeling by replacing recurrent mechanisms entirely with attention mechanisms. This notebook provides a rigorous introduction to the core mathematical concepts underlying Transformers.\n",
    "\n",
    "## 1. Self-Attention Mechanism\n",
    "\n",
    "The fundamental operation in a Transformer is **scaled dot-product attention**. Given input sequences represented as matrices, we compute three projections:\n",
    "\n",
    "- **Query**: $Q = XW^Q$\n",
    "- **Key**: $K = XW^K$\n",
    "- **Value**: $V = XW^V$\n",
    "\n",
    "where $X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$ is the input matrix with $n$ tokens and embedding dimension $d_{\\text{model}}$, and $W^Q, W^K, W^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$ are learnable weight matrices.\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "The attention function is computed as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The scaling factor $\\sqrt{d_k}$ prevents the dot products from growing too large in magnitude, which would push the softmax into regions with extremely small gradients.\n",
    "\n",
    "### Why Scaling Matters\n",
    "\n",
    "For large values of $d_k$, the dot products $q \\cdot k$ can have large variance. Assuming $q$ and $k$ are vectors with components drawn from a distribution with mean 0 and variance 1, their dot product has:\n",
    "\n",
    "$$\\mathbb{E}[q \\cdot k] = 0, \\quad \\text{Var}(q \\cdot k) = d_k$$\n",
    "\n",
    "Dividing by $\\sqrt{d_k}$ normalizes the variance to 1.\n",
    "\n",
    "## 2. Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention function, we linearly project the queries, keys, and values $h$ times with different learned projections:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "\n",
    "where each head is computed as:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "with $W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.\n",
    "\n",
    "Typically, $d_k = d_v = d_{\\text{model}}/h$, so the computational cost is similar to single-head attention.\n",
    "\n",
    "## 3. Positional Encoding\n",
    "\n",
    "Since the Transformer contains no recurrence, we must inject positional information. The original paper uses sinusoidal positional encodings:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension index. This encoding allows the model to learn relative positions since:\n",
    "\n",
    "$$PE_{pos+k} = f(PE_{pos})$$\n",
    "\n",
    "can be represented as a linear function of $PE_{pos}$.\n",
    "\n",
    "## 4. Feed-Forward Network\n",
    "\n",
    "Each Transformer layer includes a position-wise feed-forward network:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "This is applied to each position separately and identically, with inner dimension $d_{ff}$ (typically $4 \\times d_{\\text{model}}$).\n",
    "\n",
    "## 5. Layer Normalization and Residual Connections\n",
    "\n",
    "Each sub-layer (attention and FFN) is wrapped with:\n",
    "\n",
    "$$\\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
    "\n",
    "Layer normalization normalizes across features:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation computed across the last dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 64      # Model dimension\n",
    "d_k = 16          # Key/Query dimension\n",
    "d_v = 16          # Value dimension\n",
    "n_heads = 4       # Number of attention heads\n",
    "seq_len = 10      # Sequence length\n",
    "d_ff = 256        # Feed-forward inner dimension\n",
    "\n",
    "print(\"Transformer Configuration:\")\n",
    "print(f\"  Model dimension (d_model): {d_model}\")\n",
    "print(f\"  Key/Query dimension (d_k): {d_k}\")\n",
    "print(f\"  Value dimension (d_v): {d_v}\")\n",
    "print(f\"  Number of heads: {n_heads}\")\n",
    "print(f\"  Sequence length: {seq_len}\")\n",
    "print(f\"  Feed-forward dimension: {d_ff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix of shape (seq_len, d_k)\n",
    "        K: Key matrix of shape (seq_len, d_k)\n",
    "        V: Value matrix of shape (seq_len, d_v)\n",
    "        mask: Optional mask for causal attention\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output of shape (seq_len, d_v)\n",
    "        attention_weights: Attention weights of shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores: QK^T / sqrt(d_k)\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided (for causal attention)\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Compute output\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test scaled dot-product attention\n",
    "Q_test = np.random.randn(seq_len, d_k)\n",
    "K_test = np.random.randn(seq_len, d_k)\n",
    "V_test = np.random.randn(seq_len, d_v)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "print(f\"Input Q shape: {Q_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention weights sum per row (should be 1.0): {attn_weights.sum(axis=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_k, d_v):\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        # Initialize projection weights\n",
    "        self.W_Q = [np.random.randn(d_model, d_k) * 0.1 for _ in range(n_heads)]\n",
    "        self.W_K = [np.random.randn(d_model, d_k) * 0.1 for _ in range(n_heads)]\n",
    "        self.W_V = [np.random.randn(d_model, d_v) * 0.1 for _ in range(n_heads)]\n",
    "        self.W_O = np.random.randn(n_heads * d_v, d_model) * 0.1\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            X: Input of shape (seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Output of shape (seq_len, d_model)\n",
    "            attention_weights: List of attention weights for each head\n",
    "        \"\"\"\n",
    "        heads = []\n",
    "        attention_weights_all = []\n",
    "        \n",
    "        for i in range(self.n_heads):\n",
    "            # Project to Q, K, V\n",
    "            Q = np.matmul(X, self.W_Q[i])\n",
    "            K = np.matmul(X, self.W_K[i])\n",
    "            V = np.matmul(X, self.W_V[i])\n",
    "            \n",
    "            # Compute attention\n",
    "            head_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "            heads.append(head_output)\n",
    "            attention_weights_all.append(attn_weights)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        concat_heads = np.concatenate(heads, axis=-1)\n",
    "        \n",
    "        # Final projection\n",
    "        output = np.matmul(concat_heads, self.W_O)\n",
    "        \n",
    "        return output, attention_weights_all\n",
    "\n",
    "# Test multi-head attention\n",
    "mha = MultiHeadAttention(d_model, n_heads, d_k, d_v)\n",
    "X_test = np.random.randn(seq_len, d_model)\n",
    "output, attn_weights_list = mha.forward(X_test)\n",
    "\n",
    "print(f\"Input shape: {X_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of attention heads: {len(attn_weights_list)}\")\n",
    "print(f\"Each head's attention weights shape: {attn_weights_list[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional encodings.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length of sequence\n",
    "        d_model: Model dimension\n",
    "    \n",
    "    Returns:\n",
    "        PE: Positional encoding matrix of shape (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    PE = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            # Compute the angle\n",
    "            angle = pos / (10000 ** (i / d_model))\n",
    "            \n",
    "            # Apply sin to even indices\n",
    "            PE[pos, i] = np.sin(angle)\n",
    "            \n",
    "            # Apply cos to odd indices\n",
    "            if i + 1 < d_model:\n",
    "                PE[pos, i + 1] = np.cos(angle)\n",
    "    \n",
    "    return PE\n",
    "\n",
    "# Generate positional encodings\n",
    "PE = positional_encoding(seq_len, d_model)\n",
    "print(f\"Positional encoding shape: {PE.shape}\")\n",
    "print(f\"\\nFirst position encoding (first 8 dims): {PE[0, :8]}\")\n",
    "print(f\"Second position encoding (first 8 dims): {PE[1, :8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, gamma=None, beta=None, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Apply layer normalization.\n",
    "    \n",
    "    Args:\n",
    "        x: Input of shape (..., d_model)\n",
    "        gamma: Scale parameter\n",
    "        beta: Shift parameter\n",
    "        eps: Small constant for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Normalized output\n",
    "    \"\"\"\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    \n",
    "    x_norm = (x - mean) / (std + eps)\n",
    "    \n",
    "    if gamma is not None and beta is not None:\n",
    "        x_norm = gamma * x_norm + beta\n",
    "    \n",
    "    return x_norm\n",
    "\n",
    "def feed_forward_network(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network.\n",
    "    \n",
    "    Args:\n",
    "        x: Input of shape (seq_len, d_model)\n",
    "        W1, b1: First layer weights and bias\n",
    "        W2, b2: Second layer weights and bias\n",
    "    \n",
    "    Returns:\n",
    "        Output of shape (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    # First linear layer with ReLU\n",
    "    hidden = np.maximum(0, np.matmul(x, W1) + b1)\n",
    "    \n",
    "    # Second linear layer\n",
    "    output = np.matmul(hidden, W2) + b2\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test layer norm\n",
    "x_test = np.random.randn(seq_len, d_model) * 10 + 5\n",
    "x_norm = layer_norm(x_test)\n",
    "print(f\"Before LayerNorm - Mean: {x_test.mean(axis=-1).mean():.4f}, Std: {x_test.std(axis=-1).mean():.4f}\")\n",
    "print(f\"After LayerNorm - Mean: {x_norm.mean(axis=-1).mean():.4f}, Std: {x_norm.std(axis=-1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer:\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_k, d_v, d_ff):\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads, d_k, d_v)\n",
    "        \n",
    "        # Feed-forward network weights\n",
    "        self.W1 = np.random.randn(d_model, d_ff) * 0.1\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) * 0.1\n",
    "        self.b2 = np.zeros(d_model)\n",
    "        \n",
    "        # Layer norm parameters\n",
    "        self.gamma1 = np.ones(d_model)\n",
    "        self.beta1 = np.zeros(d_model)\n",
    "        self.gamma2 = np.ones(d_model)\n",
    "        self.beta2 = np.zeros(d_model)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            X: Input of shape (seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            output: Output of shape (seq_len, d_model)\n",
    "            attention_weights: Attention weights from MHA\n",
    "        \"\"\"\n",
    "        # Multi-head attention with residual connection and layer norm\n",
    "        attn_output, attention_weights = self.mha.forward(X)\n",
    "        X = layer_norm(X + attn_output, self.gamma1, self.beta1)\n",
    "        \n",
    "        # Feed-forward network with residual connection and layer norm\n",
    "        ff_output = feed_forward_network(X, self.W1, self.b1, self.W2, self.b2)\n",
    "        output = layer_norm(X + ff_output, self.gamma2, self.beta2)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test encoder layer\n",
    "encoder_layer = TransformerEncoderLayer(d_model, n_heads, d_k, d_v, d_ff)\n",
    "X_test = np.random.randn(seq_len, d_model)\n",
    "output, attn_weights = encoder_layer.forward(X_test)\n",
    "\n",
    "print(f\"Input shape: {X_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output mean: {output.mean():.4f}\")\n",
    "print(f\"Output std: {output.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of scaling on attention distribution\n",
    "print(\"Effect of Scaling Factor on Attention Distribution\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate random Q, K\n",
    "Q_demo = np.random.randn(5, d_k) * np.sqrt(d_k)  # Scale up to show effect\n",
    "K_demo = np.random.randn(5, d_k) * np.sqrt(d_k)\n",
    "\n",
    "# Without scaling\n",
    "scores_unscaled = np.matmul(Q_demo, K_demo.T)\n",
    "attn_unscaled = softmax(scores_unscaled, axis=-1)\n",
    "\n",
    "# With scaling\n",
    "scores_scaled = scores_unscaled / np.sqrt(d_k)\n",
    "attn_scaled = softmax(scores_scaled, axis=-1)\n",
    "\n",
    "print(f\"\\nUnscaled scores range: [{scores_unscaled.min():.2f}, {scores_unscaled.max():.2f}]\")\n",
    "print(f\"Scaled scores range: [{scores_scaled.min():.2f}, {scores_scaled.max():.2f}]\")\n",
    "print(f\"\\nUnscaled attention entropy: {-np.sum(attn_unscaled * np.log(attn_unscaled + 1e-9), axis=-1).mean():.4f}\")\n",
    "print(f\"Scaled attention entropy: {-np.sum(attn_scaled * np.log(attn_scaled + 1e-9), axis=-1).mean():.4f}\")\n",
    "print(\"\\nHigher entropy = more distributed attention (better for learning)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Positional Encoding Visualization\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "PE_vis = positional_encoding(50, d_model)\n",
    "im1 = ax1.imshow(PE_vis.T, aspect='auto', cmap='RdBu')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Dimension')\n",
    "ax1.set_title('Positional Encoding')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# 2. Attention Weights Heatmap\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "im2 = ax2.imshow(attn_weights_list[0], cmap='viridis', aspect='auto')\n",
    "ax2.set_xlabel('Key Position')\n",
    "ax2.set_ylabel('Query Position')\n",
    "ax2.set_title('Attention Weights (Head 1)')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "# 3. Multi-head Attention Comparison\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "for i, weights in enumerate(attn_weights_list):\n",
    "    ax3.plot(weights[0, :], label=f'Head {i+1}', alpha=0.7)\n",
    "ax3.set_xlabel('Key Position')\n",
    "ax3.set_ylabel('Attention Weight')\n",
    "ax3.set_title('Attention Distribution (Query pos 0)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scaling Effect on Softmax\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "for scale in [0.5, 1.0, 2.0, 4.0]:\n",
    "    y = softmax(x_range * scale)\n",
    "    ax4.plot(x_range, y, label=f'scale={scale}')\n",
    "ax4.set_xlabel('Input Score')\n",
    "ax4.set_ylabel('Softmax Output')\n",
    "ax4.set_title('Effect of Scaling on Softmax')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Positional Encoding Sinusoids\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "positions = np.arange(50)\n",
    "for i in [0, 4, 8, 16, 32]:\n",
    "    if i < d_model:\n",
    "        ax5.plot(positions, PE_vis[:, i], label=f'dim {i}')\n",
    "ax5.set_xlabel('Position')\n",
    "ax5.set_ylabel('Encoding Value')\n",
    "ax5.set_title('Positional Encoding Sinusoids')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Causal Mask Visualization\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "causal_mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "Q_causal = np.random.randn(seq_len, d_k)\n",
    "K_causal = np.random.randn(seq_len, d_k)\n",
    "V_causal = np.random.randn(seq_len, d_v)\n",
    "_, attn_causal = scaled_dot_product_attention(Q_causal, K_causal, V_causal, causal_mask)\n",
    "im6 = ax6.imshow(attn_causal, cmap='viridis', aspect='auto')\n",
    "ax6.set_xlabel('Key Position')\n",
    "ax6.set_ylabel('Query Position')\n",
    "ax6.set_title('Causal (Masked) Attention')\n",
    "plt.colorbar(im6, ax=ax6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the core components of the Transformer architecture:\n",
    "\n",
    "1. **Scaled Dot-Product Attention**: The fundamental attention mechanism that computes weighted sums of values based on query-key compatibility.\n",
    "\n",
    "2. **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces at different positions.\n",
    "\n",
    "3. **Positional Encoding**: Sinusoidal functions that inject position information into the model without using recurrence.\n",
    "\n",
    "4. **Layer Normalization**: Stabilizes training by normalizing activations across features.\n",
    "\n",
    "5. **Feed-Forward Networks**: Position-wise fully connected layers that process each position independently.\n",
    "\n",
    "### Key Equations\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "\n",
    "### References\n",
    "\n",
    "- Vaswani, A., et al. (2017). \"Attention Is All You Need.\" *Advances in Neural Information Processing Systems*.\n",
    "- Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). \"Layer Normalization.\" *arXiv preprint arXiv:1607.06450*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
