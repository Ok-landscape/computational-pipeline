{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism: Theory and Implementation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **attention mechanism** is a fundamental component in modern deep learning architectures, particularly in natural language processing and computer vision. It allows models to dynamically focus on relevant parts of the input when producing an output, mimicking the human cognitive process of selective attention.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "The core attention mechanism computes a weighted sum of **values** ($V$) based on the compatibility between **queries** ($Q$) and **keys** ($K$). The scaled dot-product attention is defined as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "where:\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_k}$ is the query matrix\n",
    "- $K \\in \\mathbb{R}^{m \\times d_k}$ is the key matrix\n",
    "- $V \\in \\mathbb{R}^{m \\times d_v}$ is the value matrix\n",
    "- $d_k$ is the dimension of queries and keys\n",
    "- $d_v$ is the dimension of values\n",
    "\n",
    "### Attention Weights\n",
    "\n",
    "The attention weights $\\alpha$ are computed as:\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp\\left(\\frac{q_i \\cdot k_j}{\\sqrt{d_k}}\\right)}{\\sum_{l=1}^{m} \\exp\\left(\\frac{q_i \\cdot k_l}{\\sqrt{d_k}}\\right)}$$\n",
    "\n",
    "This softmax normalization ensures that $\\sum_{j=1}^{m} \\alpha_{ij} = 1$, making the weights interpretable as a probability distribution over the keys.\n",
    "\n",
    "### Scaling Factor\n",
    "\n",
    "The scaling factor $\\frac{1}{\\sqrt{d_k}}$ is crucial for preventing the dot products from growing too large in magnitude. When $d_k$ is large, the dot products can have large magnitudes, pushing the softmax into regions with extremely small gradients. The scaling counteracts this effect:\n",
    "\n",
    "$$\\text{Var}(q \\cdot k) = d_k \\cdot \\text{Var}(q_i) \\cdot \\text{Var}(k_i) \\approx d_k$$\n",
    "\n",
    "Dividing by $\\sqrt{d_k}$ normalizes the variance to approximately 1.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Multi-head attention extends the basic mechanism by allowing the model to jointly attend to information from different representation subspaces:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where each head is computed as:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "with learned projection matrices $W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$, and $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "We implement the core attention mechanism following the mathematical formulation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q : ndarray of shape (n, d_k)\n",
    "        Query matrix\n",
    "    K : ndarray of shape (m, d_k)\n",
    "        Key matrix\n",
    "    V : ndarray of shape (m, d_v)\n",
    "        Value matrix\n",
    "    mask : ndarray of shape (n, m), optional\n",
    "        Attention mask (1 for positions to attend, 0 for positions to mask)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : ndarray of shape (n, d_v)\n",
    "        Attention output\n",
    "    attention_weights : ndarray of shape (n, m)\n",
    "        Attention weight matrix\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores: QK^T / sqrt(d_k)\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided (set masked positions to -inf)\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Compute weighted sum of values\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "We extend the basic attention to multiple heads, allowing the model to capture different types of relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize Multi-Head Attention.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        d_model : int\n",
    "            Model dimension\n",
    "        num_heads : int\n",
    "            Number of attention heads\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_v = d_model // num_heads\n",
    "        \n",
    "        # Initialize projection matrices with Xavier initialization\n",
    "        scale = np.sqrt(2.0 / (d_model + self.d_k))\n",
    "        self.W_Q = np.random.randn(num_heads, d_model, self.d_k) * scale\n",
    "        self.W_K = np.random.randn(num_heads, d_model, self.d_k) * scale\n",
    "        self.W_V = np.random.randn(num_heads, d_model, self.d_v) * scale\n",
    "        self.W_O = np.random.randn(num_heads * self.d_v, d_model) * scale\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        Q : ndarray of shape (n, d_model)\n",
    "            Query matrix\n",
    "        K : ndarray of shape (m, d_model)\n",
    "            Key matrix\n",
    "        V : ndarray of shape (m, d_model)\n",
    "            Value matrix\n",
    "        mask : ndarray of shape (n, m), optional\n",
    "            Attention mask\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : ndarray of shape (n, d_model)\n",
    "            Multi-head attention output\n",
    "        attention_weights : list of ndarrays\n",
    "            Attention weights for each head\n",
    "        \"\"\"\n",
    "        heads = []\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            # Project Q, K, V for this head\n",
    "            Q_i = np.matmul(Q, self.W_Q[i])\n",
    "            K_i = np.matmul(K, self.W_K[i])\n",
    "            V_i = np.matmul(V, self.W_V[i])\n",
    "            \n",
    "            # Compute attention for this head\n",
    "            head_output, attention_weights = scaled_dot_product_attention(Q_i, K_i, V_i, mask)\n",
    "            heads.append(head_output)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        concat = np.concatenate(heads, axis=-1)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = np.matmul(concat, self.W_O)\n",
    "        \n",
    "        return output, attention_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration: Self-Attention on a Sequence\n",
    "\n",
    "Let's demonstrate the attention mechanism on a simple sequence, simulating how a model might attend to different parts of an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic sequence (e.g., 8 tokens with embedding dimension 64)\n",
    "seq_length = 8\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "# Generate synthetic embeddings\n",
    "# Simulate different token types with distinct patterns\n",
    "X = np.zeros((seq_length, d_model))\n",
    "for i in range(seq_length):\n",
    "    # Create embeddings with some structure\n",
    "    X[i] = np.sin(np.arange(d_model) * (i + 1) * 0.1) + np.random.randn(d_model) * 0.1\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Sequence length: {seq_length}\")\n",
    "print(f\"Model dimension: {d_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attention: Q, K, V all come from the same input\n",
    "Q = K = V = X\n",
    "\n",
    "# Compute single-head attention\n",
    "output_single, weights_single = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Single-head output shape: {output_single.shape}\")\n",
    "print(f\"Attention weights shape: {weights_single.shape}\")\n",
    "print(f\"\\nAttention weights (rows sum to 1):\")\n",
    "print(weights_single.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "output_multi, weights_multi = mha.forward(Q, K, V)\n",
    "\n",
    "print(f\"Multi-head output shape: {output_multi.shape}\")\n",
    "print(f\"Number of attention heads: {len(weights_multi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "### Attention Weight Heatmaps\n",
    "\n",
    "Visualizing attention weights helps understand what the model is \"looking at\" when processing each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Token labels for visualization\n",
    "tokens = [f'Token {i}' for i in range(seq_length)]\n",
    "\n",
    "# 1. Single-head attention heatmap\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "im1 = ax1.imshow(weights_single, cmap='Blues', aspect='auto')\n",
    "ax1.set_xlabel('Key Position')\n",
    "ax1.set_ylabel('Query Position')\n",
    "ax1.set_title('Single-Head Attention Weights')\n",
    "ax1.set_xticks(range(seq_length))\n",
    "ax1.set_yticks(range(seq_length))\n",
    "ax1.set_xticklabels(range(seq_length))\n",
    "ax1.set_yticklabels(range(seq_length))\n",
    "plt.colorbar(im1, ax=ax1, fraction=0.046)\n",
    "\n",
    "# 2-5. Multi-head attention heatmaps\n",
    "for i, weights in enumerate(weights_multi):\n",
    "    ax = fig.add_subplot(2, 3, i + 2)\n",
    "    im = ax.imshow(weights, cmap='Reds', aspect='auto')\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "    ax.set_title(f'Head {i + 1} Attention')\n",
    "    ax.set_xticks(range(seq_length))\n",
    "    ax.set_yticks(range(seq_length))\n",
    "    ax.set_xticklabels(range(seq_length))\n",
    "    ax.set_yticklabels(range(seq_length))\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "# 6. Attention entropy analysis\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "\n",
    "# Compute entropy of attention distributions for each head\n",
    "def attention_entropy(weights):\n",
    "    \"\"\"Compute entropy of attention distribution for each query position.\"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-10\n",
    "    entropy = -np.sum(weights * np.log(weights + eps), axis=-1)\n",
    "    return entropy\n",
    "\n",
    "positions = np.arange(seq_length)\n",
    "entropy_single = attention_entropy(weights_single)\n",
    "ax6.plot(positions, entropy_single, 'b-o', linewidth=2, markersize=8, label='Single-head')\n",
    "\n",
    "for i, weights in enumerate(weights_multi):\n",
    "    entropy = attention_entropy(weights)\n",
    "    ax6.plot(positions, entropy, '--', marker='s', markersize=6, \n",
    "             alpha=0.7, label=f'Head {i + 1}')\n",
    "\n",
    "ax6.set_xlabel('Query Position')\n",
    "ax6.set_ylabel('Entropy (nats)')\n",
    "ax6.set_title('Attention Distribution Entropy')\n",
    "ax6.legend(loc='best', fontsize=8)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "ax6.set_xticks(positions)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "### Interpretation of Attention Patterns\n",
    "\n",
    "1. **Single-head attention** shows the overall attention pattern when using a single attention mechanism.\n",
    "\n",
    "2. **Multi-head attention** reveals that different heads learn to attend to different aspects:\n",
    "   - Some heads may focus on local patterns (attending to nearby positions)\n",
    "   - Other heads may capture global dependencies (attending to distant positions)\n",
    "   - This diversity allows the model to capture multiple types of relationships simultaneously\n",
    "\n",
    "3. **Entropy analysis** measures how \"spread out\" the attention is:\n",
    "   - Low entropy: Focused attention on specific positions\n",
    "   - High entropy: Distributed attention across many positions\n",
    "   - Maximum entropy for 8 positions: $\\ln(8) \\approx 2.08$ nats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Attention statistics\n",
    "print(\"=\" * 50)\n",
    "print(\"ATTENTION MECHANISM ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Single-Head Attention Statistics:\")\n",
    "print(f\"   - Mean attention weight: {weights_single.mean():.4f}\")\n",
    "print(f\"   - Max attention weight: {weights_single.max():.4f}\")\n",
    "print(f\"   - Min attention weight: {weights_single.min():.4f}\")\n",
    "print(f\"   - Mean entropy: {entropy_single.mean():.4f} nats\")\n",
    "\n",
    "print(\"\\n2. Multi-Head Attention Statistics:\")\n",
    "for i, weights in enumerate(weights_multi):\n",
    "    entropy = attention_entropy(weights)\n",
    "    print(f\"   Head {i + 1}:\")\n",
    "    print(f\"     - Max weight: {weights.max():.4f}\")\n",
    "    print(f\"     - Mean entropy: {entropy.mean():.4f} nats\")\n",
    "\n",
    "print(\"\\n3. Theoretical Bounds:\")\n",
    "max_entropy = np.log(seq_length)\n",
    "print(f\"   - Maximum entropy (uniform attention): {max_entropy:.4f} nats\")\n",
    "print(f\"   - Minimum entropy (focused attention): 0 nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal (Masked) Attention\n",
    "\n",
    "In autoregressive models (like GPT), we use causal masking to prevent attending to future positions:\n",
    "\n",
    "$$\\text{mask}_{ij} = \\begin{cases} 1 & \\text{if } i \\geq j \\\\ 0 & \\text{if } i < j \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create causal mask\n",
    "causal_mask = np.tril(np.ones((seq_length, seq_length)))\n",
    "\n",
    "print(\"Causal Mask (lower triangular):\")\n",
    "print(causal_mask.astype(int))\n",
    "\n",
    "# Compute masked attention\n",
    "output_masked, weights_masked = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "print(\"\\nMasked Attention Weights:\")\n",
    "print(weights_masked.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The attention mechanism is a powerful tool that enables neural networks to:\n",
    "\n",
    "1. **Dynamically weight** different parts of the input based on relevance\n",
    "2. **Capture long-range dependencies** without the limitations of recurrent architectures\n",
    "3. **Provide interpretability** through attention weight visualization\n",
    "\n",
    "Key takeaways:\n",
    "- The scaling factor $\\frac{1}{\\sqrt{d_k}}$ is crucial for stable gradients\n",
    "- Multi-head attention allows capturing diverse relationship types\n",
    "- Causal masking enables autoregressive generation\n",
    "- Attention weights can be interpreted as a soft retrieval mechanism\n",
    "\n",
    "This mechanism forms the backbone of Transformer architectures and has revolutionized natural language processing, computer vision, and other domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
