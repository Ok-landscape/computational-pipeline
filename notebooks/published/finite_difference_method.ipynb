{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Difference Method for Solving Differential Equations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **Finite Difference Method (FDM)** is a numerical technique for approximating solutions to differential equations by replacing derivatives with finite difference approximations. This method transforms continuous differential equations into systems of algebraic equations that can be solved computationally.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Taylor Series Expansion\n",
    "\n",
    "The finite difference approximations are derived from Taylor series expansions. For a smooth function $f(x)$:\n",
    "\n",
    "$$f(x + h) = f(x) + h f'(x) + \\frac{h^2}{2!} f''(x) + \\frac{h^3}{3!} f'''(x) + \\mathcal{O}(h^4)$$\n",
    "\n",
    "$$f(x - h) = f(x) - h f'(x) + \\frac{h^2}{2!} f''(x) - \\frac{h^3}{3!} f'''(x) + \\mathcal{O}(h^4)$$\n",
    "\n",
    "### Finite Difference Approximations\n",
    "\n",
    "**Forward Difference (First-order accurate):**\n",
    "$$f'(x) \\approx \\frac{f(x+h) - f(x)}{h} + \\mathcal{O}(h)$$\n",
    "\n",
    "**Backward Difference (First-order accurate):**\n",
    "$$f'(x) \\approx \\frac{f(x) - f(x-h)}{h} + \\mathcal{O}(h)$$\n",
    "\n",
    "**Central Difference (Second-order accurate):**\n",
    "$$f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h} + \\mathcal{O}(h^2)$$\n",
    "\n",
    "**Second Derivative (Central Difference):**\n",
    "$$f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2} + \\mathcal{O}(h^2)$$\n",
    "\n",
    "## Application: Solving a Boundary Value Problem\n",
    "\n",
    "We will solve the second-order ODE:\n",
    "\n",
    "$$\\frac{d^2 y}{dx^2} + p(x) \\frac{dy}{dx} + q(x) y = r(x)$$\n",
    "\n",
    "with boundary conditions $y(a) = \\alpha$ and $y(b) = \\beta$.\n",
    "\n",
    "### Specific Problem\n",
    "\n",
    "Consider the boundary value problem:\n",
    "\n",
    "$$\\frac{d^2 y}{dx^2} - y = -x$$\n",
    "\n",
    "with $y(0) = 0$ and $y(1) = 0$.\n",
    "\n",
    "The analytical solution is:\n",
    "$$y(x) = x - \\frac{\\sinh(x)}{\\sinh(1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import solve\n",
    "\n",
    "# Define the problem parameters\n",
    "a, b = 0, 1  # Domain boundaries\n",
    "alpha, beta = 0, 0  # Boundary conditions\n",
    "\n",
    "# Number of interior grid points\n",
    "N = 50\n",
    "h = (b - a) / (N + 1)  # Step size\n",
    "\n",
    "# Create grid points (interior only)\n",
    "x = np.linspace(a + h, b - h, N)\n",
    "\n",
    "print(f\"Grid spacing h = {h:.6f}\")\n",
    "print(f\"Number of interior points: {N}\")\n",
    "print(f\"Expected truncation error: O(hÂ²) = O({h**2:.2e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization\n",
    "\n",
    "Using the central difference formula for the second derivative:\n",
    "\n",
    "$$\\frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} - y_i = -x_i$$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$y_{i-1} - (2 + h^2)y_i + y_{i+1} = -h^2 x_i$$\n",
    "\n",
    "This forms a tridiagonal system $\\mathbf{A}\\mathbf{y} = \\mathbf{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tridiagonal matrix A\n",
    "main_diag = -(2 + h**2) * np.ones(N)\n",
    "off_diag = np.ones(N - 1)\n",
    "\n",
    "A = np.diag(main_diag) + np.diag(off_diag, 1) + np.diag(off_diag, -1)\n",
    "\n",
    "# Construct the right-hand side vector b\n",
    "b_vec = -h**2 * x\n",
    "\n",
    "# Apply boundary conditions\n",
    "b_vec[0] -= alpha  # y_0 = alpha\n",
    "b_vec[-1] -= beta  # y_{N+1} = beta\n",
    "\n",
    "# Display the structure of the matrix (for small N)\n",
    "print(\"Matrix A (tridiagonal structure):\")\n",
    "print(f\"Main diagonal: {main_diag[0]:.4f}\")\n",
    "print(f\"Off-diagonals: {off_diag[0]:.4f}\")\n",
    "print(f\"\\nCondition number of A: {np.linalg.cond(A):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the linear system\n",
    "y_numerical = solve(A, b_vec)\n",
    "\n",
    "# Include boundary points for plotting\n",
    "x_full = np.concatenate([[a], x, [b]])\n",
    "y_full = np.concatenate([[alpha], y_numerical, [beta]])\n",
    "\n",
    "# Analytical solution\n",
    "def analytical_solution(x):\n",
    "    return x - np.sinh(x) / np.sinh(1)\n",
    "\n",
    "y_analytical = analytical_solution(x_full)\n",
    "\n",
    "# Calculate errors\n",
    "error = np.abs(y_full - y_analytical)\n",
    "max_error = np.max(error)\n",
    "rms_error = np.sqrt(np.mean(error**2))\n",
    "\n",
    "print(f\"Maximum absolute error: {max_error:.6e}\")\n",
    "print(f\"RMS error: {rms_error:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Analysis\n",
    "\n",
    "The finite difference method has a truncation error of $\\mathcal{O}(h^2)$. We verify this by solving the problem with different grid spacings and observing the error reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_fdm(N):\n",
    "    \"\"\"Solve the BVP using FDM with N interior points.\"\"\"\n",
    "    h = (b - a) / (N + 1)\n",
    "    x = np.linspace(a + h, b - h, N)\n",
    "    \n",
    "    # Build system\n",
    "    main_diag = -(2 + h**2) * np.ones(N)\n",
    "    off_diag = np.ones(N - 1)\n",
    "    A = np.diag(main_diag) + np.diag(off_diag, 1) + np.diag(off_diag, -1)\n",
    "    \n",
    "    b_vec = -h**2 * x\n",
    "    b_vec[0] -= alpha\n",
    "    b_vec[-1] -= beta\n",
    "    \n",
    "    y = solve(A, b_vec)\n",
    "    \n",
    "    # Full solution with boundaries\n",
    "    x_full = np.concatenate([[a], x, [b]])\n",
    "    y_full = np.concatenate([[alpha], y, [beta]])\n",
    "    \n",
    "    return x_full, y_full, h\n",
    "\n",
    "# Test convergence\n",
    "N_values = [10, 20, 40, 80, 160, 320]\n",
    "errors = []\n",
    "h_values = []\n",
    "\n",
    "for N in N_values:\n",
    "    x_sol, y_sol, h = solve_fdm(N)\n",
    "    y_exact = analytical_solution(x_sol)\n",
    "    max_err = np.max(np.abs(y_sol - y_exact))\n",
    "    errors.append(max_err)\n",
    "    h_values.append(h)\n",
    "\n",
    "# Calculate convergence rate\n",
    "errors = np.array(errors)\n",
    "h_values = np.array(h_values)\n",
    "\n",
    "# Linear regression in log-log space\n",
    "coeffs = np.polyfit(np.log(h_values), np.log(errors), 1)\n",
    "convergence_rate = coeffs[0]\n",
    "\n",
    "print(\"Convergence Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'N':>6} {'h':>12} {'Max Error':>14} {'Rate':>8}\")\n",
    "print(\"-\"*50)\n",
    "for i, (N, h, err) in enumerate(zip(N_values, h_values, errors)):\n",
    "    if i > 0:\n",
    "        rate = np.log(errors[i-1]/err) / np.log(h_values[i-1]/h)\n",
    "        print(f\"{N:>6} {h:>12.6f} {err:>14.6e} {rate:>8.2f}\")\n",
    "    else:\n",
    "        print(f\"{N:>6} {h:>12.6f} {err:>14.6e} {'---':>8}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Overall convergence rate: {convergence_rate:.2f}\")\n",
    "print(f\"Expected rate (second-order): 2.00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Solution comparison\n",
    "ax1 = axes[0, 0]\n",
    "x_fine = np.linspace(a, b, 1000)\n",
    "ax1.plot(x_fine, analytical_solution(x_fine), 'b-', linewidth=2, label='Analytical')\n",
    "ax1.plot(x_full, y_full, 'ro', markersize=4, label=f'FDM (N={N})')\n",
    "ax1.set_xlabel('x', fontsize=12)\n",
    "ax1.set_ylabel('y', fontsize=12)\n",
    "ax1.set_title(\"Solution of $y'' - y = -x$\", fontsize=14)\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.semilogy(x_full, error + 1e-16, 'g-', linewidth=1.5)\n",
    "ax2.set_xlabel('x', fontsize=12)\n",
    "ax2.set_ylabel('Absolute Error', fontsize=12)\n",
    "ax2.set_title('Error Distribution', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Convergence plot\n",
    "ax3 = axes[1, 0]\n",
    "ax3.loglog(h_values, errors, 'ko-', linewidth=2, markersize=8, label='FDM Error')\n",
    "# Reference line for O(h^2)\n",
    "h_ref = np.array([h_values[0], h_values[-1]])\n",
    "err_ref = errors[0] * (h_ref / h_values[0])**2\n",
    "ax3.loglog(h_ref, err_ref, 'r--', linewidth=1.5, label='$\\mathcal{O}(h^2)$ reference')\n",
    "ax3.set_xlabel('Step size h', fontsize=12)\n",
    "ax3.set_ylabel('Maximum Error', fontsize=12)\n",
    "ax3.set_title(f'Convergence Rate: {convergence_rate:.2f}', fontsize=14)\n",
    "ax3.legend(loc='best')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Matrix structure visualization\n",
    "ax4 = axes[1, 1]\n",
    "# Create a small matrix to visualize\n",
    "N_small = 10\n",
    "h_small = 1 / (N_small + 1)\n",
    "A_small = np.diag(-(2 + h_small**2) * np.ones(N_small))\n",
    "A_small += np.diag(np.ones(N_small - 1), 1)\n",
    "A_small += np.diag(np.ones(N_small - 1), -1)\n",
    "im = ax4.imshow(A_small, cmap='RdBu_r', aspect='equal')\n",
    "ax4.set_title('Tridiagonal Matrix Structure', fontsize=14)\n",
    "ax4.set_xlabel('Column index', fontsize=12)\n",
    "ax4.set_ylabel('Row index', fontsize=12)\n",
    "plt.colorbar(im, ax=ax4, fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated the Finite Difference Method for solving a second-order boundary value problem:\n",
    "\n",
    "1. **Discretization**: The continuous domain was discretized into grid points, and derivatives were approximated using central differences.\n",
    "\n",
    "2. **Linear System**: The discretized equation leads to a tridiagonal linear system $\\mathbf{A}\\mathbf{y} = \\mathbf{b}$, which is efficiently solvable.\n",
    "\n",
    "3. **Convergence**: The method exhibits second-order convergence ($\\mathcal{O}(h^2)$), meaning the error decreases by a factor of 4 when the grid spacing is halved.\n",
    "\n",
    "4. **Accuracy**: With $N=50$ interior points, we achieved errors on the order of $10^{-5}$.\n",
    "\n",
    "### Key Advantages of FDM\n",
    "\n",
    "- Simple to implement and understand\n",
    "- Efficient for regular geometries\n",
    "- Sparse matrix structure allows for fast solvers\n",
    "- Easy to extend to higher dimensions\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Less flexible for irregular domains\n",
    "- May require fine grids for high accuracy\n",
    "- Boundary conditions can be complex to implement for some geometries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
