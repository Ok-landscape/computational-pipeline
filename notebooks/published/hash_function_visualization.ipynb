{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash Function Visualization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A **hash function** is a mathematical function $h: \\mathcal{U} \\to \\{0, 1, \\ldots, m-1\\}$ that maps data of arbitrary size to fixed-size values. Hash functions are fundamental to computer science, with applications in data structures (hash tables), cryptography, data integrity verification, and distributed systems.\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "### Properties of Good Hash Functions\n",
    "\n",
    "An ideal hash function should satisfy:\n",
    "\n",
    "1. **Determinism**: For any input $x$, $h(x)$ always produces the same output\n",
    "2. **Uniformity**: The hash values should be uniformly distributed over the output range:\n",
    "   $$P(h(x) = k) \\approx \\frac{1}{m} \\quad \\forall k \\in \\{0, 1, \\ldots, m-1\\}$$\n",
    "3. **Avalanche Effect**: A small change in input should cause significant change in output:\n",
    "   $$\\text{If } x' = x \\oplus \\delta, \\text{ then } h(x') \\text{ differs significantly from } h(x)$$\n",
    "\n",
    "### Division Method\n",
    "\n",
    "The simplest hash function uses the division method:\n",
    "$$h(k) = k \\mod m$$\n",
    "\n",
    "where $k$ is the key converted to an integer and $m$ is the table size.\n",
    "\n",
    "### Multiplication Method\n",
    "\n",
    "The multiplication method, proposed by Knuth, uses:\n",
    "$$h(k) = \\lfloor m \\cdot (kA \\mod 1) \\rfloor$$\n",
    "\n",
    "where $A$ is a constant in $(0, 1)$. Knuth suggests $A \\approx \\frac{\\sqrt{5} - 1}{2} \\approx 0.6180339887$ (the golden ratio conjugate).\n",
    "\n",
    "### Polynomial Rolling Hash\n",
    "\n",
    "For strings, polynomial hashing computes:\n",
    "$$h(s) = \\left( \\sum_{i=0}^{n-1} s[i] \\cdot p^i \\right) \\mod m$$\n",
    "\n",
    "where $p$ is a prime base and $s[i]$ is the ASCII value of the $i$-th character.\n",
    "\n",
    "## Collision Analysis\n",
    "\n",
    "By the **Birthday Paradox**, if we hash $n$ items into $m$ buckets, the expected number of collisions is:\n",
    "$$E[\\text{collisions}] \\approx n - m + m\\left(1 - \\frac{1}{m}\\right)^n$$\n",
    "\n",
    "For large $m$, this approximates to:\n",
    "$$E[\\text{collisions}] \\approx \\frac{n^2}{2m}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import hashlib\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define hash table size\n",
    "TABLE_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Hash Functions\n",
    "\n",
    "We implement several hash functions to compare their distribution properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def division_hash(key, m=TABLE_SIZE):\n",
    "    \"\"\"Division method: h(k) = k mod m\"\"\"\n",
    "    if isinstance(key, str):\n",
    "        key = sum(ord(c) for c in key)\n",
    "    return key % m\n",
    "\n",
    "def multiplication_hash(key, m=TABLE_SIZE):\n",
    "    \"\"\"Multiplication method using golden ratio\"\"\"\n",
    "    A = (np.sqrt(5) - 1) / 2  # Golden ratio conjugate\n",
    "    if isinstance(key, str):\n",
    "        key = sum(ord(c) for c in key)\n",
    "    return int(m * ((key * A) % 1))\n",
    "\n",
    "def polynomial_hash(key, m=TABLE_SIZE, p=31):\n",
    "    \"\"\"Polynomial rolling hash for strings\"\"\"\n",
    "    if isinstance(key, int):\n",
    "        key = str(key)\n",
    "    hash_val = 0\n",
    "    p_pow = 1\n",
    "    for char in key:\n",
    "        hash_val = (hash_val + ord(char) * p_pow) % m\n",
    "        p_pow = (p_pow * p) % m\n",
    "    return hash_val\n",
    "\n",
    "def djb2_hash(key, m=TABLE_SIZE):\n",
    "    \"\"\"DJB2 hash algorithm by Dan Bernstein\"\"\"\n",
    "    if isinstance(key, int):\n",
    "        key = str(key)\n",
    "    hash_val = 5381\n",
    "    for char in key:\n",
    "        hash_val = ((hash_val << 5) + hash_val) + ord(char)\n",
    "    return hash_val % m\n",
    "\n",
    "def sha256_hash(key, m=TABLE_SIZE):\n",
    "    \"\"\"SHA-256 cryptographic hash (truncated)\"\"\"\n",
    "    if isinstance(key, int):\n",
    "        key = str(key)\n",
    "    hash_bytes = hashlib.sha256(key.encode()).digest()\n",
    "    hash_int = int.from_bytes(hash_bytes[:8], 'big')\n",
    "    return hash_int % m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data\n",
    "\n",
    "We create several datasets to test the hash functions:\n",
    "1. Sequential integers\n",
    "2. Random integers\n",
    "3. Random strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test datasets\n",
    "n_samples = 1000\n",
    "\n",
    "# Sequential integers\n",
    "sequential_keys = list(range(n_samples))\n",
    "\n",
    "# Random integers\n",
    "random_int_keys = np.random.randint(0, 100000, n_samples).tolist()\n",
    "\n",
    "# Random strings\n",
    "def generate_random_string(length=8):\n",
    "    chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "    return ''.join(np.random.choice(list(chars)) for _ in range(length))\n",
    "\n",
    "random_string_keys = [generate_random_string() for _ in range(n_samples)]\n",
    "\n",
    "print(f\"Generated {n_samples} keys for each dataset\")\n",
    "print(f\"Sequential: {sequential_keys[:5]}...\")\n",
    "print(f\"Random int: {random_int_keys[:5]}...\")\n",
    "print(f\"Random str: {random_string_keys[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Distribution Analysis\n",
    "\n",
    "We analyze how well each hash function distributes keys across the hash table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distribution(hash_func, keys, name):\n",
    "    \"\"\"Analyze the distribution of hash values\"\"\"\n",
    "    hash_values = [hash_func(k) for k in keys]\n",
    "    counts = Counter(hash_values)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    values = list(counts.values())\n",
    "    mean_count = np.mean(values)\n",
    "    std_count = np.std(values)\n",
    "    max_count = max(values)\n",
    "    min_count = min(values) if len(counts) == TABLE_SIZE else 0\n",
    "    \n",
    "    # Chi-squared statistic for uniformity\n",
    "    expected = len(keys) / TABLE_SIZE\n",
    "    chi_squared = sum((counts.get(i, 0) - expected)**2 / expected \n",
    "                      for i in range(TABLE_SIZE))\n",
    "    \n",
    "    return {\n",
    "        'hash_values': hash_values,\n",
    "        'counts': counts,\n",
    "        'mean': mean_count,\n",
    "        'std': std_count,\n",
    "        'chi_squared': chi_squared,\n",
    "        'buckets_used': len(counts)\n",
    "    }\n",
    "\n",
    "# Hash functions to compare\n",
    "hash_functions = {\n",
    "    'Division': division_hash,\n",
    "    'Multiplication': multiplication_hash,\n",
    "    'Polynomial': polynomial_hash,\n",
    "    'DJB2': djb2_hash,\n",
    "    'SHA-256': sha256_hash\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distributions for random string keys\n",
    "results = {}\n",
    "for name, func in hash_functions.items():\n",
    "    results[name] = analyze_distribution(func, random_string_keys, name)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean: {results[name]['mean']:.2f}, Std: {results[name]['std']:.2f}\")\n",
    "    print(f\"  Chi-squared: {results[name]['chi_squared']:.2f}\")\n",
    "    print(f\"  Buckets used: {results[name]['buckets_used']}/{TABLE_SIZE}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We create comprehensive visualizations to compare hash function performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Color palette\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(hash_functions)))\n",
    "\n",
    "# 1. Distribution histograms (top row)\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    ax = fig.add_subplot(3, len(hash_functions), idx + 1)\n",
    "    \n",
    "    # Create histogram of bucket occupancy\n",
    "    bucket_counts = [result['counts'].get(i, 0) for i in range(TABLE_SIZE)]\n",
    "    ax.bar(range(TABLE_SIZE), bucket_counts, color=colors[idx], alpha=0.7, width=1.0)\n",
    "    ax.axhline(y=n_samples/TABLE_SIZE, color='red', linestyle='--', \n",
    "               label=f'Expected={n_samples/TABLE_SIZE:.1f}')\n",
    "    ax.set_title(f'{name}\\n$\\\\chi^2={result[\"chi_squared\"]:.1f}$', fontsize=10)\n",
    "    ax.set_xlabel('Bucket Index')\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('Count')\n",
    "    ax.set_xlim(0, TABLE_SIZE)\n",
    "\n",
    "# 2. Avalanche effect visualization (middle row)\n",
    "ax_avalanche = fig.add_subplot(3, 2, 3)\n",
    "\n",
    "# Test avalanche effect: small input changes should cause large output changes\n",
    "base_strings = [f'test{i}' for i in range(100)]\n",
    "modified_strings = [f'test{i}!' for i in range(100)]  # Add single character\n",
    "\n",
    "avalanche_data = {}\n",
    "for name, func in hash_functions.items():\n",
    "    base_hashes = [func(s) for s in base_strings]\n",
    "    mod_hashes = [func(s) for s in modified_strings]\n",
    "    changes = [abs(b - m) for b, m in zip(base_hashes, mod_hashes)]\n",
    "    avalanche_data[name] = changes\n",
    "\n",
    "bp = ax_avalanche.boxplot([avalanche_data[name] for name in hash_functions.keys()],\n",
    "                          labels=hash_functions.keys(), patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "ax_avalanche.set_ylabel('Hash Value Change')\n",
    "ax_avalanche.set_title('Avalanche Effect\\n(Change in hash for single character addition)')\n",
    "ax_avalanche.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Collision analysis (middle row)\n",
    "ax_collision = fig.add_subplot(3, 2, 4)\n",
    "\n",
    "# Calculate collisions for different load factors\n",
    "load_factors = np.linspace(0.1, 2.0, 20)\n",
    "collision_data = {name: [] for name in hash_functions.keys()}\n",
    "\n",
    "for lf in load_factors:\n",
    "    n_keys = int(TABLE_SIZE * lf)\n",
    "    test_keys = [generate_random_string() for _ in range(n_keys)]\n",
    "    \n",
    "    for name, func in hash_functions.items():\n",
    "        hashes = [func(k) for k in test_keys]\n",
    "        unique_hashes = len(set(hashes))\n",
    "        collisions = n_keys - unique_hashes\n",
    "        collision_rate = collisions / n_keys if n_keys > 0 else 0\n",
    "        collision_data[name].append(collision_rate)\n",
    "\n",
    "for idx, (name, rates) in enumerate(collision_data.items()):\n",
    "    ax_collision.plot(load_factors, rates, color=colors[idx], \n",
    "                      label=name, linewidth=2, marker='o', markersize=3)\n",
    "\n",
    "ax_collision.set_xlabel('Load Factor (n/m)')\n",
    "ax_collision.set_ylabel('Collision Rate')\n",
    "ax_collision.set_title('Collision Rate vs Load Factor')\n",
    "ax_collision.legend(loc='upper left', fontsize=8)\n",
    "ax_collision.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Heat map of hash distribution (bottom left)\n",
    "ax_heat = fig.add_subplot(3, 2, 5)\n",
    "\n",
    "# Create 2D visualization: hash of numbers 0-999\n",
    "heat_data = np.zeros((10, TABLE_SIZE))\n",
    "test_nums = list(range(1000))\n",
    "sha_hashes = [sha256_hash(n) for n in test_nums]\n",
    "\n",
    "for i, h in enumerate(sha_hashes):\n",
    "    row = i // 100\n",
    "    heat_data[row, h] += 1\n",
    "\n",
    "im = ax_heat.imshow(heat_data, aspect='auto', cmap='YlOrRd')\n",
    "ax_heat.set_xlabel('Hash Bucket')\n",
    "ax_heat.set_ylabel('Input Range (×100)')\n",
    "ax_heat.set_title('SHA-256 Distribution Heatmap\\n(Integers 0-999)')\n",
    "plt.colorbar(im, ax=ax_heat, label='Count')\n",
    "\n",
    "# 5. Chi-squared comparison (bottom right)\n",
    "ax_chi = fig.add_subplot(3, 2, 6)\n",
    "\n",
    "chi_values = [results[name]['chi_squared'] for name in hash_functions.keys()]\n",
    "bars = ax_chi.bar(hash_functions.keys(), chi_values, color=colors)\n",
    "\n",
    "# Add critical value line (df = TABLE_SIZE - 1, α = 0.05)\n",
    "# For df=99, critical value ≈ 123.2\n",
    "critical_value = 123.2\n",
    "ax_chi.axhline(y=critical_value, color='red', linestyle='--', \n",
    "               label=f'Critical value (α=0.05)')\n",
    "ax_chi.set_ylabel('Chi-squared Statistic')\n",
    "ax_chi.set_title('Uniformity Test (Chi-squared)\\nLower is better')\n",
    "ax_chi.tick_params(axis='x', rotation=45)\n",
    "ax_chi.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Distribution Quality**: The SHA-256 and DJB2 hash functions typically show the most uniform distribution, with lower $\\chi^2$ statistics indicating better uniformity.\n",
    "\n",
    "2. **Avalanche Effect**: Cryptographic hash functions (SHA-256) demonstrate stronger avalanche properties, where small input changes produce large, unpredictable output changes.\n",
    "\n",
    "3. **Collision Behavior**: All hash functions follow similar collision curves, but the rate of increase with load factor varies:\n",
    "   - For load factor $\\alpha = n/m < 1$: Collisions grow approximately as $\\frac{n^2}{2m}$\n",
    "   - For $\\alpha > 1$: Collision rate approaches $(1 - 1/m)$\n",
    "\n",
    "4. **Trade-offs**:\n",
    "   - **Division method**: Fast but poor distribution for certain patterns\n",
    "   - **Multiplication method**: Good balance of speed and distribution\n",
    "   - **Cryptographic hashes**: Best distribution but computationally expensive\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- For hash tables: Use multiplication method or DJB2 for good performance\n",
    "- For security applications: Use cryptographic hashes (SHA-256)\n",
    "- For string hashing: Polynomial or DJB2 provide excellent results\n",
    "- Always choose table size $m$ to be prime for better distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
