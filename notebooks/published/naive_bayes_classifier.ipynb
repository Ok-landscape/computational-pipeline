{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "The Naive Bayes classifier is a probabilistic machine learning algorithm based on **Bayes' Theorem** with a strong (naive) independence assumption between features. Despite its simplicity, it performs remarkably well in many real-world applications, particularly text classification and spam filtering.\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "The foundation of Naive Bayes is Bayes' theorem, which describes the probability of an event based on prior knowledge of conditions related to the event:\n",
    "\n",
    "$$P(C_k | \\mathbf{x}) = \\frac{P(\\mathbf{x} | C_k) P(C_k)}{P(\\mathbf{x})}$$\n",
    "\n",
    "Where:\n",
    "- $P(C_k | \\mathbf{x})$ is the **posterior probability** of class $C_k$ given feature vector $\\mathbf{x}$\n",
    "- $P(\\mathbf{x} | C_k)$ is the **likelihood** of observing $\\mathbf{x}$ given class $C_k$\n",
    "- $P(C_k)$ is the **prior probability** of class $C_k$\n",
    "- $P(\\mathbf{x})$ is the **evidence** (normalizing constant)\n",
    "\n",
    "### The Naive Independence Assumption\n",
    "\n",
    "The \"naive\" aspect assumes that all features are conditionally independent given the class label. For a feature vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$:\n",
    "\n",
    "$$P(\\mathbf{x} | C_k) = \\prod_{i=1}^{n} P(x_i | C_k)$$\n",
    "\n",
    "This simplification reduces the computational complexity from exponential to linear in the number of features.\n",
    "\n",
    "### Classification Rule\n",
    "\n",
    "The classifier assigns the class with the highest posterior probability:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{C_k} P(C_k) \\prod_{i=1}^{n} P(x_i | C_k)$$\n",
    "\n",
    "Since $P(\\mathbf{x})$ is constant for all classes, we can ignore it for classification purposes.\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "For continuous features, we assume each feature follows a Gaussian (normal) distribution:\n",
    "\n",
    "$$P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{k,i})^2}{2\\sigma_{k,i}^2}\\right)$$\n",
    "\n",
    "Where $\\mu_{k,i}$ and $\\sigma_{k,i}^2$ are the mean and variance of feature $i$ for class $k$, estimated from training data.\n",
    "\n",
    "### Log-Likelihood Formulation\n",
    "\n",
    "To avoid numerical underflow from multiplying many small probabilities, we use logarithms:\n",
    "\n",
    "$$\\log P(C_k | \\mathbf{x}) \\propto \\log P(C_k) + \\sum_{i=1}^{n} \\log P(x_i | C_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement a Gaussian Naive Bayes classifier from scratch and demonstrate it on a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes Classifier implementation.\n",
    "    \n",
    "    Assumes features are continuous and follow Gaussian distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "        self.means = None\n",
    "        self.variances = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Naive Bayes model to training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training feature matrix\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target class labels\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.variances = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # Calculate statistics for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.priors[idx] = X_c.shape[0] / n_samples\n",
    "            self.means[idx, :] = X_c.mean(axis=0)\n",
    "            self.variances[idx, :] = X_c.var(axis=0) + 1e-9  # Add small value for numerical stability\n",
    "    \n",
    "    def _calculate_likelihood(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Calculate Gaussian likelihood P(x|C).\n",
    "        \n",
    "        Uses log-likelihood for numerical stability.\n",
    "        \"\"\"\n",
    "        return -0.5 * np.sum(np.log(2 * np.pi * var) + ((x - mean) ** 2) / var)\n",
    "    \n",
    "    def _calculate_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Calculate posterior probability for each class.\n",
    "        \n",
    "        Returns log-posteriors (unnormalized).\n",
    "        \"\"\"\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            prior = np.log(self.priors[idx])\n",
    "            likelihood = self._calculate_likelihood(x, self.means[idx], self.variances[idx])\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        return posteriors\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Feature matrix\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array, shape (n_samples,)\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            posteriors = self._calculate_posterior(x)\n",
    "            y_pred.append(self.classes[np.argmax(posteriors)])\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for samples in X.\n",
    "        \n",
    "        Returns normalized probabilities using softmax.\n",
    "        \"\"\"\n",
    "        probas = []\n",
    "        for x in X:\n",
    "            posteriors = np.array(self._calculate_posterior(x))\n",
    "            # Apply softmax for normalization\n",
    "            exp_posteriors = np.exp(posteriors - np.max(posteriors))\n",
    "            probas.append(exp_posteriors / exp_posteriors.sum())\n",
    "        return np.array(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset Generation\n",
    "\n",
    "We create a two-class classification problem with two features. Each class is generated from a distinct multivariate Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples_per_class=200):\n",
    "    \"\"\"\n",
    "    Generate synthetic 2D Gaussian data for binary classification.\n",
    "    \n",
    "    Class 0: centered at (2, 2)\n",
    "    Class 1: centered at (6, 6)\n",
    "    \"\"\"\n",
    "    # Class 0 parameters\n",
    "    mean_0 = [2, 2]\n",
    "    cov_0 = [[1.5, 0.5], [0.5, 1.5]]\n",
    "    \n",
    "    # Class 1 parameters\n",
    "    mean_1 = [6, 6]\n",
    "    cov_1 = [[1.5, -0.5], [-0.5, 1.5]]\n",
    "    \n",
    "    # Generate samples\n",
    "    X_0 = np.random.multivariate_normal(mean_0, cov_0, n_samples_per_class)\n",
    "    X_1 = np.random.multivariate_normal(mean_1, cov_1, n_samples_per_class)\n",
    "    \n",
    "    # Combine data\n",
    "    X = np.vstack([X_0, X_1])\n",
    "    y = np.hstack([np.zeros(n_samples_per_class), np.ones(n_samples_per_class)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_synthetic_data(200)\n",
    "\n",
    "# Split into training and test sets\n",
    "n_train = 300\n",
    "indices = np.random.permutation(len(X))\n",
    "X_train, X_test = X[indices[:n_train]], X[indices[n_train:]]\n",
    "y_train, y_test = y[indices[:n_train]], y[indices[n_train:]]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display learned parameters\n",
    "print(\"\\nLearned Parameters:\")\n",
    "print(f\"Class priors: {gnb.priors}\")\n",
    "print(f\"\\nClass 0 - Mean: {gnb.means[0]}, Variance: {gnb.variances[0]}\")\n",
    "print(f\"Class 1 - Mean: {gnb.means[1]}, Variance: {gnb.variances[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We visualize the decision boundary and class distributions to understand how the Naive Bayes classifier partitions the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Naive Bayes Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary and data points.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict on mesh grid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax1 = axes[0]\n",
    "    ax1.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    ax1.contour(xx, yy, Z, colors='k', linewidths=0.5)\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', \n",
    "                          edgecolors='k', alpha=0.7, s=50)\n",
    "    ax1.set_xlabel('Feature $x_1$', fontsize=12)\n",
    "    ax1.set_ylabel('Feature $x_2$', fontsize=12)\n",
    "    ax1.set_title(title, fontsize=14)\n",
    "    ax1.legend(*scatter.legend_elements(), title=\"Class\")\n",
    "    \n",
    "    # Plot probability contours\n",
    "    ax2 = axes[1]\n",
    "    proba = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    proba = proba.reshape(xx.shape)\n",
    "    \n",
    "    contour = ax2.contourf(xx, yy, proba, levels=20, cmap='RdBu_r', alpha=0.8)\n",
    "    ax2.contour(xx, yy, proba, levels=[0.5], colors='k', linewidths=2)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', \n",
    "                edgecolors='k', alpha=0.7, s=50)\n",
    "    ax2.set_xlabel('Feature $x_1$', fontsize=12)\n",
    "    ax2.set_ylabel('Feature $x_2$', fontsize=12)\n",
    "    ax2.set_title('Posterior Probability $P(C_1|\\mathbf{x})$', fontsize=14)\n",
    "    plt.colorbar(contour, ax=ax2, label='Probability')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create visualization\n",
    "fig = plot_decision_boundary(gnb, X, y)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate classification metrics manually.\n",
    "    \"\"\"\n",
    "    # Confusion matrix elements\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': np.array([[tn, fp], [fn, tp]]),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "# Calculate and display metrics\n",
    "metrics = calculate_metrics(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics['confusion_matrix'])\n",
    "print(f\"\\nAccuracy:  {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Naive Independence Assumption\n",
    "\n",
    "The Naive Bayes classifier assumes features are conditionally independent given the class. Let's examine when this assumption holds and its implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature correlation for each class\n",
    "print(\"Feature Correlation Analysis:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for class_label in [0, 1]:\n",
    "    X_class = X_train[y_train == class_label]\n",
    "    correlation = np.corrcoef(X_class[:, 0], X_class[:, 1])[0, 1]\n",
    "    print(f\"Class {int(class_label)}: Correlation between features = {correlation:.4f}\")\n",
    "\n",
    "print(\"\\nNote: The naive independence assumption is violated when features are correlated.\")\n",
    "print(\"However, Naive Bayes often performs well despite this violation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Properties of Naive Bayes:\n",
    "\n",
    "1. **Computational Efficiency**: Training is $O(nd)$ where $n$ is samples and $d$ is features\n",
    "\n",
    "2. **Interpretable**: Learned parameters (means, variances, priors) have clear probabilistic meaning\n",
    "\n",
    "3. **Works with Small Data**: Requires less training data than more complex models\n",
    "\n",
    "4. **Robust to Irrelevant Features**: Irrelevant features contribute equally across classes\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "1. **Independence Assumption**: Features are rarely truly independent in practice\n",
    "\n",
    "2. **Probability Estimates**: May be poorly calibrated due to the naive assumption\n",
    "\n",
    "3. **Zero Probability Problem**: Requires smoothing (Laplace) for discrete features\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- Spam filtering\n",
    "- Document classification\n",
    "- Sentiment analysis\n",
    "- Medical diagnosis\n",
    "- Real-time prediction systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
