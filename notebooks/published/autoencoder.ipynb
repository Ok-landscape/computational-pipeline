{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Neural Network for Unsupervised Representation Learning\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "An **autoencoder** is an unsupervised neural network architecture designed to learn efficient representations (encodings) of input data. The network learns to compress data into a lower-dimensional latent space and then reconstruct it, effectively learning the most salient features of the data distribution.\n",
    "\n",
    "## 2. Mathematical Foundation\n",
    "\n",
    "### 2.1 Architecture Overview\n",
    "\n",
    "An autoencoder consists of two main components:\n",
    "\n",
    "1. **Encoder** $f_\\phi: \\mathcal{X} \\rightarrow \\mathcal{Z}$ that maps input $\\mathbf{x}$ to latent representation $\\mathbf{z}$\n",
    "2. **Decoder** $g_\\theta: \\mathcal{Z} \\rightarrow \\mathcal{X}$ that reconstructs $\\hat{\\mathbf{x}}$ from $\\mathbf{z}$\n",
    "\n",
    "### 2.2 Encoder Function\n",
    "\n",
    "For a single hidden layer encoder:\n",
    "\n",
    "$$\\mathbf{z} = f_\\phi(\\mathbf{x}) = \\sigma(\\mathbf{W}_e \\mathbf{x} + \\mathbf{b}_e)$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{W}_e \\in \\mathbb{R}^{d_z \\times d_x}$ is the encoder weight matrix\n",
    "- $\\mathbf{b}_e \\in \\mathbb{R}^{d_z}$ is the encoder bias vector\n",
    "- $\\sigma(\\cdot)$ is a nonlinear activation function\n",
    "- $d_x$ is the input dimension, $d_z$ is the latent dimension\n",
    "\n",
    "### 2.3 Decoder Function\n",
    "\n",
    "The decoder reconstructs the input:\n",
    "\n",
    "$$\\hat{\\mathbf{x}} = g_\\theta(\\mathbf{z}) = \\sigma(\\mathbf{W}_d \\mathbf{z} + \\mathbf{b}_d)$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{W}_d \\in \\mathbb{R}^{d_x \\times d_z}$ is the decoder weight matrix\n",
    "- $\\mathbf{b}_d \\in \\mathbb{R}^{d_x}$ is the decoder bias vector\n",
    "\n",
    "### 2.4 Loss Function\n",
    "\n",
    "The autoencoder is trained to minimize the reconstruction error. For Mean Squared Error (MSE):\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{x}, \\hat{\\mathbf{x}}) = \\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2 = \\sum_{i=1}^{d_x} (x_i - \\hat{x}_i)^2$$\n",
    "\n",
    "The objective is to find optimal parameters:\n",
    "\n",
    "$$\\phi^*, \\theta^* = \\arg\\min_{\\phi, \\theta} \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}(\\mathbf{x}^{(n)}, g_\\theta(f_\\phi(\\mathbf{x}^{(n)})))$$\n",
    "\n",
    "### 2.5 Backpropagation Gradients\n",
    "\n",
    "For gradient descent optimization, we compute:\n",
    "\n",
    "**Output layer gradient:**\n",
    "$$\\delta_{out} = (\\hat{\\mathbf{x}} - \\mathbf{x}) \\odot \\sigma'(\\mathbf{W}_d \\mathbf{z} + \\mathbf{b}_d)$$\n",
    "\n",
    "**Hidden layer gradient:**\n",
    "$$\\delta_{hidden} = (\\mathbf{W}_d^T \\delta_{out}) \\odot \\sigma'(\\mathbf{W}_e \\mathbf{x} + \\mathbf{b}_e)$$\n",
    "\n",
    "**Weight updates:**\n",
    "$$\\mathbf{W}_d \\leftarrow \\mathbf{W}_d - \\eta \\cdot \\delta_{out} \\mathbf{z}^T$$\n",
    "$$\\mathbf{W}_e \\leftarrow \\mathbf{W}_e - \\eta \\cdot \\delta_{hidden} \\mathbf{x}^T$$\n",
    "\n",
    "where $\\eta$ is the learning rate and $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "### 2.6 Activation Functions\n",
    "\n",
    "**Sigmoid:**\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}, \\quad \\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "**ReLU:**\n",
    "$$\\text{ReLU}(x) = \\max(0, x), \\quad \\text{ReLU}'(x) = \\begin{cases} 1 & x > 0 \\\\ 0 & x \\leq 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "We will implement a simple autoencoder from scratch using NumPy to compress and reconstruct 2D data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    \"\"\"\n",
    "    Simple Autoencoder with one hidden layer.\n",
    "    Architecture: input -> encoder -> latent -> decoder -> output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize autoencoder with random weights.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input data\n",
    "        latent_dim : int\n",
    "            Dimension of latent representation\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Xavier initialization for weights\n",
    "        scale_e = np.sqrt(2.0 / (input_dim + latent_dim))\n",
    "        scale_d = np.sqrt(2.0 / (latent_dim + input_dim))\n",
    "        \n",
    "        # Encoder weights and biases\n",
    "        self.W_e = np.random.randn(latent_dim, input_dim) * scale_e\n",
    "        self.b_e = np.zeros((latent_dim, 1))\n",
    "        \n",
    "        # Decoder weights and biases\n",
    "        self.W_d = np.random.randn(input_dim, latent_dim) * scale_d\n",
    "        self.b_d = np.zeros((input_dim, 1))\n",
    "        \n",
    "        # Store training history\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to latent representation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : ndarray\n",
    "            Input data of shape (input_dim, batch_size)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        z : ndarray\n",
    "            Latent representation of shape (latent_dim, batch_size)\n",
    "        \"\"\"\n",
    "        self.z_pre = self.W_e @ x + self.b_e\n",
    "        self.z = self.sigmoid(self.z_pre)\n",
    "        return self.z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent representation to reconstruction.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        z : ndarray\n",
    "            Latent representation of shape (latent_dim, batch_size)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        x_hat : ndarray\n",
    "            Reconstructed data of shape (input_dim, batch_size)\n",
    "        \"\"\"\n",
    "        self.x_hat_pre = self.W_d @ z + self.b_d\n",
    "        self.x_hat = self.sigmoid(self.x_hat_pre)\n",
    "        return self.x_hat\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the autoencoder.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : ndarray\n",
    "            Input data of shape (input_dim, batch_size)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        x_hat : ndarray\n",
    "            Reconstructed data\n",
    "        \"\"\"\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat\n",
    "    \n",
    "    def compute_loss(self, x, x_hat):\n",
    "        \"\"\"\n",
    "        Compute Mean Squared Error loss.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : ndarray\n",
    "            Original input\n",
    "        x_hat : ndarray\n",
    "            Reconstructed output\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            MSE loss value\n",
    "        \"\"\"\n",
    "        return np.mean((x - x_hat) ** 2)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        \"\"\"\n",
    "        Backward pass to compute gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : ndarray\n",
    "            Original input data\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[1]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        delta_out = (self.x_hat - x) * self.sigmoid_derivative(self.x_hat_pre)\n",
    "        \n",
    "        # Decoder gradients\n",
    "        dW_d = (delta_out @ self.z.T) / batch_size\n",
    "        db_d = np.mean(delta_out, axis=1, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradient\n",
    "        delta_hidden = (self.W_d.T @ delta_out) * self.sigmoid_derivative(self.z_pre)\n",
    "        \n",
    "        # Encoder gradients\n",
    "        dW_e = (delta_hidden @ x.T) / batch_size\n",
    "        db_e = np.mean(delta_hidden, axis=1, keepdims=True)\n",
    "        \n",
    "        # Update weights using gradient descent\n",
    "        self.W_d -= self.lr * dW_d\n",
    "        self.b_d -= self.lr * db_d\n",
    "        self.W_e -= self.lr * dW_e\n",
    "        self.b_e -= self.lr * db_e\n",
    "    \n",
    "    def train(self, X, epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the autoencoder.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray\n",
    "            Training data of shape (input_dim, n_samples)\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        verbose : bool\n",
    "            Whether to print training progress\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            x_hat = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(X, x_hat)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 200 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Synthetic Data\n",
    "\n",
    "We create a 2D dataset with a specific structure that the autoencoder should learn to compress and reconstruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=500):\n",
    "    \"\"\"\n",
    "    Generate synthetic 2D data for autoencoder training.\n",
    "    Creates data along a curved manifold.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray\n",
    "        Data of shape (2, n_samples), normalized to [0, 1]\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, 2 * np.pi, n_samples)\n",
    "    \n",
    "    # Create spiral pattern\n",
    "    x1 = t * np.cos(t) + np.random.randn(n_samples) * 0.3\n",
    "    x2 = t * np.sin(t) + np.random.randn(n_samples) * 0.3\n",
    "    \n",
    "    X = np.vstack([x1, x2])\n",
    "    \n",
    "    # Normalize to [0, 1] for sigmoid output\n",
    "    X = (X - X.min(axis=1, keepdims=True)) / (X.max(axis=1, keepdims=True) - X.min(axis=1, keepdims=True))\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Generate training data\n",
    "X_train = generate_data(500)\n",
    "print(f\"Data shape: {X_train.shape}\")\n",
    "print(f\"Data range: [{X_train.min():.3f}, {X_train.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Autoencoder\n",
    "\n",
    "We train an autoencoder that compresses 2D data into a 1D latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize autoencoder: 2D input -> 1D latent -> 2D output\n",
    "ae = Autoencoder(input_dim=2, latent_dim=1, learning_rate=1.0)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Autoencoder...\")\n",
    "print(\"=\"*40)\n",
    "ae.train(X_train, epochs=2000, verbose=True)\n",
    "print(\"=\"*40)\n",
    "print(f\"Final Loss: {ae.loss_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reconstructions\n",
    "X_reconstructed = ae.forward(X_train)\n",
    "latent_codes = ae.z\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(ae.loss_history, 'b-', linewidth=1.5)\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('MSE Loss', fontsize=11)\n",
    "ax1.set_title('Training Loss Curve', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: Original vs Reconstructed Data\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.scatter(X_train[0, :], X_train[1, :], c='blue', alpha=0.5, s=20, label='Original')\n",
    "ax2.scatter(X_reconstructed[0, :], X_reconstructed[1, :], c='red', alpha=0.5, s=20, label='Reconstructed')\n",
    "ax2.set_xlabel('$x_1$', fontsize=11)\n",
    "ax2.set_ylabel('$x_2$', fontsize=11)\n",
    "ax2.set_title('Original vs Reconstructed', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Reconstruction Error\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "errors = np.sqrt(np.sum((X_train - X_reconstructed)**2, axis=0))\n",
    "scatter = ax3.scatter(X_train[0, :], X_train[1, :], c=errors, cmap='viridis', s=20)\n",
    "plt.colorbar(scatter, ax=ax3, label='Reconstruction Error')\n",
    "ax3.set_xlabel('$x_1$', fontsize=11)\n",
    "ax3.set_ylabel('$x_2$', fontsize=11)\n",
    "ax3.set_title('Spatial Reconstruction Error', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Latent Space Distribution\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.hist(latent_codes.flatten(), bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "ax4.set_xlabel('Latent Value $z$', fontsize=11)\n",
    "ax4.set_ylabel('Frequency', fontsize=11)\n",
    "ax4.set_title('Latent Space Distribution', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Data colored by latent code\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "scatter2 = ax5.scatter(X_train[0, :], X_train[1, :], c=latent_codes.flatten(), cmap='coolwarm', s=20)\n",
    "plt.colorbar(scatter2, ax=ax5, label='Latent Code $z$')\n",
    "ax5.set_xlabel('$x_1$', fontsize=11)\n",
    "ax5.set_ylabel('$x_2$', fontsize=11)\n",
    "ax5.set_title('Data Colored by Latent Code', fontsize=12, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Decoder manifold visualization\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "z_range = np.linspace(0.01, 0.99, 100).reshape(1, -1)\n",
    "decoded_manifold = ae.decode(z_range)\n",
    "ax6.plot(decoded_manifold[0, :], decoded_manifold[1, :], 'g-', linewidth=2, label='Learned Manifold')\n",
    "ax6.scatter(X_train[0, :], X_train[1, :], c='blue', alpha=0.3, s=10, label='Training Data')\n",
    "ax6.set_xlabel('$x_1$', fontsize=11)\n",
    "ax6.set_ylabel('$x_2$', fontsize=11)\n",
    "ax6.set_title('Learned Decoder Manifold', fontsize=12, fontweight='bold')\n",
    "ax6.legend(loc='upper right')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Autoencoder Analysis: 2D → 1D → 2D Compression', fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "mse = np.mean((X_train - X_reconstructed)**2)\n",
    "mae = np.mean(np.abs(X_train - X_reconstructed))\n",
    "max_error = np.max(np.sqrt(np.sum((X_train - X_reconstructed)**2, axis=0)))\n",
    "\n",
    "# Compression ratio\n",
    "compression_ratio = ae.input_dim / ae.latent_dim\n",
    "\n",
    "print(\"Autoencoder Performance Metrics\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Input Dimension:      {ae.input_dim}\")\n",
    "print(f\"Latent Dimension:     {ae.latent_dim}\")\n",
    "print(f\"Compression Ratio:    {compression_ratio:.1f}x\")\n",
    "print(f\"\")\n",
    "print(f\"Mean Squared Error:   {mse:.6f}\")\n",
    "print(f\"Mean Absolute Error:  {mae:.6f}\")\n",
    "print(f\"Max Point Error:      {max_error:.6f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "This notebook demonstrated the implementation and analysis of a simple autoencoder:\n",
    "\n",
    "1. **Architecture**: We built a 2D → 1D → 2D autoencoder that compresses data by 50%\n",
    "\n",
    "2. **Training**: The model was trained using gradient descent with backpropagation, minimizing MSE loss\n",
    "\n",
    "3. **Results**: \n",
    "   - The autoencoder successfully learned to compress and reconstruct the spiral data\n",
    "   - The latent space captures the main variation in the data (position along the spiral)\n",
    "   - The decoder learns a continuous manifold through the data\n",
    "\n",
    "4. **Key Insights**:\n",
    "   - Even a simple 1D bottleneck can capture meaningful structure in 2D data\n",
    "   - The latent codes naturally order the data along its principal variation\n",
    "   - Reconstruction error varies spatially, indicating areas of better/worse compression\n",
    "\n",
    "### Extensions\n",
    "\n",
    "This basic autoencoder can be extended to:\n",
    "- **Variational Autoencoders (VAEs)**: Add probabilistic latent space\n",
    "- **Denoising Autoencoders**: Add noise to inputs for robust features\n",
    "- **Convolutional Autoencoders**: Use convolutions for image data\n",
    "- **Deep Autoencoders**: Add multiple encoder/decoder layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
