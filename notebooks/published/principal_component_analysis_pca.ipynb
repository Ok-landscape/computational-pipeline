{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Principal Component Analysis (PCA) is a fundamental dimensionality reduction technique in statistics and machine learning. It transforms a dataset of possibly correlated variables into a set of linearly uncorrelated variables called **principal components**.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Covariance Matrix\n",
    "\n",
    "Given a centered data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ where $n$ is the number of observations and $p$ is the number of features, the covariance matrix is:\n",
    "\n",
    "$$\\mathbf{C} = \\frac{1}{n-1} \\mathbf{X}^T \\mathbf{X}$$\n",
    "\n",
    "### Eigendecomposition\n",
    "\n",
    "PCA finds the eigenvectors and eigenvalues of the covariance matrix:\n",
    "\n",
    "$$\\mathbf{C} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$$\n",
    "\n",
    "where $\\mathbf{v}_i$ is the $i$-th eigenvector (principal component direction) and $\\lambda_i$ is the corresponding eigenvalue (variance explained).\n",
    "\n",
    "### Projection\n",
    "\n",
    "The data is projected onto the principal components:\n",
    "\n",
    "$$\\mathbf{Z} = \\mathbf{X} \\mathbf{V}_k$$\n",
    "\n",
    "where $\\mathbf{V}_k \\in \\mathbb{R}^{p \\times k}$ contains the top $k$ eigenvectors as columns.\n",
    "\n",
    "### Variance Explained\n",
    "\n",
    "The proportion of variance explained by the $i$-th component is:\n",
    "\n",
    "$$\\text{Explained Variance Ratio}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{p} \\lambda_j}$$\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Generate synthetic multivariate data with known correlation structure\n",
    "2. Implement PCA from scratch using eigendecomposition\n",
    "3. Visualize the principal components and explained variance\n",
    "4. Demonstrate dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We generate synthetic 4-dimensional data with a specific covariance structure. The data will have most of its variance concentrated in fewer dimensions, making it ideal for demonstrating PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of samples and features\n",
    "n_samples = 300\n",
    "n_features = 4\n",
    "\n",
    "# Create a covariance matrix with specific structure\n",
    "# High variance in first two dimensions, correlated features\n",
    "true_cov = np.array([\n",
    "    [5.0, 2.5, 1.0, 0.5],\n",
    "    [2.5, 3.0, 0.8, 0.3],\n",
    "    [1.0, 0.8, 1.0, 0.2],\n",
    "    [0.5, 0.3, 0.2, 0.5]\n",
    "])\n",
    "\n",
    "# Generate multivariate normal data\n",
    "mean = np.zeros(n_features)\n",
    "X = np.random.multivariate_normal(mean, true_cov, n_samples)\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"\\nSample covariance matrix:\")\n",
    "print(np.cov(X.T).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Implementation\n",
    "\n",
    "We implement PCA from scratch following these steps:\n",
    "1. Center the data (subtract mean)\n",
    "2. Compute the covariance matrix\n",
    "3. Perform eigendecomposition\n",
    "4. Sort eigenvectors by eigenvalues (descending)\n",
    "5. Project data onto principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, n_components=None):\n",
    "    \"\"\"\n",
    "    Perform Principal Component Analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        Input data matrix\n",
    "    n_components : int, optional\n",
    "        Number of components to keep. If None, keep all.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_transformed : ndarray\n",
    "        Projected data\n",
    "    components : ndarray\n",
    "        Principal component vectors\n",
    "    explained_variance : ndarray\n",
    "        Variance explained by each component\n",
    "    explained_variance_ratio : ndarray\n",
    "        Proportion of variance explained\n",
    "    \"\"\"\n",
    "    # Step 1: Center the data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # Step 2: Compute covariance matrix\n",
    "    n_samples = X.shape[0]\n",
    "    cov_matrix = np.dot(X_centered.T, X_centered) / (n_samples - 1)\n",
    "    \n",
    "    # Step 3: Eigendecomposition\n",
    "    eigenvalues, eigenvectors = linalg.eigh(cov_matrix)\n",
    "    \n",
    "    # Step 4: Sort by eigenvalues in descending order\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    total_variance = np.sum(eigenvalues)\n",
    "    explained_variance_ratio = eigenvalues / total_variance\n",
    "    \n",
    "    # Step 5: Select components and project\n",
    "    if n_components is None:\n",
    "        n_components = X.shape[1]\n",
    "    \n",
    "    components = eigenvectors[:, :n_components]\n",
    "    X_transformed = np.dot(X_centered, components)\n",
    "    \n",
    "    return (X_transformed, components, \n",
    "            eigenvalues[:n_components], \n",
    "            explained_variance_ratio[:n_components])\n",
    "\n",
    "# Apply PCA\n",
    "X_pca, components, explained_var, explained_var_ratio = pca(X)\n",
    "\n",
    "print(\"Eigenvalues (variance explained by each PC):\")\n",
    "for i, (var, ratio) in enumerate(zip(explained_var, explained_var_ratio)):\n",
    "    print(f\"  PC{i+1}: {var:.4f} ({ratio*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative variance explained: {np.cumsum(explained_var_ratio)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We create a comprehensive visualization showing:\n",
    "1. Original data projected onto first two features\n",
    "2. Data in principal component space\n",
    "3. Explained variance (scree plot)\n",
    "4. Principal component loadings (contribution of original features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Original data (first two features)\n",
    "ax1 = axes[0, 0]\n",
    "scatter1 = ax1.scatter(X[:, 0], X[:, 1], c=X_pca[:, 0], cmap='viridis', \n",
    "                       alpha=0.6, edgecolors='none')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_title('Original Data (Features 1-2)\\nColored by PC1 score')\n",
    "plt.colorbar(scatter1, ax=ax1, label='PC1 score')\n",
    "\n",
    "# Add principal component directions\n",
    "mean_x = np.mean(X, axis=0)\n",
    "for i in range(2):\n",
    "    # Scale eigenvector for visualization\n",
    "    scale = np.sqrt(explained_var[i]) * 2\n",
    "    ax1.arrow(mean_x[0], mean_x[1], \n",
    "              components[0, i] * scale, components[1, i] * scale,\n",
    "              head_width=0.15, head_length=0.1, fc=f'C{i+1}', ec=f'C{i+1}',\n",
    "              linewidth=2)\n",
    "    ax1.text(mean_x[0] + components[0, i] * scale * 1.2,\n",
    "             mean_x[1] + components[1, i] * scale * 1.2,\n",
    "             f'PC{i+1}', fontsize=10, fontweight='bold', color=f'C{i+1}')\n",
    "\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axis('equal')\n",
    "\n",
    "# Plot 2: Data in PC space\n",
    "ax2 = axes[0, 1]\n",
    "scatter2 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=X_pca[:, 0], \n",
    "                       cmap='viridis', alpha=0.6, edgecolors='none')\n",
    "ax2.set_xlabel('Principal Component 1')\n",
    "ax2.set_ylabel('Principal Component 2')\n",
    "ax2.set_title('Data in Principal Component Space')\n",
    "ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=ax2, label='PC1 score')\n",
    "\n",
    "# Plot 3: Scree plot (explained variance)\n",
    "ax3 = axes[1, 0]\n",
    "pc_labels = [f'PC{i+1}' for i in range(len(explained_var_ratio))]\n",
    "bars = ax3.bar(pc_labels, explained_var_ratio * 100, color='steelblue', \n",
    "               edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add cumulative variance line\n",
    "cumsum = np.cumsum(explained_var_ratio) * 100\n",
    "ax3.plot(pc_labels, cumsum, 'ro-', linewidth=2, markersize=8, label='Cumulative')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, ratio) in enumerate(zip(bars, explained_var_ratio)):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{ratio*100:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax3.set_xlabel('Principal Component')\n",
    "ax3.set_ylabel('Explained Variance (%)')\n",
    "ax3.set_title('Scree Plot: Variance Explained by Each PC')\n",
    "ax3.legend(loc='center right')\n",
    "ax3.set_ylim(0, 110)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Component loadings heatmap\n",
    "ax4 = axes[1, 1]\n",
    "feature_names = [f'Feature {i+1}' for i in range(n_features)]\n",
    "\n",
    "im = ax4.imshow(components.T, cmap='RdBu_r', aspect='auto', \n",
    "                vmin=-1, vmax=1)\n",
    "ax4.set_xticks(range(n_features))\n",
    "ax4.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "ax4.set_yticks(range(n_features))\n",
    "ax4.set_yticklabels(pc_labels)\n",
    "ax4.set_xlabel('Original Features')\n",
    "ax4.set_ylabel('Principal Components')\n",
    "ax4.set_title('PC Loadings (Feature Contributions)')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(n_features):\n",
    "    for j in range(n_features):\n",
    "        text = ax4.text(j, i, f'{components[j, i]:.2f}',\n",
    "                       ha='center', va='center', fontsize=9,\n",
    "                       color='white' if abs(components[j, i]) > 0.5 else 'black')\n",
    "\n",
    "plt.colorbar(im, ax=ax4, label='Loading')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Example\n",
    "\n",
    "We demonstrate reconstruction error when keeping different numbers of components. The reconstruction is:\n",
    "\n",
    "$$\\hat{\\mathbf{X}} = \\mathbf{Z}_k \\mathbf{V}_k^T + \\boldsymbol{\\mu}$$\n",
    "\n",
    "where $\\mathbf{Z}_k$ is the projection onto $k$ components and $\\boldsymbol{\\mu}$ is the original mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_error(X, n_components):\n",
    "    \"\"\"\n",
    "    Compute mean squared reconstruction error for given number of components.\n",
    "    \"\"\"\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_centered = X - X_mean\n",
    "    \n",
    "    # Get transformation and components\n",
    "    X_transformed, components, _, _ = pca(X, n_components)\n",
    "    \n",
    "    # Reconstruct\n",
    "    X_reconstructed = np.dot(X_transformed, components.T) + X_mean\n",
    "    \n",
    "    # Compute MSE\n",
    "    mse = np.mean((X - X_reconstructed) ** 2)\n",
    "    return mse\n",
    "\n",
    "# Calculate reconstruction error for different numbers of components\n",
    "print(\"Reconstruction Error (MSE) by Number of Components:\")\n",
    "print(\"=\" * 50)\n",
    "for k in range(1, n_features + 1):\n",
    "    mse = reconstruction_error(X, k)\n",
    "    print(f\"  {k} component(s): MSE = {mse:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Note: With 2 components, we retain ~87% of variance\")\n",
    "print(\"with minimal reconstruction error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Mathematical foundation** of PCA through covariance matrix eigendecomposition\n",
    "2. **Implementation** from scratch using NumPy and SciPy\n",
    "3. **Variance decomposition** showing how PCA identifies directions of maximum variance\n",
    "4. **Dimensionality reduction** with controlled information loss\n",
    "\n",
    "Key takeaways:\n",
    "- The first two principal components capture approximately 87% of the total variance\n",
    "- PC loadings reveal which original features contribute most to each component\n",
    "- Reconstruction error quantifies information loss from dimensionality reduction\n",
    "\n",
    "PCA is widely used for:\n",
    "- Data visualization (reducing to 2-3 dimensions)\n",
    "- Noise reduction (keeping top components)\n",
    "- Feature extraction for downstream machine learning\n",
    "- Exploratory data analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
