{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a fundamental statistical method for estimating the parameters of a probability distribution by maximizing a likelihood function. Given observed data, MLE finds the parameter values that make the observed data most probable.\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### Likelihood Function\n",
    "\n",
    "Given a set of independent and identically distributed (i.i.d.) observations $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ from a probability distribution with parameter(s) $\\theta$, the **likelihood function** is defined as:\n",
    "\n",
    "$$L(\\theta | \\mathbf{x}) = \\prod_{i=1}^{n} f(x_i | \\theta)$$\n",
    "\n",
    "where $f(x_i | \\theta)$ is the probability density function (PDF) for continuous distributions or probability mass function (PMF) for discrete distributions.\n",
    "\n",
    "### Log-Likelihood\n",
    "\n",
    "In practice, we work with the **log-likelihood** function due to numerical stability and mathematical convenience:\n",
    "\n",
    "$$\\ell(\\theta | \\mathbf{x}) = \\ln L(\\theta | \\mathbf{x}) = \\sum_{i=1}^{n} \\ln f(x_i | \\theta)$$\n",
    "\n",
    "The maximum of $\\ell(\\theta)$ occurs at the same point as the maximum of $L(\\theta)$ since the logarithm is a monotonically increasing function.\n",
    "\n",
    "### Maximum Likelihood Estimator\n",
    "\n",
    "The MLE $\\hat{\\theta}_{\\text{MLE}}$ is obtained by solving:\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta} \\ell(\\theta | \\mathbf{x})$$\n",
    "\n",
    "For differentiable likelihood functions, we find critical points by setting the **score function** to zero:\n",
    "\n",
    "$$\\frac{\\partial \\ell(\\theta)}{\\partial \\theta} = 0$$\n",
    "\n",
    "### Properties of MLE\n",
    "\n",
    "Under certain regularity conditions, MLE has desirable asymptotic properties:\n",
    "\n",
    "1. **Consistency**: $\\hat{\\theta}_{\\text{MLE}} \\xrightarrow{p} \\theta_0$ as $n \\to \\infty$\n",
    "2. **Asymptotic Normality**: $\\sqrt{n}(\\hat{\\theta}_{\\text{MLE}} - \\theta_0) \\xrightarrow{d} \\mathcal{N}(0, I(\\theta_0)^{-1})$\n",
    "3. **Efficiency**: MLE achieves the Cramér-Rao lower bound asymptotically\n",
    "\n",
    "where $I(\\theta)$ is the Fisher Information:\n",
    "\n",
    "$$I(\\theta) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Normal Distribution\n",
    "\n",
    "For a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, the PDF is:\n",
    "\n",
    "$$f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "The log-likelihood for $n$ observations is:\n",
    "\n",
    "$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "Taking derivatives and setting to zero yields the MLEs:\n",
    "\n",
    "$$\\hat{\\mu}_{\\text{MLE}} = \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "$$\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters for normal distribution\n",
    "true_mu = 5.0\n",
    "true_sigma = 2.0\n",
    "\n",
    "# Generate sample data\n",
    "n_samples = 100\n",
    "data = np.random.normal(true_mu, true_sigma, n_samples)\n",
    "\n",
    "# Analytical MLE estimates\n",
    "mu_mle = np.mean(data)\n",
    "sigma_mle = np.sqrt(np.mean((data - mu_mle)**2))\n",
    "\n",
    "print(f\"True parameters: μ = {true_mu}, σ = {true_sigma}\")\n",
    "print(f\"MLE estimates: μ̂ = {mu_mle:.4f}, σ̂ = {sigma_mle:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define negative log-likelihood function for normal distribution\n",
    "def neg_log_likelihood_normal(params, data):\n",
    "    mu, sigma = params\n",
    "    if sigma <= 0:\n",
    "        return np.inf\n",
    "    n = len(data)\n",
    "    nll = (n/2) * np.log(2 * np.pi) + n * np.log(sigma) + \\\n",
    "          (1/(2 * sigma**2)) * np.sum((data - mu)**2)\n",
    "    return nll\n",
    "\n",
    "# Numerical optimization to find MLE\n",
    "initial_guess = [0.0, 1.0]\n",
    "result = minimize(neg_log_likelihood_normal, initial_guess, args=(data,),\n",
    "                  method='L-BFGS-B', bounds=[(-np.inf, np.inf), (1e-6, np.inf)])\n",
    "\n",
    "mu_opt, sigma_opt = result.x\n",
    "print(f\"Numerical optimization: μ̂ = {mu_opt:.4f}, σ̂ = {sigma_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Exponential Distribution\n",
    "\n",
    "For an exponential distribution with rate parameter $\\lambda$, the PDF is:\n",
    "\n",
    "$$f(x | \\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0$$\n",
    "\n",
    "The log-likelihood is:\n",
    "\n",
    "$$\\ell(\\lambda) = n \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "The MLE is:\n",
    "\n",
    "$$\\hat{\\lambda}_{\\text{MLE}} = \\frac{n}{\\sum_{i=1}^{n} x_i} = \\frac{1}{\\bar{x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameter for exponential distribution\n",
    "true_lambda = 0.5\n",
    "\n",
    "# Generate exponential data\n",
    "exp_data = np.random.exponential(1/true_lambda, n_samples)\n",
    "\n",
    "# Analytical MLE\n",
    "lambda_mle = 1 / np.mean(exp_data)\n",
    "\n",
    "print(f\"True parameter: λ = {true_lambda}\")\n",
    "print(f\"MLE estimate: λ̂ = {lambda_mle:.4f}\")\n",
    "\n",
    "# Visualize log-likelihood function\n",
    "lambda_range = np.linspace(0.1, 1.0, 200)\n",
    "log_likelihood = n_samples * np.log(lambda_range) - lambda_range * np.sum(exp_data)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lambda_range, log_likelihood, 'b-', linewidth=2)\n",
    "plt.axvline(x=lambda_mle, color='r', linestyle='--', label=f'MLE: λ̂ = {lambda_mle:.3f}')\n",
    "plt.axvline(x=true_lambda, color='g', linestyle=':', label=f'True: λ = {true_lambda}')\n",
    "plt.xlabel('λ', fontsize=12)\n",
    "plt.ylabel('Log-Likelihood ℓ(λ)', fontsize=12)\n",
    "plt.title('Log-Likelihood Function for Exponential Distribution', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Poisson Distribution\n",
    "\n",
    "For a Poisson distribution with parameter $\\lambda$, the PMF is:\n",
    "\n",
    "$$P(X = k | \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
    "\n",
    "The log-likelihood is:\n",
    "\n",
    "$$\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( x_i \\ln(\\lambda) - \\lambda - \\ln(x_i!) \\right)$$\n",
    "\n",
    "The MLE is simply the sample mean:\n",
    "\n",
    "$$\\hat{\\lambda}_{\\text{MLE}} = \\bar{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameter for Poisson distribution\n",
    "true_lambda_poisson = 3.5\n",
    "\n",
    "# Generate Poisson data\n",
    "poisson_data = np.random.poisson(true_lambda_poisson, n_samples)\n",
    "\n",
    "# Analytical MLE\n",
    "lambda_mle_poisson = np.mean(poisson_data)\n",
    "\n",
    "print(f\"True parameter: λ = {true_lambda_poisson}\")\n",
    "print(f\"MLE estimate: λ̂ = {lambda_mle_poisson:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Likelihood Surface for Normal Distribution\n",
    "\n",
    "We now visualize the likelihood surface as a function of both $\\mu$ and $\\sigma$ for the normal distribution example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid for likelihood surface\n",
    "mu_range = np.linspace(3, 7, 100)\n",
    "sigma_range = np.linspace(1, 3, 100)\n",
    "MU, SIGMA = np.meshgrid(mu_range, sigma_range)\n",
    "\n",
    "# Calculate log-likelihood over the grid\n",
    "LOG_LIKELIHOOD = np.zeros_like(MU)\n",
    "for i in range(len(sigma_range)):\n",
    "    for j in range(len(mu_range)):\n",
    "        mu_val = mu_range[j]\n",
    "        sigma_val = sigma_range[i]\n",
    "        LOG_LIKELIHOOD[i, j] = -neg_log_likelihood_normal([mu_val, sigma_val], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Contour plot of likelihood surface\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "contour = ax1.contourf(MU, SIGMA, LOG_LIKELIHOOD, levels=50, cmap='viridis')\n",
    "ax1.plot(mu_mle, sigma_mle, 'r*', markersize=15, label='MLE')\n",
    "ax1.plot(true_mu, true_sigma, 'w+', markersize=12, markeredgewidth=2, label='True')\n",
    "ax1.set_xlabel('μ', fontsize=12)\n",
    "ax1.set_ylabel('σ', fontsize=12)\n",
    "ax1.set_title('Log-Likelihood Surface', fontsize=14)\n",
    "ax1.legend(loc='upper right')\n",
    "plt.colorbar(contour, ax=ax1, label='Log-Likelihood')\n",
    "\n",
    "# Plot 2: Histogram with fitted distribution\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax2.hist(data, bins=20, density=True, alpha=0.7, color='steelblue', edgecolor='black', label='Data')\n",
    "x_plot = np.linspace(data.min() - 1, data.max() + 1, 200)\n",
    "ax2.plot(x_plot, stats.norm.pdf(x_plot, mu_mle, sigma_mle), 'r-', \n",
    "         linewidth=2, label=f'MLE fit: N({mu_mle:.2f}, {sigma_mle:.2f}²)')\n",
    "ax2.plot(x_plot, stats.norm.pdf(x_plot, true_mu, true_sigma), 'g--', \n",
    "         linewidth=2, label=f'True: N({true_mu}, {true_sigma}²)')\n",
    "ax2.set_xlabel('x', fontsize=12)\n",
    "ax2.set_ylabel('Density', fontsize=12)\n",
    "ax2.set_title('Data Histogram with Fitted Distributions', fontsize=14)\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Consistency of MLE - varying sample sizes\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "sample_sizes = [10, 25, 50, 100, 250, 500, 1000]\n",
    "n_trials = 100\n",
    "mu_estimates = np.zeros((len(sample_sizes), n_trials))\n",
    "\n",
    "for i, n in enumerate(sample_sizes):\n",
    "    for j in range(n_trials):\n",
    "        sample = np.random.normal(true_mu, true_sigma, n)\n",
    "        mu_estimates[i, j] = np.mean(sample)\n",
    "\n",
    "ax3.boxplot(mu_estimates.T, labels=[str(n) for n in sample_sizes])\n",
    "ax3.axhline(y=true_mu, color='r', linestyle='--', label=f'True μ = {true_mu}')\n",
    "ax3.set_xlabel('Sample Size', fontsize=12)\n",
    "ax3.set_ylabel('μ̂ (MLE estimate)', fontsize=12)\n",
    "ax3.set_title('Consistency of MLE: Convergence with Sample Size', fontsize=14)\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Asymptotic normality demonstration\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "n_large = 500\n",
    "n_simulations = 1000\n",
    "mu_estimates_large = np.array([np.mean(np.random.normal(true_mu, true_sigma, n_large)) \n",
    "                                for _ in range(n_simulations)])\n",
    "\n",
    "# Standardize the estimates\n",
    "standardized = np.sqrt(n_large) * (mu_estimates_large - true_mu) / true_sigma\n",
    "\n",
    "ax4.hist(standardized, bins=30, density=True, alpha=0.7, color='steelblue', \n",
    "         edgecolor='black', label='Standardized MLE')\n",
    "z = np.linspace(-4, 4, 200)\n",
    "ax4.plot(z, stats.norm.pdf(z), 'r-', linewidth=2, label='N(0, 1)')\n",
    "ax4.set_xlabel('Standardized μ̂', fontsize=12)\n",
    "ax4.set_ylabel('Density', fontsize=12)\n",
    "ax4.set_title('Asymptotic Normality of MLE', fontsize=14)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Information and Confidence Intervals\n",
    "\n",
    "The Fisher Information provides a measure of the amount of information that an observable random variable carries about an unknown parameter. For the normal distribution with known $\\sigma$, the Fisher Information for $\\mu$ is:\n",
    "\n",
    "$$I(\\mu) = \\frac{n}{\\sigma^2}$$\n",
    "\n",
    "This allows us to construct asymptotic confidence intervals:\n",
    "\n",
    "$$\\hat{\\mu} \\pm z_{\\alpha/2} \\sqrt{I(\\mu)^{-1}} = \\hat{\\mu} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Fisher Information and confidence interval\n",
    "fisher_info = n_samples / sigma_mle**2\n",
    "se_mu = 1 / np.sqrt(fisher_info)  # Standard error\n",
    "\n",
    "# 95% confidence interval\n",
    "alpha = 0.05\n",
    "z_critical = stats.norm.ppf(1 - alpha/2)\n",
    "ci_lower = mu_mle - z_critical * se_mu\n",
    "ci_upper = mu_mle + z_critical * se_mu\n",
    "\n",
    "print(f\"MLE estimate: μ̂ = {mu_mle:.4f}\")\n",
    "print(f\"Fisher Information: I(μ) = {fisher_info:.4f}\")\n",
    "print(f\"Standard Error: SE(μ̂) = {se_mu:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "print(f\"True μ = {true_mu} {'is' if ci_lower <= true_mu <= ci_upper else 'is NOT'} in the CI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Maximum Likelihood Estimation provides a principled and widely applicable framework for parameter estimation. Key takeaways:\n",
    "\n",
    "1. **Generality**: MLE can be applied to any parametric distribution\n",
    "2. **Optimality**: Under regularity conditions, MLE is asymptotically efficient\n",
    "3. **Practical Implementation**: Can be computed analytically or numerically\n",
    "4. **Inference**: Fisher Information enables confidence interval construction\n",
    "\n",
    "The examples demonstrated MLE for normal, exponential, and Poisson distributions, showcasing both analytical solutions and numerical optimization approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
