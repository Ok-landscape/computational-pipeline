{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Logistic regression is a fundamental statistical method for binary classification problems. Despite its name, it is a classification algorithm rather than a regression algorithm. It models the probability that an instance belongs to a particular class.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### The Logistic Function (Sigmoid)\n",
    "\n",
    "The core of logistic regression is the **sigmoid function** (also called the logistic function):\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "This function maps any real-valued number to the range $(0, 1)$, making it suitable for modeling probabilities.\n",
    "\n",
    "### Hypothesis Function\n",
    "\n",
    "For a feature vector $\\mathbf{x} \\in \\mathbb{R}^n$ and weight vector $\\mathbf{w} \\in \\mathbb{R}^n$ with bias $b$, the hypothesis is:\n",
    "\n",
    "$$h_{\\mathbf{w},b}(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}$$\n",
    "\n",
    "This represents $P(y=1 | \\mathbf{x}; \\mathbf{w}, b)$, the probability that the output is class 1 given input $\\mathbf{x}$.\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "The decision boundary is defined where $h_{\\mathbf{w},b}(\\mathbf{x}) = 0.5$, which occurs when:\n",
    "\n",
    "$$\\mathbf{w}^T \\mathbf{x} + b = 0$$\n",
    "\n",
    "### Loss Function: Binary Cross-Entropy\n",
    "\n",
    "For a single training example $(\\mathbf{x}^{(i)}, y^{(i)})$, the loss function is:\n",
    "\n",
    "$$\\mathcal{L}(h_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log(h_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) - (1 - y^{(i)}) \\log(1 - h_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "The total cost over $m$ training examples is:\n",
    "\n",
    "$$J(\\mathbf{w}, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) \\right]$$\n",
    "\n",
    "### Gradient Descent Update\n",
    "\n",
    "The gradients are computed as:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$$\n",
    "\n",
    "The parameters are updated as:\n",
    "\n",
    "$$w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}$$\n",
    "\n",
    "$$b := b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    # Clip values to avoid overflow in exp\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"Logistic Regression classifier using gradient descent.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the logistic regression model.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        self.cost_history = []\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = sigmoid(linear_model)\n",
    "            \n",
    "            # Compute cost\n",
    "            epsilon = 1e-15  # Small value to avoid log(0)\n",
    "            cost = -np.mean(y * np.log(y_predicted + epsilon) + \n",
    "                           (1 - y) * np.log(1 - y_predicted + epsilon))\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return sigmoid(linear_model)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset\n",
    "\n",
    "We create a linearly separable dataset with two classes for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(n_samples=200, noise=0.5):\n",
    "    \"\"\"Generate synthetic binary classification data.\"\"\"\n",
    "    n_per_class = n_samples // 2\n",
    "    \n",
    "    # Class 0: centered at (-1, -1)\n",
    "    X0 = np.random.randn(n_per_class, 2) * noise + np.array([-1, -1])\n",
    "    \n",
    "    # Class 1: centered at (1, 1)\n",
    "    X1 = np.random.randn(n_per_class, 2) * noise + np.array([1, 1])\n",
    "    \n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.hstack([np.zeros(n_per_class), np.ones(n_per_class)])\n",
    "    \n",
    "    # Shuffle the data\n",
    "    shuffle_idx = np.random.permutation(n_samples)\n",
    "    X = X[shuffle_idx]\n",
    "    y = y[shuffle_idx]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_classification_data(n_samples=300, noise=0.8)\n",
    "\n",
    "# Split into train and test sets (80-20 split)\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = LogisticRegression(learning_rate=0.1, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Learned weights: {model.weights}\")\n",
    "print(f\"Learned bias: {model.bias:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculate classification accuracy.\"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_acc = accuracy(y_train, y_train_pred)\n",
    "test_acc = accuracy(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We visualize the sigmoid function, decision boundary, training convergence, and classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Sigmoid Function\n",
    "ax1 = axes[0, 0]\n",
    "z = np.linspace(-10, 10, 200)\n",
    "ax1.plot(z, sigmoid(z), 'b-', linewidth=2, label=r'$\\sigma(z) = \\frac{1}{1+e^{-z}}$')\n",
    "ax1.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision threshold (0.5)')\n",
    "ax1.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax1.set_xlabel('z', fontsize=12)\n",
    "ax1.set_ylabel(r'$\\sigma(z)$', fontsize=12)\n",
    "ax1.set_title('Sigmoid (Logistic) Function', fontsize=14)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(-0.05, 1.05)\n",
    "\n",
    "# Plot 2: Training Convergence\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(model.cost_history, 'b-', linewidth=1.5)\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Cost (Binary Cross-Entropy)', fontsize=12)\n",
    "ax2.set_title('Training Convergence', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Plot 3: Decision Boundary\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "# Create mesh grid for decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Predict probabilities for mesh points\n",
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot probability contours\n",
    "contour = ax3.contourf(xx, yy, Z, levels=50, cmap='RdYlBu_r', alpha=0.8)\n",
    "plt.colorbar(contour, ax=ax3, label='P(y=1)')\n",
    "\n",
    "# Plot decision boundary (where P=0.5)\n",
    "ax3.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "# Plot training data\n",
    "scatter = ax3.scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "                      cmap=ListedColormap(['#FF6B6B', '#4ECDC4']),\n",
    "                      edgecolors='black', s=50, alpha=0.8)\n",
    "ax3.set_xlabel('$x_1$', fontsize=12)\n",
    "ax3.set_ylabel('$x_2$', fontsize=12)\n",
    "ax3.set_title('Decision Boundary with Training Data', fontsize=14)\n",
    "\n",
    "# Plot 4: Test Results with Predictions\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Plot probability contours\n",
    "contour2 = ax4.contourf(xx, yy, Z, levels=50, cmap='RdYlBu_r', alpha=0.8)\n",
    "plt.colorbar(contour2, ax=ax4, label='P(y=1)')\n",
    "ax4.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "# Plot test data with prediction indicators\n",
    "correct_mask = y_test == y_test_pred\n",
    "incorrect_mask = ~correct_mask\n",
    "\n",
    "# Correct predictions\n",
    "ax4.scatter(X_test[correct_mask, 0], X_test[correct_mask, 1], \n",
    "            c=y_test[correct_mask],\n",
    "            cmap=ListedColormap(['#FF6B6B', '#4ECDC4']),\n",
    "            edgecolors='black', s=80, alpha=0.9, marker='o',\n",
    "            label='Correct')\n",
    "\n",
    "# Incorrect predictions (if any)\n",
    "if np.any(incorrect_mask):\n",
    "    ax4.scatter(X_test[incorrect_mask, 0], X_test[incorrect_mask, 1],\n",
    "                c=y_test[incorrect_mask],\n",
    "                cmap=ListedColormap(['#FF6B6B', '#4ECDC4']),\n",
    "                edgecolors='black', s=150, alpha=0.9, marker='X',\n",
    "                label='Incorrect')\n",
    "\n",
    "ax4.set_xlabel('$x_1$', fontsize=12)\n",
    "ax4.set_ylabel('$x_2$', fontsize=12)\n",
    "ax4.set_title(f'Test Set Predictions (Accuracy: {test_acc:.2%})', fontsize=14)\n",
    "ax4.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we implemented **logistic regression from scratch** for binary classification:\n",
    "\n",
    "1. **Mathematical Foundation**: We derived the sigmoid function, hypothesis, binary cross-entropy loss, and gradient descent updates.\n",
    "\n",
    "2. **Implementation**: Built a complete `LogisticRegression` class with methods for training, probability prediction, and classification.\n",
    "\n",
    "3. **Results**: The model successfully learned to separate the two classes, as visualized by the decision boundary.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- Logistic regression finds a **linear decision boundary** in feature space\n",
    "- The sigmoid function converts linear outputs to probabilities\n",
    "- Binary cross-entropy is the appropriate loss for probabilistic classification\n",
    "- The gradient descent updates have the same form as linear regression, but with sigmoid applied\n",
    "\n",
    "### Extensions\n",
    "\n",
    "This basic implementation can be extended with:\n",
    "- **Regularization** (L1/L2) to prevent overfitting\n",
    "- **Multi-class classification** via one-vs-rest or softmax\n",
    "- **Feature scaling** for faster convergence\n",
    "- **More sophisticated optimizers** (Adam, RMSprop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
