{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Logistic regression is a fundamental classification algorithm that models the probability of a binary outcome. Despite its name, it is a classification method rather than a regression technique. It serves as a cornerstone of statistical learning and provides an excellent foundation for understanding more complex classification algorithms.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### The Logistic Function (Sigmoid)\n",
    "\n",
    "The logistic function transforms any real-valued input into a probability between 0 and 1:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where $z$ is the linear combination of features:\n",
    "\n",
    "$$z = \\mathbf{w}^T \\mathbf{x} + b = \\sum_{i=1}^{n} w_i x_i + b$$\n",
    "\n",
    "### Probability Model\n",
    "\n",
    "For binary classification with classes $y \\in \\{0, 1\\}$, logistic regression models:\n",
    "\n",
    "$$P(y=1 | \\mathbf{x}; \\mathbf{w}, b) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$$\n",
    "\n",
    "$$P(y=0 | \\mathbf{x}; \\mathbf{w}, b) = 1 - \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$$\n",
    "\n",
    "### Log-Odds (Logit) Interpretation\n",
    "\n",
    "The logit function is the inverse of the sigmoid:\n",
    "\n",
    "$$\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "This shows that logistic regression models the log-odds as a linear function of the features.\n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "Given a dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{m}$, the likelihood function is:\n",
    "\n",
    "$$L(\\mathbf{w}, b) = \\prod_{i=1}^{m} \\hat{p}_i^{y_i} (1 - \\hat{p}_i)^{1-y_i}$$\n",
    "\n",
    "where $\\hat{p}_i = \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b)$.\n",
    "\n",
    "### Cross-Entropy Loss Function\n",
    "\n",
    "Taking the negative log-likelihood gives the binary cross-entropy loss:\n",
    "\n",
    "$$J(\\mathbf{w}, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\ln(\\hat{p}_i) + (1 - y_i) \\ln(1 - \\hat{p}_i) \\right]$$\n",
    "\n",
    "### Gradient Descent Update Rules\n",
    "\n",
    "The gradients of the loss function are:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{p}_i - y_i) x_{ij}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{p}_i - y_i)$$\n",
    "\n",
    "The parameters are updated as:\n",
    "\n",
    "$$\\mathbf{w} := \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} J$$\n",
    "\n",
    "$$b := b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement logistic regression from scratch and apply it to a synthetic binary classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Dataset\n",
    "\n",
    "We create a two-class dataset with two features for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_samples=200, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate a synthetic binary classification dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Total number of samples\n",
    "    noise : float\n",
    "        Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray of shape (n_samples, 2)\n",
    "        Feature matrix\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Binary labels\n",
    "    \"\"\"\n",
    "    n_per_class = n_samples // 2\n",
    "    \n",
    "    # Class 0: centered at (-1, -1)\n",
    "    X0 = np.random.randn(n_per_class, 2) * 0.8 + np.array([-1, -1])\n",
    "    \n",
    "    # Class 1: centered at (1, 1)\n",
    "    X1 = np.random.randn(n_per_class, 2) * 0.8 + np.array([1, 1])\n",
    "    \n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.hstack([np.zeros(n_per_class), np.ones(n_per_class)])\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate dataset\n",
    "X, y = generate_dataset(n_samples=300)\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Class distribution: {np.sum(y==0):.0f} samples in class 0, {np.sum(y==1):.0f} samples in class 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression classifier using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate : float\n",
    "        Learning rate for gradient descent\n",
    "    n_iterations : int\n",
    "        Number of training iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute the sigmoid function.\"\"\"\n",
    "        # Clip values to avoid overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _compute_loss(self, y, y_pred):\n",
    "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
    "        m = len(y)\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training features\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Training labels\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        self.loss_history = []\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(z)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Record loss\n",
    "            loss = self._compute_loss(y, y_pred)\n",
    "            self.loss_history.append(loss)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self._sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split dataset into training and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray\n",
    "        Features\n",
    "    y : ndarray\n",
    "        Labels\n",
    "    test_size : float\n",
    "        Proportion of test set\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_samples = len(y)\n",
    "    n_test = int(n_samples * test_size)\n",
    "    \n",
    "    indices = np.random.permutation(n_samples)\n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "    \n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = LogisticRegression(learning_rate=0.5, n_iterations=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Display learned parameters\n",
    "print(f\"Learned weights: w = [{model.weights[0]:.4f}, {model.weights[1]:.4f}]\")\n",
    "print(f\"Learned bias: b = {model.bias:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute classification metrics.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing accuracy, precision, recall, and F1-score\n",
    "    \"\"\"\n",
    "    # True positives, false positives, etc.\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': np.array([[tn, fp], [fn, tp]])\n",
    "    }\n",
    "\n",
    "# Evaluate on training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_metrics = compute_metrics(y_train, y_train_pred)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "test_metrics = compute_metrics(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(f\"  Accuracy:  {train_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {train_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {train_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {train_metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(test_metrics['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We create a comprehensive visualization showing the decision boundary, training history, and probability contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Plot 1: Decision Boundary\n",
    "ax1 = axes[0, 0]\n",
    "\n",
    "# Create mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Predict on mesh grid\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision regions\n",
    "ax1.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)\n",
    "ax1.contour(xx, yy, Z, colors='k', linewidths=0.5)\n",
    "\n",
    "# Plot data points\n",
    "scatter = ax1.scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "                      cmap=cmap_bold, edgecolors='k', alpha=0.7, s=50, label='Train')\n",
    "ax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n",
    "            cmap=cmap_bold, edgecolors='k', alpha=0.7, s=100, marker='s', label='Test')\n",
    "\n",
    "# Plot decision boundary line\n",
    "w1, w2 = model.weights\n",
    "b = model.bias\n",
    "x_boundary = np.linspace(x_min, x_max, 100)\n",
    "y_boundary = -(w1 * x_boundary + b) / w2\n",
    "ax1.plot(x_boundary, y_boundary, 'k-', linewidth=2, label='Decision Boundary')\n",
    "\n",
    "ax1.set_xlabel('$x_1$', fontsize=12)\n",
    "ax1.set_ylabel('$x_2$', fontsize=12)\n",
    "ax1.set_title('Decision Boundary', fontsize=14)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_xlim(x_min, x_max)\n",
    "ax1.set_ylim(y_min, y_max)\n",
    "\n",
    "# Plot 2: Training Loss History\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(model.loss_history, 'b-', linewidth=1.5)\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "ax2.set_title('Training Loss History', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, len(model.loss_history))\n",
    "\n",
    "# Plot 3: Probability Contours\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "# Compute probability on mesh grid\n",
    "Z_prob = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_prob = Z_prob.reshape(xx.shape)\n",
    "\n",
    "# Plot probability contours\n",
    "contour = ax3.contourf(xx, yy, Z_prob, levels=20, cmap='RdBu_r', alpha=0.8)\n",
    "ax3.contour(xx, yy, Z_prob, levels=[0.5], colors='k', linewidths=2)\n",
    "plt.colorbar(contour, ax=ax3, label='$P(y=1|\\mathbf{x})$')\n",
    "\n",
    "# Plot data points\n",
    "ax3.scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "            cmap=cmap_bold, edgecolors='k', alpha=0.7, s=50)\n",
    "ax3.scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n",
    "            cmap=cmap_bold, edgecolors='k', alpha=0.7, s=100, marker='s')\n",
    "\n",
    "ax3.set_xlabel('$x_1$', fontsize=12)\n",
    "ax3.set_ylabel('$x_2$', fontsize=12)\n",
    "ax3.set_title('Probability Contours', fontsize=14)\n",
    "ax3.set_xlim(x_min, x_max)\n",
    "ax3.set_ylim(y_min, y_max)\n",
    "\n",
    "# Plot 4: Sigmoid Function and Decision\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Plot sigmoid function\n",
    "z_range = np.linspace(-6, 6, 200)\n",
    "sigmoid_values = 1 / (1 + np.exp(-z_range))\n",
    "ax4.plot(z_range, sigmoid_values, 'b-', linewidth=2, label='$\\sigma(z)$')\n",
    "ax4.axhline(y=0.5, color='r', linestyle='--', linewidth=1, label='Threshold = 0.5')\n",
    "ax4.axvline(x=0, color='gray', linestyle=':', linewidth=1)\n",
    "\n",
    "# Highlight regions\n",
    "ax4.fill_between(z_range, 0, sigmoid_values, where=(z_range < 0), \n",
    "                 alpha=0.3, color='red', label='Predict Class 0')\n",
    "ax4.fill_between(z_range, 0, sigmoid_values, where=(z_range >= 0), \n",
    "                 alpha=0.3, color='blue', label='Predict Class 1')\n",
    "\n",
    "ax4.set_xlabel('$z = \\mathbf{w}^T\\mathbf{x} + b$', fontsize=12)\n",
    "ax4.set_ylabel('$\\sigma(z)$', fontsize=12)\n",
    "ax4.set_title('Sigmoid Function', fontsize=14)\n",
    "ax4.legend(loc='upper left', fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_xlim(-6, 6)\n",
    "ax4.set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved as 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Linear Decision Boundary**: Logistic regression creates a linear decision boundary in feature space. The equation $\\mathbf{w}^T \\mathbf{x} + b = 0$ defines this hyperplane.\n",
    "\n",
    "2. **Probabilistic Output**: Unlike hard classifiers, logistic regression provides calibrated probability estimates, which are valuable for risk assessment and decision-making under uncertainty.\n",
    "\n",
    "3. **Convex Optimization**: The cross-entropy loss function is convex, guaranteeing convergence to the global minimum with gradient descent.\n",
    "\n",
    "4. **Interpretability**: Each weight $w_j$ represents the change in log-odds for a one-unit increase in feature $x_j$, holding other features constant.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Linear Separability**: Logistic regression assumes data is linearly separable, which limits its expressiveness for complex decision boundaries.\n",
    "\n",
    "2. **Feature Engineering**: Performance heavily depends on feature selection and transformation.\n",
    "\n",
    "3. **Class Imbalance**: Standard logistic regression can be biased toward the majority class in imbalanced datasets.\n",
    "\n",
    "### Extensions\n",
    "\n",
    "1. **Regularization**: L1 (Lasso) and L2 (Ridge) penalties can be added to prevent overfitting:\n",
    "   $$J_{reg} = J + \\frac{\\lambda}{2m} \\|\\mathbf{w}\\|^2_2$$\n",
    "\n",
    "2. **Multiclass Classification**: The softmax function generalizes logistic regression to multiple classes (multinomial logistic regression).\n",
    "\n",
    "3. **Kernel Methods**: Applying kernel tricks can create nonlinear decision boundaries while maintaining the probabilistic framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Logistic regression remains a fundamental tool in machine learning due to its interpretability, efficiency, and probabilistic framework. This implementation demonstrates the core concepts of maximum likelihood estimation, gradient descent optimization, and the sigmoid function. Understanding logistic regression provides a solid foundation for more advanced classification methods, including neural networks where the sigmoid (or softmax) function serves as the output activation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
