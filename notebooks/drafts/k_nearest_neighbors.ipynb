{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Classification\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "K-Nearest Neighbors is a non-parametric, instance-based learning algorithm used for classification and regression. Unlike parametric methods that learn a fixed set of parameters, KNN stores the entire training dataset and makes predictions based on the local neighborhood of query points.\n",
    "\n",
    "### Distance Metrics\n",
    "\n",
    "The fundamental operation in KNN is computing distances between points. For two points $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ and $\\mathbf{y} = (y_1, y_2, \\ldots, y_n)$ in $\\mathbb{R}^n$:\n",
    "\n",
    "**Euclidean Distance (L2 norm):**\n",
    "$$d_E(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} = \\|\\mathbf{x} - \\mathbf{y}\\|_2$$\n",
    "\n",
    "**Manhattan Distance (L1 norm):**\n",
    "$$d_M(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{n} |x_i - y_i| = \\|\\mathbf{x} - \\mathbf{y}\\|_1$$\n",
    "\n",
    "**Minkowski Distance (Lp norm):**\n",
    "$$d_p(\\mathbf{x}, \\mathbf{y}) = \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{1/p}$$\n",
    "\n",
    "### Classification Algorithm\n",
    "\n",
    "Given a training set $\\mathcal{D} = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_N, y_N)\\}$ where $y_i \\in \\{1, 2, \\ldots, C\\}$ represents class labels, the KNN classifier predicts the class of a query point $\\mathbf{x}_q$ as follows:\n",
    "\n",
    "1. Compute distances $d(\\mathbf{x}_q, \\mathbf{x}_i)$ for all training points\n",
    "2. Identify the $k$ nearest neighbors: $\\mathcal{N}_k(\\mathbf{x}_q)$\n",
    "3. Assign class by majority vote:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{c \\in \\{1,\\ldots,C\\}} \\sum_{i \\in \\mathcal{N}_k(\\mathbf{x}_q)} \\mathbb{1}(y_i = c)$$\n",
    "\n",
    "where $\\mathbb{1}(\\cdot)$ is the indicator function.\n",
    "\n",
    "### Weighted KNN\n",
    "\n",
    "To give closer neighbors more influence, we can weight votes by inverse distance:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{c} \\sum_{i \\in \\mathcal{N}_k(\\mathbf{x}_q)} w_i \\cdot \\mathbb{1}(y_i = c)$$\n",
    "\n",
    "where $w_i = \\frac{1}{d(\\mathbf{x}_q, \\mathbf{x}_i) + \\epsilon}$ and $\\epsilon$ prevents division by zero.\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "- **Training:** $O(1)$ - simply store the data\n",
    "- **Prediction:** $O(Nd)$ for $N$ training samples and $d$ features (naive implementation)\n",
    "- With KD-trees or Ball trees: $O(d \\log N)$ average case\n",
    "\n",
    "### Choosing K\n",
    "\n",
    "The hyperparameter $k$ controls the bias-variance tradeoff:\n",
    "- **Small $k$:** Low bias, high variance (sensitive to noise)\n",
    "- **Large $k$:** High bias, low variance (over-smoothing)\n",
    "\n",
    "Common practice: Use cross-validation to select optimal $k$, often choosing odd values to avoid ties in binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "We implement the KNN classifier without relying on scikit-learn, demonstrating the core algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors:\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors classifier implemented from scratch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Number of neighbors to consider\n",
    "    distance_metric : str\n",
    "        'euclidean' or 'manhattan'\n",
    "    weighted : bool\n",
    "        Whether to use distance-weighted voting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=3, distance_metric='euclidean', weighted=False):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.weighted = weighted\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Store training data.\"\"\"\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "        return self\n",
    "    \n",
    "    def _compute_distance(self, x1, x2):\n",
    "        \"\"\"Compute distance between two points.\"\"\"\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(x1 - x2))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {self.distance_metric}\")\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"Predict class for a single sample.\"\"\"\n",
    "        # Compute distances to all training points\n",
    "        distances = np.array([self._compute_distance(x, x_train) \n",
    "                              for x_train in self.X_train])\n",
    "        \n",
    "        # Get indices of k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_labels = self.y_train[k_indices]\n",
    "        k_distances = distances[k_indices]\n",
    "        \n",
    "        if self.weighted:\n",
    "            # Weighted voting by inverse distance\n",
    "            weights = 1.0 / (k_distances + 1e-10)\n",
    "            class_weights = {}\n",
    "            for label, weight in zip(k_labels, weights):\n",
    "                class_weights[label] = class_weights.get(label, 0) + weight\n",
    "            return max(class_weights, key=class_weights.get)\n",
    "        else:\n",
    "            # Majority voting\n",
    "            most_common = Counter(k_labels).most_common(1)\n",
    "            return most_common[0][0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for multiple samples.\"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self._predict_single(x) for x in X])\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Compute accuracy score.\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset\n",
    "\n",
    "We create a 2D classification problem with three distinct clusters to visualize the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiclass_data(n_samples_per_class=100):\n",
    "    \"\"\"\n",
    "    Generate synthetic 2D data with three classes.\n",
    "    Each class is a Gaussian cluster.\n",
    "    \"\"\"\n",
    "    # Class 0: Cluster centered at (0, 0)\n",
    "    X0 = np.random.randn(n_samples_per_class, 2) * 0.8 + np.array([0, 0])\n",
    "    y0 = np.zeros(n_samples_per_class, dtype=int)\n",
    "    \n",
    "    # Class 1: Cluster centered at (3, 3)\n",
    "    X1 = np.random.randn(n_samples_per_class, 2) * 0.8 + np.array([3, 3])\n",
    "    y1 = np.ones(n_samples_per_class, dtype=int)\n",
    "    \n",
    "    # Class 2: Cluster centered at (3, 0)\n",
    "    X2 = np.random.randn(n_samples_per_class, 2) * 0.8 + np.array([3, 0])\n",
    "    y2 = np.full(n_samples_per_class, 2, dtype=int)\n",
    "    \n",
    "    # Combine all data\n",
    "    X = np.vstack([X0, X1, X2])\n",
    "    y = np.hstack([y0, y1, y2])\n",
    "    \n",
    "    # Shuffle the data\n",
    "    shuffle_idx = np.random.permutation(len(y))\n",
    "    return X[shuffle_idx], y[shuffle_idx]\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_multiclass_data(n_samples_per_class=100)\n",
    "\n",
    "# Split into train and test sets (80-20 split)\n",
    "n_train = int(0.8 * len(y))\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Decision Boundaries\n",
    "\n",
    "We create a mesh grid over the feature space and classify each point to visualize how KNN partitions the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, title, ax):\n",
    "    \"\"\"\n",
    "    Plot decision boundary for a 2D classifier.\n",
    "    \"\"\"\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict on mesh grid\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision regions\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    ax.contour(xx, yy, Z, colors='k', linewidths=0.5)\n",
    "    \n",
    "    # Plot training points\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                         edgecolors='black', s=50)\n",
    "    \n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    \n",
    "    return scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('K-Nearest Neighbors: Effect of K on Decision Boundaries', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Test different values of k\n",
    "k_values = [1, 3, 5, 7, 15, 25]\n",
    "\n",
    "for ax, k in zip(axes.flat, k_values):\n",
    "    # Train KNN with this k\n",
    "    knn = KNearestNeighbors(k=k, distance_metric='euclidean')\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    train_acc = knn.score(X_train, y_train)\n",
    "    test_acc = knn.score(X_test, y_test)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    title = f'k = {k}\\nTrain Acc: {train_acc:.2f}, Test Acc: {test_acc:.2f}'\n",
    "    plot_decision_boundary(knn, X_train, y_train, title, ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDecision boundary visualization saved to 'plot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation for Optimal K\n",
    "\n",
    "We perform k-fold cross-validation to find the optimal number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, k_neighbors, n_folds=5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for KNN.\n",
    "    \n",
    "    Returns mean and std of accuracy across folds.\n",
    "    \"\"\"\n",
    "    n_samples = len(y)\n",
    "    fold_size = n_samples // n_folds\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for fold in range(n_folds):\n",
    "        # Define validation set\n",
    "        val_start = fold * fold_size\n",
    "        val_end = (fold + 1) * fold_size if fold < n_folds - 1 else n_samples\n",
    "        val_idx = indices[val_start:val_end]\n",
    "        train_idx = np.concatenate([indices[:val_start], indices[val_end:]])\n",
    "        \n",
    "        # Split data\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        knn = KNearestNeighbors(k=k_neighbors)\n",
    "        knn.fit(X_tr, y_tr)\n",
    "        acc = knn.score(X_val, y_val)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    return np.mean(accuracies), np.std(accuracies)\n",
    "\n",
    "# Test range of k values\n",
    "k_range = list(range(1, 31, 2))  # Odd values to avoid ties\n",
    "cv_results = []\n",
    "\n",
    "print(\"Cross-validation results:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for k in k_range:\n",
    "    mean_acc, std_acc = k_fold_cross_validation(X_train, y_train, k, n_folds=5)\n",
    "    cv_results.append((k, mean_acc, std_acc))\n",
    "    if k <= 11 or k >= 25:\n",
    "        print(f\"k = {k:2d}: Accuracy = {mean_acc:.3f} Â± {std_acc:.3f}\")\n",
    "\n",
    "# Find optimal k\n",
    "best_k = max(cv_results, key=lambda x: x[1])[0]\n",
    "print(f\"\\nOptimal k = {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross-validation results\n",
    "k_vals = [r[0] for r in cv_results]\n",
    "means = [r[1] for r in cv_results]\n",
    "stds = [r[2] for r in cv_results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(k_vals, means, yerr=stds, marker='o', capsize=4, capthick=2)\n",
    "plt.axvline(x=best_k, color='r', linestyle='--', label=f'Optimal k = {best_k}')\n",
    "plt.xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "plt.ylabel('Cross-Validation Accuracy', fontsize=12)\n",
    "plt.title('KNN: Cross-Validation Accuracy vs. K', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Euclidean vs Manhattan Distance\n",
    "\n",
    "We compare the two most common distance metrics to see how they affect classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Euclidean distance\n",
    "knn_euclidean = KNearestNeighbors(k=5, distance_metric='euclidean')\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "acc_euclidean = knn_euclidean.score(X_test, y_test)\n",
    "plot_decision_boundary(knn_euclidean, X_train, y_train, \n",
    "                       f'Euclidean Distance (L2)\\nTest Accuracy: {acc_euclidean:.3f}', \n",
    "                       axes[0])\n",
    "\n",
    "# Manhattan distance\n",
    "knn_manhattan = KNearestNeighbors(k=5, distance_metric='manhattan')\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "acc_manhattan = knn_manhattan.score(X_test, y_test)\n",
    "plot_decision_boundary(knn_manhattan, X_train, y_train, \n",
    "                       f'Manhattan Distance (L1)\\nTest Accuracy: {acc_manhattan:.3f}', \n",
    "                       axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Euclidean Distance Test Accuracy: {acc_euclidean:.3f}\")\n",
    "print(f\"Manhattan Distance Test Accuracy: {acc_manhattan:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted vs Unweighted KNN\n",
    "\n",
    "Compare standard majority voting with distance-weighted voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Unweighted KNN\n",
    "knn_unweighted = KNearestNeighbors(k=7, weighted=False)\n",
    "knn_unweighted.fit(X_train, y_train)\n",
    "acc_unweighted = knn_unweighted.score(X_test, y_test)\n",
    "plot_decision_boundary(knn_unweighted, X_train, y_train, \n",
    "                       f'Unweighted (Majority Vote)\\nTest Accuracy: {acc_unweighted:.3f}', \n",
    "                       axes[0])\n",
    "\n",
    "# Weighted KNN\n",
    "knn_weighted = KNearestNeighbors(k=7, weighted=True)\n",
    "knn_weighted.fit(X_train, y_train)\n",
    "acc_weighted = knn_weighted.score(X_test, y_test)\n",
    "plot_decision_boundary(knn_weighted, X_train, y_train, \n",
    "                       f'Weighted (Inverse Distance)\\nTest Accuracy: {acc_weighted:.3f}', \n",
    "                       axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Unweighted KNN Test Accuracy: {acc_unweighted:.3f}\")\n",
    "print(f\"Weighted KNN Test Accuracy: {acc_weighted:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **K-Nearest Neighbors** is a simple yet powerful non-parametric classifier that makes no assumptions about the underlying data distribution.\n",
    "\n",
    "2. **Choice of K**: Smaller values of $k$ lead to more complex decision boundaries (low bias, high variance), while larger values produce smoother boundaries (high bias, low variance). Cross-validation is essential for selecting the optimal $k$.\n",
    "\n",
    "3. **Distance Metrics**: Euclidean distance works well for continuous features, while Manhattan distance may be preferable for high-dimensional data or when features have different scales.\n",
    "\n",
    "4. **Weighted Voting**: Distance-weighted KNN can improve performance by giving closer neighbors more influence, especially useful when points near decision boundaries have varying distances to neighbors.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Computational Cost**: $O(Nd)$ prediction time for $N$ samples and $d$ features\n",
    "- **Curse of Dimensionality**: Performance degrades in high-dimensional spaces\n",
    "- **Feature Scaling**: Requires normalization when features have different scales\n",
    "- **Memory**: Must store entire training set\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Normalize/standardize features before applying KNN\n",
    "- Use cross-validation to tune $k$\n",
    "- Consider dimensionality reduction (PCA) for high-dimensional data\n",
    "- Use efficient data structures (KD-trees, Ball trees) for large datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
